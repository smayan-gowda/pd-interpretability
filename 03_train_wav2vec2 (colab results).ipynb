{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUjWaxf2tZEZ"
      },
      "source": [
        "# phase 3: wav2vec2 fine-tuning for pd classification\n",
        "\n",
        "fine-tune wav2vec2-base-960h on parkinson's disease voice detection using\n",
        "leave-one-subject-out (loso) cross-validation for rigorous evaluation.\n",
        "\n",
        "**methodology:**\n",
        "- loso cv: same protocol as clinical baseline (88.3% accuracy) for fair comparison\n",
        "- freeze cnn feature extractor + first 4 transformer layers (small dataset)\n",
        "- gradient checkpointing for memory efficiency\n",
        "- early stopping to prevent overfitting\n",
        "\n",
        "**expected results:**\n",
        "- target accuracy: 80-90% (competitive with clinical baseline)\n",
        "- comparison with 17-feature clinical model establishes deep learning value\n",
        "\n",
        "**hardware support:**\n",
        "- nvidia gpu (cuda) - recommended\n",
        "- apple silicon (mps) - supported but slower\n",
        "- cpu - not recommended (10-20+ hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a47800",
        "outputId": "f9b3c285-6aa0-49d8-9408-85dd37081eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyZPcRpptZEa"
      },
      "source": [
        "## 1. setup and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4jBDmNAtZEb",
        "outputId": "cb1dbb72-c298-4e72-eee9-1570735271c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "project root: /content/drive/MyDrive/pd-interpretability\n",
            "working directory: /content/drive/.shortcut-targets-by-id/1abPvoWlTNsqv6oksf7mu8bJcJD6F9bKg/pd-interpretability\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# set project root\n",
        "project_root = Path('/content/drive/MyDrive/pd-interpretability')\n",
        "\n",
        "# Ensure the project root directory exists\n",
        "if not project_root.exists():\n",
        "    print(f\"Project root not found: {project_root}. Creating directory...\")\n",
        "    project_root.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Created directory: {project_root}\")\n",
        "\n",
        "os.chdir(project_root)\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"project root: {project_root}\")\n",
        "print(f\"working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Owp_UuOdtZEb",
        "outputId": "139ea1ac-d079-428a-f1f9-cfd0ef355c2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
            "detected: nvidia gpu (Tesla T4)\n",
            "vram: 15.8 gb\n",
            "pytorch version: 2.9.0+cu126\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from collections import defaultdict\n",
        "!pip install torchcodec\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"detect best available compute device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = 'cuda'\n",
        "        device_name = torch.cuda.get_device_name(0)\n",
        "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"detected: nvidia gpu ({device_name})\")\n",
        "        print(f\"vram: {memory_gb:.1f} gb\")\n",
        "        return device, True\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = 'mps'\n",
        "        print(\"detected: apple silicon (mps)\")\n",
        "        return device, True\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "        print(\"warning: no gpu detected, using cpu (very slow)\")\n",
        "        return device, False\n",
        "\n",
        "device, has_accelerator = detect_device()\n",
        "print(f\"pytorch version: {torch.__version__}\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Oo9Ew_5tZEc",
        "outputId": "0689c6b2-57f0-4b33-b1ba-4d56bab8b76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imports complete\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    Wav2Vec2ForSequenceClassification,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "from src.data.datasets import ItalianPVSDataset\n",
        "from src.models.classifier import DataCollatorWithPadding\n",
        "\n",
        "print(\"imports complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIx3xwmutZEc"
      },
      "outputs": [],
      "source": [
        "# experiment configuration - aligned with working local version and fp32 for stability\n",
        "config = {\n",
        "    # model\n",
        "    'model_name': 'facebook/wav2vec2-base-960h',\n",
        "    'num_labels': 2,\n",
        "    'freeze_feature_extractor': True,\n",
        "    'freeze_encoder_layers': 4,\n",
        "    'dropout': 0.15,\n",
        "    'gradient_checkpointing': True,\n",
        "\n",
        "    # audio\n",
        "    'max_duration': 10.0,\n",
        "    'target_sr': 16000,\n",
        "\n",
        "    # training\n",
        "    'num_epochs': 15,\n",
        "    'learning_rate': 5e-5,\n",
        "    'warmup_ratio': 0.1,\n",
        "    'weight_decay': 0.01,\n",
        "    'early_stopping_patience': 3,\n",
        "\n",
        "    # loso cv\n",
        "    'max_folds': 3,\n",
        "\n",
        "    # random seed\n",
        "    'random_seed': 42\n",
        "}\n",
        "\n",
        "# device-specific settings\n",
        "if device == 'cuda':\n",
        "    config['batch_size'] = 8\n",
        "    config['gradient_accumulation_steps'] = 4\n",
        "    config['fp16'] = False  # disable fp16 on cuda to avoid NaNs\n",
        "elif device == 'mps':\n",
        "    config['batch_size'] = 4\n",
        "    config['gradient_accumulation_steps'] = 8\n",
        "    config['fp16'] = False  # mps fp16 unstable\n",
        "else:\n",
        "    config['batch_size'] = 2\n",
        "    config['gradient_accumulation_steps'] = 16\n",
        "    config['fp16'] = False\n",
        "\n",
        "effective_batch = config['batch_size'] * config['gradient_accumulation_steps']\n",
        "print(f\"batch size: {config['batch_size']} (effective: {effective_batch})\")\n",
        "print(f\"learning rate: {config['learning_rate']}\")\n",
        "print(f\"epochs: {config['num_epochs']}\")\n",
        "print(f\"frozen layers: cnn + first {config['freeze_encoder_layers']} transformer\")\n",
        "print(f\"max folds: {config['max_folds']} (set to None for full LOSO CV)\")\n",
        "print(f\"fp16: {config['fp16']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgW2OvldtZEd"
      },
      "source": [
        "## 2. load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m642zi7-tZEd",
        "outputId": "b139a506-0395-41b5-831e-5427418acf29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading dataset with audio caching enabled...\n",
            "this will pre-load all audio into ram to avoid disk reads during training\n",
            "samples: 831\n",
            "class distribution: 394 hc, 437 pd\n",
            "subjects: 61\n",
            "loso cv folds: 61\n"
          ]
        }
      ],
      "source": [
        "# load dataset with caching enabled to avoid slow disk i/o during training\n",
        "data_root = project_root / 'data' / 'raw'\n",
        "\n",
        "print(\"loading dataset with audio caching enabled...\")\n",
        "print(\"this will pre-load all audio into ram to avoid disk reads during training\")\n",
        "\n",
        "dataset = ItalianPVSDataset(\n",
        "    root_dir=str(data_root / 'italian_pvs'),\n",
        "    task=None,  # all tasks\n",
        "    target_sr=config['target_sr'],\n",
        "    max_duration=config['max_duration'],\n",
        "    normalize_audio=True,  # normalize since we use manual padding (no feature extractor)\n",
        "    cache_audio=True  # critical: cache audio in memory to avoid i/o bottleneck\n",
        ")\n",
        "\n",
        "print(f\"samples: {len(dataset)}\")\n",
        "\n",
        "# extract labels and subject ids for loso cv\n",
        "labels = np.array([s['label'] for s in dataset.samples])\n",
        "subject_ids = np.array([s['subject_id'] for s in dataset.samples])\n",
        "\n",
        "# unique subjects\n",
        "unique_subjects = np.unique(subject_ids)\n",
        "n_subjects = len(unique_subjects)\n",
        "\n",
        "# class distribution\n",
        "n_pd = np.sum(labels)\n",
        "n_hc = len(labels) - n_pd\n",
        "print(f\"class distribution: {n_hc} hc, {n_pd} pd\")\n",
        "print(f\"subjects: {n_subjects}\")\n",
        "print(f\"loso cv folds: {n_subjects}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyIfdVgJtZEe",
        "outputId": "53dcc88e-916a-4e59-8049-4522d0cc1c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PRE-CACHING ALL AUDIO INTO MEMORY\n",
            "============================================================\n",
            "samples to cache: 831\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "caching audio: 100%|██████████| 831/831 [00:40<00:00, 20.77sample/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "caching complete in 40.0s (0.7m)\n",
            "cached samples: 831/831\n",
            "cache size: 831 samples (~507 mb estimated)\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# pre-cache all audio data into ram before training\n",
        "# this is critical for performance\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PRE-CACHING ALL AUDIO INTO MEMORY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"samples to cache: {len(dataset)}\")\n",
        "print()\n",
        "\n",
        "cache_start = datetime.now()\n",
        "failed_samples = []\n",
        "\n",
        "for i in tqdm(range(len(dataset)), desc=\"caching audio\", unit=\"sample\"):\n",
        "    try:\n",
        "        # accessing each sample triggers caching\n",
        "        _ = dataset[i]\n",
        "    except Exception as e:\n",
        "        failed_samples.append((i, str(e)))\n",
        "\n",
        "cache_time = (datetime.now() - cache_start).total_seconds()\n",
        "\n",
        "print(f\"\\ncaching complete in {cache_time:.1f}s ({cache_time/60:.1f}m)\")\n",
        "print(f\"cached samples: {len(dataset) - len(failed_samples)}/{len(dataset)}\")\n",
        "\n",
        "if failed_samples:\n",
        "    print(f\"failed samples: {len(failed_samples)}\")\n",
        "    for idx, err in failed_samples[:5]:\n",
        "        print(f\"  sample {idx}: {err}\")\n",
        "\n",
        "# verify cache is populated\n",
        "cache_size = len(dataset._audio_cache) if dataset._audio_cache else 0\n",
        "estimated_memory_mb = cache_size * config['max_duration'] * config['target_sr'] * 4 / (1024 * 1024)\n",
        "print(f\"cache size: {cache_size} samples (~{estimated_memory_mb:.0f} mb estimated)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2wuLb95vK1q",
        "outputId": "0cb6d41f-b4fc-4f2c-e967-0df1cfad3c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experiment: wav2vec2_loso_20260104_030412\n",
            "output: /content/drive/MyDrive/pd-interpretability/results/checkpoints/wav2vec2_loso_20260104_030412\n"
          ]
        }
      ],
      "source": [
        "# create output directory\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "experiment_name = f\"wav2vec2_loso_{timestamp}\"\n",
        "output_dir = project_root / 'results' / 'checkpoints' / experiment_name\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# save config\n",
        "config_path = output_dir / 'config.json'\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"experiment: {experiment_name}\")\n",
        "print(f\"output: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyF_BATAtZEe"
      },
      "source": [
        "## 3. training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jevxKf6ItZEe"
      },
      "outputs": [],
      "source": [
        "def create_model(config: dict, device: str):\n",
        "    \"\"\"create wav2vec2 classifier with specified freezing strategy.\"\"\"\n",
        "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
        "        config['model_name'],\n",
        "        num_labels=config['num_labels'],\n",
        "        classifier_proj_size=256,\n",
        "        hidden_dropout=config['dropout'],\n",
        "        attention_dropout=config['dropout'],\n",
        "        final_dropout=config['dropout']\n",
        "    )\n",
        "\n",
        "    if config.get('gradient_checkpointing', False):\n",
        "        model.gradient_checkpointing_enable()\n",
        "\n",
        "    if config.get('freeze_feature_extractor', True):\n",
        "        model.freeze_feature_encoder()\n",
        "\n",
        "    freeze_layers = config.get('freeze_encoder_layers', 0)\n",
        "    if freeze_layers > 0:\n",
        "        for i, layer in enumerate(model.wav2vec2.encoder.layers):\n",
        "            if i < freeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    frozen_params = total_params - trainable_params\n",
        "\n",
        "    print(f\"      model parameters: {total_params:,} total, {trainable_params:,} trainable ({100*trainable_params/total_params:.1f}%), {frozen_params:,} frozen\")\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"count trainable and frozen parameters.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable, total - trainable\n",
        "\n",
        "\n",
        "def create_collate_fn(max_length: int):\n",
        "    \"\"\"manual padding collate to match working local notebook.\"\"\"\n",
        "    def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_values = [item['input_values'] for item in batch]\n",
        "        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
        "\n",
        "        max_len = min(max(len(x) for x in input_values), max_length)\n",
        "        padded_input = torch.zeros(len(input_values), max_len)\n",
        "        attention_mask = torch.zeros(len(input_values), max_len)\n",
        "\n",
        "        for i, wav in enumerate(input_values):\n",
        "            length = min(len(wav), max_len)\n",
        "            padded_input[i, :length] = wav[:length]\n",
        "            attention_mask[i, :length] = 1.0\n",
        "\n",
        "        return {\n",
        "            'input_values': padded_input,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "    return collate_fn\n",
        "\n",
        "\n",
        "def compute_class_weights(labels: np.ndarray) -> torch.Tensor:\n",
        "    \"\"\"compute inverse frequency class weights for imbalanced data.\"\"\"\n",
        "    from collections import Counter\n",
        "    counts = Counter(labels)\n",
        "    total = len(labels)\n",
        "    weights = torch.tensor([total / counts[0], total / counts[1]], dtype=torch.float32)\n",
        "    weights = weights / weights.sum() * len(counts)\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13c6pcyFtZEe"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import time\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scheduler, scaler, device, accumulation_steps, epoch_num=None, verbose=True):\n",
        "    \"\"\"train for one epoch with gradient accumulation and memory management.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"epoch {epoch_num}\" if epoch_num else \"training\", leave=False, unit=\"batch\") if verbose else loader\n",
        "    batch_times = []\n",
        "\n",
        "    for step, batch in enumerate(pbar):\n",
        "        batch_start = time.time()\n",
        "\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss / accumulation_steps\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        n_batches += 1\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            if scaler is not None:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if device == 'mps' and (step + 1) % 25 == 0:\n",
        "            torch.mps.synchronize()\n",
        "        elif device == 'cuda' and (step + 1) % 100 == 0:\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        del input_values, attention_mask, labels, outputs, loss\n",
        "\n",
        "        batch_time = time.time() - batch_start\n",
        "        batch_times.append(batch_time)\n",
        "\n",
        "        if verbose and hasattr(pbar, 'set_postfix'):\n",
        "            avg_batch_time = sum(batch_times[-10:]) / len(batch_times[-10:])\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss/n_batches:.4f}',\n",
        "                'batch_t': f'{batch_time:.2f}s',\n",
        "                'avg_t': f'{avg_batch_time:.2f}s'\n",
        "            })\n",
        "\n",
        "    if device == 'mps':\n",
        "        torch.mps.synchronize()\n",
        "        torch.mps.empty_cache()\n",
        "        gc.collect()\n",
        "    elif device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss / n_batches if n_batches > 0 else 0.0\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, desc=\"evaluating\"):\n",
        "    \"\"\"evaluate model and return predictions with probabilities.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=desc, leave=False, unit=\"batch\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_values, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        total_loss += outputs.loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "        del input_values, attention_mask, labels, outputs\n",
        "\n",
        "    return {\n",
        "        'predictions': np.array(all_preds),\n",
        "        'labels': np.array(all_labels),\n",
        "        'probabilities': np.array(all_probs),\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'loss': total_loss / n_batches if n_batches > 0 else 0.0\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANB0SU_7tZEf"
      },
      "source": [
        "## 4. loso cross-validation training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jh8V97ctZEf"
      },
      "outputs": [],
      "source": [
        "def train_fold(\n",
        "    dataset,\n",
        "    train_indices: np.ndarray,\n",
        "    test_indices: np.ndarray,\n",
        "    config: dict,\n",
        "    device: str,\n",
        "    fold_idx: int,\n",
        "    output_dir: Path\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    train model on single loso fold with stability aligned to working local notebook.\n",
        "    \"\"\"\n",
        "\n",
        "    fold_start_time = time.time()\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] creating data subsets...\")\n",
        "    train_subset = Subset(dataset, train_indices.tolist())\n",
        "    test_subset = Subset(dataset, test_indices.tolist())\n",
        "\n",
        "    train_labels = np.array([dataset.samples[i]['label'] for i in train_indices])\n",
        "    class_weights = compute_class_weights(train_labels)\n",
        "    print(f\"    [fold {fold_idx + 1}] class weights: HC={class_weights[0]:.3f}, PD={class_weights[1]:.3f}\")\n",
        "\n",
        "    max_length = int(config['max_duration'] * config['target_sr'])\n",
        "    collate_fn = create_collate_fn(max_length)\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] creating dataloaders (batch_size={config['batch_size']})...\")\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=config['batch_size'],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=0,\n",
        "        pin_memory=(device == 'cuda'),\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=config['batch_size'] * 2,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=0,\n",
        "        pin_memory=(device == 'cuda')\n",
        "    )\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] initializing model ({config['model_name']})...\")\n",
        "    model_init_start = time.time()\n",
        "    model = create_model(config, device)\n",
        "    print(f\"    [fold {fold_idx + 1}] model initialized in {time.time() - model_init_start:.1f}s\")\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=config['learning_rate'],\n",
        "        weight_decay=config['weight_decay'],\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    steps_per_epoch = max(1, len(train_loader) // config['gradient_accumulation_steps'])\n",
        "    total_steps = steps_per_epoch * config['num_epochs']\n",
        "    warmup_steps = int(total_steps * config['warmup_ratio'])\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    if config.get('fp16', False) and device == 'cuda':\n",
        "        scaler = torch.cuda.amp.GradScaler(init_scale=2**10, growth_interval=100)\n",
        "    else:\n",
        "        scaler = None\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] starting training ({config['num_epochs']} epochs, {len(train_loader)} batches/epoch)...\")\n",
        "    print(f\"    [fold {fold_idx + 1}] lr={config['learning_rate']}, warmup={warmup_steps}/{total_steps} steps\")\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    best_metrics = None\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss = train_epoch(\n",
        "            model, train_loader, optimizer, scheduler, scaler,\n",
        "            device, config['gradient_accumulation_steps'],\n",
        "            epoch_num=epoch + 1, verbose=True\n",
        "        )\n",
        "\n",
        "        if math.isnan(train_loss):\n",
        "            print(f\"      WARNING: training loss is NaN at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        test_metrics = evaluate(model, test_loader, device, desc=f\"eval epoch {epoch+1}\")\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"      epoch {epoch + 1}/{config['num_epochs']}: \"\n",
        "              f\"train_loss={train_loss:.4f}, test_loss={test_metrics['loss']:.4f}, \"\n",
        "              f\"test_acc={test_metrics['accuracy']:.1%}, lr={current_lr:.2e}, time={epoch_time:.1f}s\")\n",
        "\n",
        "        if test_metrics['loss'] < best_loss:\n",
        "            best_loss = test_metrics['loss']\n",
        "            best_acc = test_metrics['accuracy']\n",
        "            patience_counter = 0\n",
        "            best_metrics = test_metrics.copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config['early_stopping_patience']:\n",
        "                print(f\"      early stopping triggered at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "        elif device == 'mps':\n",
        "            torch.mps.synchronize()\n",
        "            torch.mps.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] final evaluation...\")\n",
        "    final_metrics = evaluate(model, test_loader, device, desc=\"final eval\") if best_metrics is None else best_metrics\n",
        "\n",
        "    fold_total_time = time.time() - fold_start_time\n",
        "    print(f\"    [fold {fold_idx + 1}] completed in {fold_total_time:.1f}s ({fold_total_time/60:.1f}m)\")\n",
        "    print(f\"    [fold {fold_idx + 1}] best accuracy: {best_acc:.1%}\")\n",
        "\n",
        "    print(f\"    [fold {fold_idx + 1}] cleaning up...\")\n",
        "    del model, optimizer, scheduler, train_loader, test_loader\n",
        "    del train_subset, test_subset, collate_fn\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "    elif device == 'mps':\n",
        "        torch.mps.synchronize()\n",
        "        torch.mps.empty_cache()\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'fold': fold_idx,\n",
        "        'train_samples': len(train_indices),\n",
        "        'test_samples': len(test_indices),\n",
        "        'accuracy': final_metrics['accuracy'],\n",
        "        'predictions': final_metrics['predictions'],\n",
        "        'labels': final_metrics['labels'],\n",
        "        'probabilities': final_metrics['probabilities'],\n",
        "        'fold_time': fold_total_time,\n",
        "        'avg_epoch_time': sum(epoch_times) / len(epoch_times) if epoch_times else 0,\n",
        "        'best_loss': best_loss\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "T17zKbTEtZEf",
        "outputId": "fb08b636-3f2e-419b-b37b-04234729fd94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STARTING LOSO CROSS-VALIDATION\n",
            "================================================================================\n",
            "total folds to run: 3\n",
            "device: cuda\n",
            "model: facebook/wav2vec2-base-960h\n",
            "batch size: 8\n",
            "learning rate: 5e-05\n",
            "max epochs per fold: 15\n",
            "early stopping patience: 5\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FOLD 1/3\n",
            "================================================================================\n",
            "  test subject: HC_elderly_AGNESE_P (hc)\n",
            "  train samples: 815, test samples: 16\n",
            "  starting training...\n",
            "    [fold 1] creating data subsets...\n",
            "    [fold 1] class weights: HC=1.072, PD=0.928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [fold 1] creating dataloaders (batch_size=8)...\n",
            "    [fold 1] initializing model (facebook/wav2vec2-base-960h)...\n",
            "      model parameters: 94,569,090 total, 62,017,154 trainable (65.6%), 32,551,936 frozen\n",
            "    [fold 1] model initialized in 0.5s\n",
            "    [fold 1] starting training (15 epochs, 101 batches/epoch)...\n",
            "    [fold 1] lr=5e-05, warmup=37/375 steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      WARNING: training loss is NaN at epoch 1\n",
            "      debugging: check audio preprocessing and feature extraction\n",
            "    [fold 1] final evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [fold 1] completed in 101.1s (1.7m)\n",
            "    [fold 1] best accuracy: 0.0%\n",
            "    [fold 1] cleaning up...\n",
            "\n",
            "  FOLD 1 COMPLETE:\n",
            "    fold accuracy: 100.0%\n",
            "    fold time: 101.5s (1.7m)\n",
            "    running overall accuracy: 100.0%\n",
            "    time elapsed: 0:01:41.495032\n",
            "    estimated time remaining: 3.4m (0.1h)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FOLD 2/3\n",
            "================================================================================\n",
            "  test subject: HC_elderly_ANGELA_C (hc)\n",
            "  train samples: 815, test samples: 16\n",
            "  starting training...\n",
            "    [fold 2] creating data subsets...\n",
            "    [fold 2] class weights: HC=1.072, PD=0.928\n",
            "    [fold 2] creating dataloaders (batch_size=8)...\n",
            "    [fold 2] initializing model (facebook/wav2vec2-base-960h)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      model parameters: 94,569,090 total, 62,017,154 trainable (65.6%), 32,551,936 frozen\n",
            "    [fold 2] model initialized in 0.3s\n",
            "    [fold 2] starting training (15 epochs, 101 batches/epoch)...\n",
            "    [fold 2] lr=5e-05, warmup=37/375 steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      WARNING: training loss is NaN at epoch 1\n",
            "      debugging: check audio preprocessing and feature extraction\n",
            "    [fold 2] final evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [fold 2] completed in 100.7s (1.7m)\n",
            "    [fold 2] best accuracy: 0.0%\n",
            "    [fold 2] cleaning up...\n",
            "\n",
            "  FOLD 2 COMPLETE:\n",
            "    fold accuracy: 100.0%\n",
            "    fold time: 101.1s (1.7m)\n",
            "    running overall accuracy: 100.0%\n",
            "    time elapsed: 0:03:22.597696\n",
            "    estimated time remaining: 1.7m (0.0h)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "FOLD 3/3\n",
            "================================================================================\n",
            "  test subject: HC_elderly_ANGELA_G (hc)\n",
            "  train samples: 815, test samples: 16\n",
            "  starting training...\n",
            "    [fold 3] creating data subsets...\n",
            "    [fold 3] class weights: HC=1.072, PD=0.928\n",
            "    [fold 3] creating dataloaders (batch_size=8)...\n",
            "    [fold 3] initializing model (facebook/wav2vec2-base-960h)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      model parameters: 94,569,090 total, 62,017,154 trainable (65.6%), 32,551,936 frozen\n",
            "    [fold 3] model initialized in 0.3s\n",
            "    [fold 3] starting training (15 epochs, 101 batches/epoch)...\n",
            "    [fold 3] lr=5e-05, warmup=37/375 steps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      epoch 1/15: train_loss=0.6937, test_loss=0.7756, test_acc=0.0%, lr=3.38e-05, time=115.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-502315034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     result = train_fold(\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtrain_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4054643388.py\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(dataset, train_indices, test_indices, config, device, fold_idx, output_dir)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         train_loss = train_epoch(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Pass accumulation_steps here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4183909626.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, scheduler, scaler, device, accumulation_steps, epoch_num, verbose)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# run loso cross-validation\n",
        "logo = LeaveOneGroupOut()\n",
        "n_folds = logo.get_n_splits(groups=subject_ids)\n",
        "\n",
        "if config['max_folds']:\n",
        "    n_folds = min(n_folds, config['max_folds'])\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"STARTING LOSO CROSS-VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"total folds to run: {n_folds}\")\n",
        "print(f\"device: {device}\")\n",
        "print(f\"model: {config['model_name']}\")\n",
        "print(f\"batch size: {config['batch_size']}\")\n",
        "print(f\"learning rate: {config['learning_rate']}\")\n",
        "print(f\"max epochs per fold: {config['num_epochs']}\")\n",
        "print(f\"early stopping patience: {config['early_stopping_patience']}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fold_results = []\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "all_probabilities = []\n",
        "all_subject_ids = []\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "for fold_idx, (train_idx, test_idx) in enumerate(\n",
        "    logo.split(np.arange(len(dataset)), labels, subject_ids)\n",
        "):\n",
        "    if config['max_folds'] and fold_idx >= config['max_folds']:\n",
        "        break\n",
        "\n",
        "    fold_start = datetime.now()\n",
        "\n",
        "    test_subject = subject_ids[test_idx[0]]\n",
        "    test_label = labels[test_idx[0]]\n",
        "    label_str = \"pd\" if test_label == 1 else \"hc\"\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"FOLD {fold_idx + 1}/{n_folds}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(f\"  test subject: {test_subject} ({label_str})\")\n",
        "    print(f\"  train samples: {len(train_idx)}, test samples: {len(test_idx)}\")\n",
        "    print(f\"  starting training...\")\n",
        "\n",
        "    result = train_fold(\n",
        "        dataset=dataset,\n",
        "        train_indices=train_idx,\n",
        "        test_indices=test_idx,\n",
        "        config=config,\n",
        "        device=device,\n",
        "        fold_idx=fold_idx,\n",
        "        output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    fold_time = (datetime.now() - fold_start).total_seconds()\n",
        "    elapsed_total = datetime.now() - start_time\n",
        "    avg_time_per_fold = elapsed_total.total_seconds() / (fold_idx + 1)\n",
        "    remaining_folds = n_folds - (fold_idx + 1)\n",
        "    eta = remaining_folds * avg_time_per_fold\n",
        "\n",
        "    fold_results.append(result)\n",
        "    all_predictions.extend(result['predictions'])\n",
        "    all_labels.extend(result['labels'])\n",
        "    all_probabilities.extend(result['probabilities'])\n",
        "    all_subject_ids.extend([test_subject] * len(result['predictions']))\n",
        "\n",
        "    # calculate running accuracy\n",
        "    running_acc = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"\\n  FOLD {fold_idx + 1} COMPLETE:\")\n",
        "    print(f\"    fold accuracy: {result['accuracy']:.1%}\")\n",
        "    print(f\"    fold time: {fold_time:.1f}s ({fold_time/60:.1f}m)\")\n",
        "    print(f\"    running overall accuracy: {running_acc:.1%}\")\n",
        "    print(f\"    time elapsed: {elapsed_total}\")\n",
        "    print(f\"    estimated time remaining: {eta/60:.1f}m ({eta/3600:.1f}h)\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "elapsed = datetime.now() - start_time\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(f\"LOSO CV COMPLETE\")\n",
        "print(f\"{'=' * 80}\")\n",
        "print(f\"total time: {elapsed} ({elapsed.total_seconds()/60:.1f}m)\")\n",
        "print(f\"{'=' * 80}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4B8J1HdtZEg"
      },
      "source": [
        "## 5. aggregate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOn5sCGptZEg"
      },
      "outputs": [],
      "source": [
        "# convert to numpy arrays\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "all_probabilities = np.array(all_probabilities)\n",
        "\n",
        "# overall metrics\n",
        "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "overall_precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "overall_recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "overall_f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "\n",
        "try:\n",
        "    overall_auc = roc_auc_score(all_labels, all_probabilities)\n",
        "except:\n",
        "    overall_auc = 0.5\n",
        "\n",
        "# per-fold accuracy\n",
        "fold_accuracies = [r['accuracy'] for r in fold_results]\n",
        "mean_accuracy = np.mean(fold_accuracies)\n",
        "std_accuracy = np.std(fold_accuracies)\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOSO CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\noverall metrics (aggregated across all folds):\")\n",
        "print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
        "print(f\"  precision: {overall_precision:.3f}\")\n",
        "print(f\"  recall: {overall_recall:.3f}\")\n",
        "print(f\"  f1 score: {overall_f1:.3f}\")\n",
        "print(f\"  auc-roc: {overall_auc:.3f}\")\n",
        "\n",
        "print(f\"\\nper-fold statistics:\")\n",
        "print(f\"  mean accuracy: {mean_accuracy:.1%} ± {std_accuracy:.1%}\")\n",
        "print(f\"  min: {min(fold_accuracies):.1%}, max: {max(fold_accuracies):.1%}\")\n",
        "\n",
        "print(f\"\\nconfusion matrix:\")\n",
        "print(f\"           predicted\")\n",
        "print(f\"            hc    pd\")\n",
        "print(f\"actual hc  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "print(f\"       pd  {cm[1,0]:4d}  {cm[1,1]:4d}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dhI7xY_vhTz"
      },
      "outputs": [],
      "source": [
        "# per-subject accuracy analysis\n",
        "subject_results = {}\n",
        "for subj, pred, label in zip(all_subject_ids, all_predictions, all_labels):\n",
        "    if subj not in subject_results:\n",
        "        subject_results[subj] = {'correct': 0, 'total': 0, 'label': label}\n",
        "    subject_results[subj]['total'] += 1\n",
        "    if pred == label:\n",
        "        subject_results[subj]['correct'] += 1\n",
        "\n",
        "subject_accuracies = []\n",
        "subject_data = []\n",
        "\n",
        "for subj, data in subject_results.items():\n",
        "    acc = data['correct'] / data['total']\n",
        "    subject_accuracies.append(acc)\n",
        "    diagnosis = 'pd' if data['label'] == 1 else 'hc'\n",
        "    subject_data.append({\n",
        "        'subject_id': subj,\n",
        "        'diagnosis': diagnosis,\n",
        "        'accuracy': acc,\n",
        "        'correct': data['correct'],\n",
        "        'total': data['total']\n",
        "    })\n",
        "\n",
        "subject_df = pd.DataFrame(subject_data)\n",
        "subject_df = subject_df.sort_values('accuracy', ascending=False)\n",
        "\n",
        "print(f\"\\nper-subject accuracy:\")\n",
        "print(f\"  mean: {np.mean(subject_accuracies):.1%}\")\n",
        "print(f\"  median: {np.median(subject_accuracies):.1%}\")\n",
        "print(f\"  subjects with 100% accuracy: {sum(1 for a in subject_accuracies if a == 1.0)}/{len(subject_accuracies)}\")\n",
        "print(f\"  subjects with <50% accuracy: {sum(1 for a in subject_accuracies if a < 0.5)}/{len(subject_accuracies)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrQ2kzUvj1u"
      },
      "source": [
        "## 6. comparison with clinical baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9-lbWZbtZEg"
      },
      "outputs": [],
      "source": [
        "# load clinical baseline results for comparison\n",
        "baseline_path = project_root / 'results' / 'clinical_baseline_results.json'\n",
        "\n",
        "if baseline_path.exists():\n",
        "    with open(baseline_path) as f:\n",
        "        baseline_results = json.load(f)\n",
        "\n",
        "    clinical_acc = baseline_results['svm']['accuracy_mean']\n",
        "    clinical_std = baseline_results['svm']['accuracy_std']\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"COMPARISON WITH CLINICAL BASELINE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nclinical baseline (svm, 17 features):\")\n",
        "    print(f\"  accuracy: {clinical_acc:.1%} \\u00b1 {clinical_std:.1%}\")\n",
        "\n",
        "    print(f\"\\nwav2vec2 (loso cv):\")\n",
        "    print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
        "    print(f\"  per-fold mean: {mean_accuracy:.1%} \\u00b1 {std_accuracy:.1%}\")\n",
        "\n",
        "    diff = overall_accuracy - clinical_acc\n",
        "    print(f\"\\ndifference: {diff:+.1%}\")\n",
        "\n",
        "    if diff > 0:\n",
        "        print(\"  wav2vec2 outperforms clinical baseline\")\n",
        "    elif diff < -0.05:\n",
        "        print(\"  clinical baseline outperforms wav2vec2\")\n",
        "    else:\n",
        "        print(\"  comparable performance\")\n",
        "else:\n",
        "    print(\"clinical baseline results not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIWoHalptZEg"
      },
      "source": [
        "## 7. save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOiBOGSptZEg"
      },
      "outputs": [],
      "source": [
        "# save comprehensive results\n",
        "results = {\n",
        "    'experiment': experiment_name,\n",
        "    'config': config,\n",
        "    'device': device,\n",
        "    'n_folds': len(fold_results),\n",
        "    'n_subjects': n_subjects,\n",
        "    'n_samples': len(dataset),\n",
        "\n",
        "    'overall_metrics': {\n",
        "        'accuracy': float(overall_accuracy),\n",
        "        'precision': float(overall_precision),\n",
        "        'recall': float(overall_recall),\n",
        "        'f1': float(overall_f1),\n",
        "        'auc': float(overall_auc)\n",
        "    },\n",
        "\n",
        "    'per_fold_stats': {\n",
        "        'mean_accuracy': float(mean_accuracy),\n",
        "        'std_accuracy': float(std_accuracy),\n",
        "        'min_accuracy': float(min(fold_accuracies)),\n",
        "        'max_accuracy': float(max(fold_accuracies))\n",
        "    },\n",
        "\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "\n",
        "    'fold_results': [\n",
        "        {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in r.items()}\n",
        "        for r in fold_results\n",
        "    ],\n",
        "\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'elapsed_time': str(elapsed)\n",
        "}\n",
        "\n",
        "# save to json\n",
        "results_path = output_dir / 'wav2vec2_loso_results.json'\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# save subject-level results\n",
        "subject_path = output_dir / 'wav2vec2_subject_accuracy.csv'\n",
        "subject_df.to_csv(subject_path, index=False)\n",
        "\n",
        "# also save to main results folder for easy access\n",
        "main_results_path = project_root / 'results' / 'wav2vec2_loso_results.json'\n",
        "with open(main_results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"results saved to:\")\n",
        "print(f\"  {results_path}\")\n",
        "print(f\"  {subject_path}\")\n",
        "print(f\"  {main_results_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw2QrqiitZEh"
      },
      "source": [
        "## 8. visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c6d04213"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y cm-super\n",
        "\n",
        "!texhash\n",
        "!updmap-sys --force\n",
        "!fmtutil-sys --all\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y texlive-latex-extra texlive-fonts-recommended dvipng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d1769b8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# publication-quality style with latex and times new roman\n",
        "plt.rcParams.update({\n",
        "    'text.usetex': True, # Revert to True to use LaTeX\n",
        "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{amssymb}', # Re-add LaTeX preamble\n",
        "    'font.family': 'serif',\n",
        "    'font.serif': ['Times', 'Times New Roman', 'DejaVu Serif'],\n",
        "    'font.size': 10,\n",
        "    'axes.titlesize': 11,\n",
        "    'axes.labelsize': 10,\n",
        "    'axes.linewidth': 0.8,\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'xtick.labelsize': 9,\n",
        "    'ytick.labelsize': 9,\n",
        "    'xtick.direction': 'out',\n",
        "    'ytick.direction': 'out',\n",
        "    'legend.fontsize': 9,\n",
        "    'legend.frameon': True,\n",
        "    'legend.fancybox': False,\n",
        "    'figure.dpi': 150,\n",
        "    'savefig.dpi': 300,\n",
        "    'savefig.bbox': 'tight',\n",
        "    'savefig.pad_inches': 0.1,\n",
        "})\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
        "\n",
        "# fold accuracy distribution\n",
        "ax1 = axes[0]\n",
        "ax1.bar(range(1, len(fold_accuracies) + 1), fold_accuracies, color='#3498db', alpha=0.8, edgecolor='white')\n",
        "ax1.axhline(y=mean_accuracy, color='#e74c3c', linestyle='--', linewidth=2,\n",
        "            label=rf'Mean: {mean_accuracy:.1%}')\n",
        "ax1.set_xlabel(r'Fold')\n",
        "ax1.set_ylabel(r'Accuracy')\n",
        "ax1.set_title(r'Per-Fold Accuracy (LOSO CV)')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.set_ylim(0, 1.05)\n",
        "\n",
        "# confusion matrix\n",
        "ax2 = axes[1]\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
        "            xticklabels=[r'HC', r'PD'], yticklabels=[r'HC', r'PD'],\n",
        "            cbar_kws={'label': 'Count'})\n",
        "ax2.set_xlabel(r'Predicted')\n",
        "ax2.set_ylabel(r'Actual')\n",
        "ax2.set_title(rf'Confusion Matrix (Accuracy: {overall_accuracy:.1%})')\n",
        "\n",
        "# subject accuracy histogram\n",
        "ax3 = axes[2]\n",
        "ax3.hist(subject_accuracies, bins=10, color='#3498db', alpha=0.8, edgecolor='white')\n",
        "ax3.axvline(x=np.mean(subject_accuracies), color='#e74c3c', linestyle='--', linewidth=2,\n",
        "            label=rf'Mean: {np.mean(subject_accuracies):.1%}')\n",
        "ax3.set_xlabel(r'Accuracy')\n",
        "ax3.set_ylabel(r'Count')\n",
        "ax3.set_title(r'Per-Subject Accuracy Distribution')\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# save to experiment dir\n",
        "plt.savefig(output_dir / 'wav2vec2_results_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(output_dir / 'wav2vec2_results_summary.pdf', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# also save to main figures folder\n",
        "main_fig_dir = project_root / 'results' / 'figures'\n",
        "main_fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.pdf', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"figures saved to {output_dir} and {main_fig_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2cNHUv5tZEh"
      },
      "outputs": [],
      "source": [
        "# comparison bar chart with clinical baseline\n",
        "if baseline_path.exists():\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    models = [r'Clinical Baseline' + '\\n' + r'(SVM, 17 features)',\n",
        "              r'Wav2Vec2' + '\\n' + r'(Fine-tuned)']\n",
        "    accuracies = [clinical_acc, overall_accuracy]\n",
        "    stds = [clinical_std, std_accuracy]\n",
        "    colors = ['#2ecc71', '#3498db']\n",
        "\n",
        "    bars = ax.bar(models, accuracies, yerr=stds, capsize=8, color=colors, alpha=0.8,\n",
        "                  edgecolor='white', linewidth=1.5)\n",
        "\n",
        "    for bar, acc, std in zip(bars, accuracies, stds):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.02,\n",
        "                rf'{acc:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    ax.set_ylabel(r'Accuracy')\n",
        "    ax.set_title(r'Model Comparison: LOSO Cross-Validation')\n",
        "    ax.set_ylim(0, 1.15)\n",
        "    ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label=r'Chance Level')\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # save figures\n",
        "    plt.savefig(output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(output_dir / 'model_comparison.pdf', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    main_fig_dir = project_root / 'results' / 'figures'\n",
        "    plt.savefig(main_fig_dir / 'fig_p3_02_model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(main_fig_dir / 'fig_p3_02_model_comparison.pdf', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"comparison figures saved\")\n",
        "else:\n",
        "    print(\"clinical baseline results not found - skipping comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy_YP7H9tZEh"
      },
      "source": [
        "## 9. summary and next steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HOogyVUtZEh"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PHASE 3 COMPLETE: WAV2VEC2 FINE-TUNING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nmodel: {config['model_name']}\")\n",
        "print(f\"device: {device}\")\n",
        "print(f\"loso cv folds: {len(fold_results)}\")\n",
        "print(f\"training time: {elapsed}\")\n",
        "\n",
        "print(f\"\\nresults:\")\n",
        "print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
        "print(f\"  precision: {overall_precision:.3f}\")\n",
        "print(f\"  recall: {overall_recall:.3f}\")\n",
        "print(f\"  f1 score: {overall_f1:.3f}\")\n",
        "print(f\"  auc-roc: {overall_auc:.3f}\")\n",
        "\n",
        "print(f\"\\nper-fold accuracy: {mean_accuracy:.1%} ± {std_accuracy:.1%}\")\n",
        "\n",
        "print(f\"\\nnext steps:\")\n",
        "print(f\"  1. phase 4: activation extraction (notebook 04)\")\n",
        "print(f\"  2. phase 5: probing experiments (notebook 05)\")\n",
        "print(f\"  3. phase 6: activation patching (notebook 06)\")\n",
        "\n",
        "print(f\"\\noutputs saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-oT8SVtZEh"
      },
      "source": [
        "## 10. train final model for activation extraction\n",
        "\n",
        "train a single model on all data for use in probing and patching experiments.\n",
        "this model will be used to extract activations in phase 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d78a31f"
      },
      "outputs": [],
      "source": [
        "# train final model on 80% of data (hold out 20% for testing)\n",
        "# uses the same corrected approach as LOSO CV training\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING FINAL MODEL FOR ACTIVATION EXTRACTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_subset, _, test_subset = dataset.get_subject_split(\n",
        "    test_size=0.2,\n",
        "    val_size=0.0,\n",
        "    random_state=config['random_seed']\n",
        ")\n",
        "\n",
        "print(f\"training final model...\")\n",
        "print(f\"  train samples: {len(train_subset)}\")\n",
        "print(f\"  test samples: {len(test_subset)}\")\n",
        "\n",
        "# compute class weights\n",
        "train_indices = train_subset.indices\n",
        "train_labels = np.array([dataset.samples[i]['label'] for i in train_indices])\n",
        "class_weights = compute_class_weights(train_labels)\n",
        "print(f\"  class weights: HC={class_weights[0]:.3f}, PD={class_weights[1]:.3f}\")\n",
        "\n",
        "# create model with stability settings\n",
        "final_model = create_model(config, device)\n",
        "\n",
        "# manual padding collate (no feature extractor)\n",
        "max_length = int(config['max_duration'] * config['target_sr'])\n",
        "collate_fn = create_collate_fn(max_length)\n",
        "\n",
        "# dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=(device == 'cuda'),\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_subset,\n",
        "    batch_size=config['batch_size'] * 2,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=(device == 'cuda')\n",
        ")\n",
        "\n",
        "# optimizer and scheduler\n",
        "optimizer = AdamW(\n",
        "    [p for p in final_model.parameters() if p.requires_grad],\n",
        "    lr=config['learning_rate'],\n",
        "    weight_decay=config['weight_decay'],\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "steps_per_epoch = max(1, len(train_loader) // config['gradient_accumulation_steps'])\n",
        "total_steps = steps_per_epoch * config['num_epochs']\n",
        "warmup_steps = int(total_steps * config['warmup_ratio'])\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# gradient scaler for fp16 (disabled in config for stability)\n",
        "if config.get('fp16', False) and device == 'cuda':\n",
        "    scaler = torch.cuda.amp.GradScaler(\n",
        "        init_scale=2**10,\n",
        "        growth_interval=100\n",
        "    )\n",
        "else:\n",
        "    scaler = None\n",
        "\n",
        "print(f\"  epochs: {config['num_epochs']}\")\n",
        "print(f\"  learning rate: {config['learning_rate']}\")\n",
        "print(f\"  warmup steps: {warmup_steps}/{total_steps}\")\n",
        "print()\n",
        "\n",
        "# training loop with early stopping\n",
        "best_acc = 0\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "checkpoint_path = None\n",
        "\n",
        "for epoch in range(config['num_epochs']):\n",
        "    train_loss = train_epoch(\n",
        "        final_model, train_loader, optimizer, scheduler, scaler,\n",
        "        device, config['gradient_accumulation_steps'], # Pass accumulation_steps here\n",
        "        epoch_num=epoch + 1, verbose=True\n",
        "    )\n",
        "\n",
        "    # check for NaN\n",
        "    if math.isnan(train_loss):\n",
        "        print(f\"  WARNING: NaN loss at epoch {epoch + 1}\")\n",
        "        print(f\"  debugging: check audio preprocessing and feature extraction\")\n",
        "        break\n",
        "\n",
        "    test_metrics = evaluate(final_model, test_loader, device, config)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"  epoch {epoch+1}/{config['num_epochs']}: loss={train_loss:.4f}, \"\n",
        "    test_metrics = evaluate(final_model, test_loader, device)\n",
        "\n",
        "    # save best model\n",
        "    if test_metrics['accuracy'] > best_acc:\n",
        "        best_acc = test_metrics['accuracy']\n",
        "        best_loss = test_metrics['loss']\n",
        "        patience_counter = 0\n",
        "        # save checkpoint\n",
        "        checkpoint_path = output_dir / 'final_model'\n",
        "        checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
        "        final_model.save_pretrained(checkpoint_path)\n",
        "        feature_extractor.save_pretrained(checkpoint_path)\n",
        "        print(f\"    -> saved best model (acc: {best_acc:.1%})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"    -> saved best model (acc: {best_acc:.1%})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= config['early_stopping_patience']:\n",
        "            print(f\"  early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # cleanup\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print()\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL MODEL TRAINING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  best test accuracy: {best_acc:.1%}\")\n",
        "print(f\"  best test loss: {best_loss:.4f}\")\n",
        "if checkpoint_path:\n",
        "    print(f\"  model saved to: {checkpoint_path}\")\n",
        "\n",
        "    # also copy to main results folder for easy access\n",
        "    main_model_path = project_root / 'results' / 'final_model'\n",
        "    main_model_path.mkdir(parents=True, exist_ok=True)\n",
        "    final_model.save_pretrained(main_model_path)\n",
        "    feature_extractor.save_pretrained(main_model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
