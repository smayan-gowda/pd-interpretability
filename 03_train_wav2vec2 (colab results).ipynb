{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUjWaxf2tZEZ"
   },
   "source": [
    "# phase 3: wav2vec2 fine-tuning for pd classification\n",
    "\n",
    "fine-tune wav2vec2-base-960h on parkinson's disease voice detection using\n",
    "leave-one-subject-out (loso) cross-validation for rigorous evaluation.\n",
    "\n",
    "**methodology:**\n",
    "- loso cv: same protocol as clinical baseline (88.3% accuracy) for fair comparison\n",
    "- freeze cnn feature extractor + first 4 transformer layers (small dataset)\n",
    "- gradient checkpointing for memory efficiency\n",
    "- early stopping to prevent overfitting\n",
    "\n",
    "**expected results:**\n",
    "- target accuracy: 80-90% (competitive with clinical baseline)\n",
    "- comparison with 17-feature clinical model establishes deep learning value\n",
    "\n",
    "**hardware support:**\n",
    "- nvidia gpu (cuda) - recommended\n",
    "- apple silicon (mps) - supported but slower\n",
    "- cpu - not recommended (10-20+ hours)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45a47800",
    "outputId": "e2fdca6a-8d98-4738-bf28-8a53512bbfb7"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyZPcRpptZEa"
   },
   "source": [
    "## 1. setup and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4jBDmNAtZEb",
    "outputId": "7b9ff3df-33ff-4d2a-be8b-5a9ceffa3d67"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "project root: /content/drive/MyDrive/pd-interpretability\n",
      "working directory: /content/drive/.shortcut-targets-by-id/1abPvoWlTNsqv6oksf7mu8bJcJD6F9bKg/pd-interpretability\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# set project root\n",
    "project_root = Path('/content/drive/MyDrive/pd-interpretability')\n",
    "\n",
    "# Ensure the project root directory exists\n",
    "if not project_root.exists():\n",
    "    print(f\"Project root not found: {project_root}. Creating directory...\")\n",
    "    project_root.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created directory: {project_root}\")\n",
    "\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"project root: {project_root}\")\n",
    "print(f\"working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "new_cell_3",
    "outputId": "95548de5-f6fa-4041-b86c-f6b91b7a2563"
   },
   "source": [
    "import shutil\n",
    "\n",
    "repo_url = \"https://github.com/smayan-gowda/pd-interpretability\"\n",
    "repo_name = repo_url.split('/')[-1]\n",
    "\n",
    "# Assuming os.chdir(project_root) was successful in the previous cell\n",
    "# Check if the current directory (project_root) contains a .git repository\n",
    "if (project_root / \".git\").exists():\n",
    "    print(f\"Repository already exists directly at {project_root}. Pulling latest changes...\")\n",
    "    # Ensure we are in the correct directory\n",
    "    os.chdir(project_root)\n",
    "    !git pull\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    # Check if there is a nested repo and try to fix it\n",
    "    nested_repo_candidate = project_root / repo_name\n",
    "    if nested_repo_candidate.is_dir() and (nested_repo_candidate / \".git\").exists():\n",
    "        print(f\"Found existing nested repository at {nested_repo_candidate}. Moving contents up to {project_root}...\")\n",
    "        for item in nested_repo_candidate.iterdir():\n",
    "            # Avoid moving the nested folder into itself if it's accidentally named the same\n",
    "            if item.name == nested_repo_candidate.name and item.is_dir():\n",
    "                continue\n",
    "            shutil.move(str(item), str(project_root / item.name))\n",
    "        shutil.rmtree(nested_repo_candidate)\n",
    "        print(f\"Moved contents and removed nested directory.\")\n",
    "\n",
    "        # After moving, the .git directory should now be in project_root, so re-evaluate\n",
    "        if (project_root / \".git\").exists():\n",
    "            print(f\"Repository now established directly at {project_root}. Pulling latest changes...\")\n",
    "            os.chdir(project_root)\n",
    "            !git pull\n",
    "            print(f\"Working directory: {os.getcwd()}\")\n",
    "        else:\n",
    "            # If after moving, .git is still not there, then clone directly\n",
    "            print(f\"Cloning repository {repo_name} directly into {project_root} after fixing nested structure...\")\n",
    "            os.chdir(project_root) # Ensure we are in project_root\n",
    "            !git clone {repo_url} .\n",
    "            print(f\"Working directory: {os.getcwd()}\")\n",
    "    else:\n",
    "        # No .git in project_root and no fixable nested repo, so clone directly\n",
    "        print(f\"Cloning repository {repo_name} directly into {project_root}...\")\n",
    "        os.chdir(project_root) # Ensure we are in project_root\n",
    "        !git clone {repo_url} .\n",
    "        print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Update sys.path to include the repository root\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# project_root is already correctly set by the Path object, no need to re-assign from repo_path\n",
    "print(f\"project root (after git operations): {project_root}\")\n",
    "print(f\"working directory (after git operations): {os.getcwd()}\")"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Repository already exists directly at /content/drive/MyDrive/pd-interpretability. Pulling latest changes...\n",
      "remote: Enumerating objects: 17, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 12 (delta 8), reused 10 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (12/12), 1.93 KiB | 0 bytes/s, done.\n",
      "From https://github.com/smayan-gowda/pd-interpretability\n",
      "   9fbd1ff..480f521  main       -> origin/main\n",
      "Updating 50e067b..480f521\n",
      "error: Your local changes to the following files would be overwritten by merge:\n",
      "\tnotebooks/colab/03_train_wav2vec2.ipynb\n",
      "Please commit your changes or stash them before you merge.\n",
      "Aborting\n",
      "Working directory: /content/drive/.shortcut-targets-by-id/1abPvoWlTNsqv6oksf7mu8bJcJD6F9bKg/pd-interpretability\n",
      "project root (after git operations): /content/drive/MyDrive/pd-interpretability\n",
      "working directory (after git operations): /content/drive/.shortcut-targets-by-id/1abPvoWlTNsqv6oksf7mu8bJcJD6F9bKg/pd-interpretability\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Owp_UuOdtZEb",
    "outputId": "7cd2957f-7718-4b90-92d2-6ec6036e4aaf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
      "detected: nvidia gpu (Tesla T4)\n",
      "vram: 15.8 gb\n",
      "pytorch version: 2.9.0+cu126\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "!pip install torchcodec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def detect_device():\n",
    "    \"\"\"detect best available compute device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"detected: nvidia gpu ({device_name})\")\n",
    "        print(f\"vram: {memory_gb:.1f} gb\")\n",
    "        return device, True\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "        print(\"detected: apple silicon (mps)\")\n",
    "        return device, True\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"warning: no gpu detected, using cpu (very slow)\")\n",
    "        return device, False\n",
    "\n",
    "device, has_accelerator = detect_device()\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Oo9Ew_5tZEc",
    "outputId": "f27dd65d-ff29-45b3-e10a-2b0adfbf5b06"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "imports complete\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from src.data.datasets import ItalianPVSDataset\n",
    "from src.models.classifier import DataCollatorWithPadding\n",
    "\n",
    "print(\"imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIx3xwmutZEc",
    "outputId": "b0c17009-6dc9-4798-80b9-d0b9d60b95f5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch size: 8 (effective: 32)\n",
      "learning rate: 2e-05\n",
      "max gradient norm: 1.0\n",
      "label smoothing: 0.1\n",
      "epochs: 20\n",
      "max folds: 3 (set to None for full LOSO CV)\n",
      "frozen layers: cnn + first 8 of 12 transformer layers\n",
      "layerdrop: 0.0 (disabled for stability)\n",
      "fp16: False (disabled to prevent NaN in Wav2Vec2 attention)\n"
     ]
    }
   ],
   "source": [
    "# experiment configuration - optimized for stable training on small medical datasets\n",
    "# key fixes: lower learning rate, gradient clipping, more frozen layers, label smoothing\n",
    "\n",
    "config = {\n",
    "    # model\n",
    "    'model_name': 'facebook/wav2vec2-base-960h',\n",
    "    'num_labels': 2,\n",
    "    'freeze_feature_extractor': True,\n",
    "    'freeze_encoder_layers': 8,  # freeze first 8 of 12 transformer layers (critical for small datasets)\n",
    "    'hidden_dropout': 0.1,\n",
    "    'attention_dropout': 0.1,\n",
    "    'final_dropout': 0.3,  # higher for classification head (regularization)\n",
    "    'layerdrop': 0.0,  # CRITICAL: disable layerdrop for training stability\n",
    "\n",
    "    # audio\n",
    "    'max_duration': 10.0,\n",
    "    'target_sr': 16000,\n",
    "\n",
    "    # training - conservative settings for small medical dataset\n",
    "    'num_epochs': 20,  # more epochs with slower learning\n",
    "    'learning_rate': 2e-5,  # CRITICAL: reduced from 5e-5 to prevent NaN\n",
    "    'warmup_ratio': 0.2,  # increased warmup for stability\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,  # CRITICAL: gradient clipping to prevent exploding gradients\n",
    "    'label_smoothing': 0.1,  # regularization for small dataset\n",
    "    'early_stopping_patience': 5,  # increased patience with lower lr\n",
    "\n",
    "    # loso cv\n",
    "    'max_folds': 3,  # set to 3 for quick test, None for full CV\n",
    "\n",
    "    # random seed\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# device-specific settings\n",
    "if device == 'cuda':\n",
    "    config['batch_size'] = 8\n",
    "    config['gradient_accumulation_steps'] = 4\n",
    "    # CRITICAL: fp16 disabled - Wav2Vec2 produces NaN in fp16 on small datasets\n",
    "    # The attention mechanism overflows in half precision with this data\n",
    "    config['fp16'] = False\n",
    "    config['gradient_checkpointing'] = True\n",
    "elif device == 'mps':\n",
    "    config['batch_size'] = 4\n",
    "    config['gradient_accumulation_steps'] = 8\n",
    "    config['fp16'] = False  # mps fp16 unstable\n",
    "    config['gradient_checkpointing'] = False  # causes memory leak on mps\n",
    "else:\n",
    "    config['batch_size'] = 2\n",
    "    config['gradient_accumulation_steps'] = 16\n",
    "    config['fp16'] = False\n",
    "    config['gradient_checkpointing'] = False\n",
    "\n",
    "effective_batch = config['batch_size'] * config['gradient_accumulation_steps']\n",
    "print(f\"batch size: {config['batch_size']} (effective: {effective_batch})\")\n",
    "print(f\"learning rate: {config['learning_rate']}\")\n",
    "print(f\"max gradient norm: {config['max_grad_norm']}\")\n",
    "print(f\"label smoothing: {config['label_smoothing']}\")\n",
    "print(f\"epochs: {config['num_epochs']}\")\n",
    "print(f\"max folds: {config['max_folds']} (set to None for full LOSO CV)\")\n",
    "print(f\"frozen layers: cnn + first {config['freeze_encoder_layers']} of 12 transformer layers\")\n",
    "print(f\"layerdrop: {config['layerdrop']} (disabled for stability)\")\n",
    "print(f\"fp16: {config['fp16']} (disabled to prevent NaN in Wav2Vec2 attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgW2OvldtZEd"
   },
   "source": [
    "## 2. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m642zi7-tZEd",
    "outputId": "1b75d1ee-ef41-4fdf-f236-106bc65fb71e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading dataset with audio caching enabled...\n",
      "this will pre-load all audio into ram to avoid disk reads during training\n",
      "samples: 831\n",
      "class distribution: 394 hc, 437 pd\n",
      "subjects: 61\n",
      "loso cv folds: 61\n"
     ]
    }
   ],
   "source": [
    "# load dataset with caching enabled to avoid slow disk i/o during training\n",
    "data_root = project_root / 'data' / 'raw'\n",
    "\n",
    "print(\"loading dataset with audio caching enabled...\")\n",
    "print(\"this will pre-load all audio into ram to avoid disk reads during training\")\n",
    "\n",
    "dataset = ItalianPVSDataset(\n",
    "    root_dir=str(data_root / 'italian_pvs'),\n",
    "    task=None,  # all tasks\n",
    "    target_sr=config['target_sr'],\n",
    "    max_duration=config['max_duration'],\n",
    "    cache_audio=True  # critical: cache audio in memory to avoid i/o bottleneck\n",
    ")\n",
    "\n",
    "print(f\"samples: {len(dataset)}\")\n",
    "\n",
    "# extract labels and subject ids for loso cv\n",
    "labels = np.array([s['label'] for s in dataset.samples])\n",
    "subject_ids = np.array([s['subject_id'] for s in dataset.samples])\n",
    "\n",
    "# unique subjects\n",
    "unique_subjects = np.unique(subject_ids)\n",
    "n_subjects = len(unique_subjects)\n",
    "\n",
    "# class distribution\n",
    "n_pd = np.sum(labels)\n",
    "n_hc = len(labels) - n_pd\n",
    "print(f\"class distribution: {n_hc} hc, {n_pd} pd\")\n",
    "print(f\"subjects: {n_subjects}\")\n",
    "print(f\"loso cv folds: {n_subjects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyIfdVgJtZEe",
    "outputId": "88c73c65-007b-41a7-f241-78cbb8ad574b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "PRE-CACHING ALL AUDIO INTO MEMORY\n",
      "============================================================\n",
      "samples to cache: 831\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "caching audio: 100%|██████████| 831/831 [00:40<00:00, 20.45sample/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "caching complete in 40.7s (0.7m)\n",
      "cached samples: 831/831\n",
      "cache size: 831 samples (~507 mb estimated)\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-cache all audio data into ram before training\n",
    "# this is critical for performance\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-CACHING ALL AUDIO INTO MEMORY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"samples to cache: {len(dataset)}\")\n",
    "print()\n",
    "\n",
    "cache_start = datetime.now()\n",
    "failed_samples = []\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"caching audio\", unit=\"sample\"):\n",
    "    try:\n",
    "        # accessing each sample triggers caching\n",
    "        _ = dataset[i]\n",
    "    except Exception as e:\n",
    "        failed_samples.append((i, str(e)))\n",
    "\n",
    "cache_time = (datetime.now() - cache_start).total_seconds()\n",
    "\n",
    "print(f\"\\ncaching complete in {cache_time:.1f}s ({cache_time/60:.1f}m)\")\n",
    "print(f\"cached samples: {len(dataset) - len(failed_samples)}/{len(dataset)}\")\n",
    "\n",
    "if failed_samples:\n",
    "    print(f\"failed samples: {len(failed_samples)}\")\n",
    "    for idx, err in failed_samples[:5]:\n",
    "        print(f\"  sample {idx}: {err}\")\n",
    "\n",
    "# verify cache is populated\n",
    "cache_size = len(dataset._audio_cache) if dataset._audio_cache else 0\n",
    "estimated_memory_mb = cache_size * config['max_duration'] * config['target_sr'] * 4 / (1024 * 1024)\n",
    "print(f\"cache size: {cache_size} samples (~{estimated_memory_mb:.0f} mb estimated)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# create output directory\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "experiment_name = f\"wav2vec2_loso_{timestamp}\"\n",
    "output_dir = project_root / 'results' / 'checkpoints' / experiment_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save config\n",
    "config_path = output_dir / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"experiment: {experiment_name}\")\n",
    "print(f\"output: {output_dir}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2wuLb95vK1q",
    "outputId": "947c683f-2639-459e-818a-66f5cfcf84fc"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "experiment: wav2vec2_loso_20260103_192930\n",
      "output: /content/drive/MyDrive/pd-interpretability/results/checkpoints/wav2vec2_loso_20260103_192930\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyF_BATAtZEe"
   },
   "source": [
    "## 3. training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jevxKf6ItZEe"
   },
   "outputs": [],
   "source": "def create_model(config: dict, device: str):\n    \"\"\"create fresh wav2vec2 model with optimized freezing strategy for small datasets.\"\"\"\n    from transformers import Wav2Vec2Config\n\n    # create config with stability settings\n    model_config = Wav2Vec2Config.from_pretrained(\n        config['model_name'],\n        num_labels=config['num_labels'],\n        hidden_dropout=config.get('hidden_dropout', 0.1),\n        attention_dropout=config.get('attention_dropout', 0.1),\n        final_dropout=config.get('final_dropout', 0.3),\n        layerdrop=config.get('layerdrop', 0.0),  # CRITICAL: disable for stability\n        classifier_proj_size=256,\n    )\n\n    model = Wav2Vec2ForSequenceClassification.from_pretrained(\n        config['model_name'],\n        config=model_config,\n        ignore_mismatched_sizes=True\n    )\n\n    # enable gradient checkpointing\n    if config.get('gradient_checkpointing', False):\n        model.gradient_checkpointing_enable()\n\n    # freeze cnn feature extractor (ALWAYS for wav2vec2)\n    if config.get('freeze_feature_extractor', True):\n        model.freeze_feature_encoder()\n\n    # freeze first n transformer layers (freeze MORE for small datasets)\n    freeze_layers = config.get('freeze_encoder_layers', 8)\n    if freeze_layers > 0:\n        for i, layer in enumerate(model.wav2vec2.encoder.layers):\n            if i < freeze_layers:\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    # count parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    frozen_params = total_params - trainable_params\n\n    print(f\"      model parameters: {total_params:,} total, {trainable_params:,} trainable ({100*trainable_params/total_params:.1f}%), {frozen_params:,} frozen\")\n\n    return model.to(device)\n\n\ndef count_parameters(model):\n    \"\"\"count trainable and frozen parameters.\"\"\"\n    total = sum(p.numel() for p in model.parameters())\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total, trainable, total - trainable\n\n\ndef create_collate_fn(feature_extractor, max_length: int):\n    \"\"\"\n    create collate function for wav2vec2 training.\n    \n    CRITICAL: wav2vec2 expects RAW waveforms, NOT normalized.\n    the model's internal feature extractor handles all preprocessing.\n    manual normalization causes NaN loss!\n    \"\"\"\n    def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n        # extract raw audio waveforms - dataset returns 1d tensors\n        input_values = [item['input_values'] for item in batch]\n        labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n\n        # pad sequences to max length in batch\n        max_len = min(max(len(x) for x in input_values), max_length)\n\n        padded_input = torch.zeros(len(input_values), max_len)\n        attention_mask = torch.zeros(len(input_values), max_len)\n\n        for i, wav in enumerate(input_values):\n            length = min(len(wav), max_len)\n            # CRITICAL: pass raw audio directly, NO normalization!\n            # wav2vec2 expects raw waveforms in natural amplitude range\n            padded_input[i, :length] = wav[:length]\n            attention_mask[i, :length] = 1.0\n\n        return {\n            'input_values': padded_input,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n\n    return collate_fn\n\n\ndef compute_class_weights(labels: np.ndarray) -> torch.Tensor:\n    \"\"\"compute inverse frequency class weights for imbalanced data.\"\"\"\n    from collections import Counter\n    counts = Counter(labels)\n    total = len(labels)\n    # inverse frequency weighting\n    weights = torch.tensor([total / counts[0], total / counts[1]], dtype=torch.float32)\n    # normalize so weights sum to num_classes\n    weights = weights / weights.sum() * len(counts)\n    return weights"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "13c6pcyFtZEe"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, scaler, device, config, epoch_num=None, verbose=True):\n",
    "    \"\"\"\n",
    "    train for one epoch with gradient accumulation, clipping, and NaN detection.\n",
    "\n",
    "    key stability features:\n",
    "    - gradient clipping (max_grad_norm)\n",
    "    - NaN/Inf detection and recovery\n",
    "    - label smoothing loss\n",
    "    - proper memory management\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    nan_batches = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    accumulation_steps = config.get('gradient_accumulation_steps', 4)\n",
    "    max_grad_norm = config.get('max_grad_norm', 1.0)\n",
    "    label_smoothing = config.get('label_smoothing', 0.1)\n",
    "\n",
    "    # create label smoothing loss\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    # create progress bar\n",
    "    pbar = tqdm(loader, desc=f\"epoch {epoch_num}\" if epoch_num else \"training\",\n",
    "                leave=False, unit=\"batch\") if verbose else loader\n",
    "\n",
    "    batch_times = []\n",
    "\n",
    "    for step, batch in enumerate(pbar):\n",
    "        batch_start = time.time()\n",
    "\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # check for NaN in inputs (silently skip)\n",
    "        if torch.isnan(input_values).any() or torch.isinf(input_values).any():\n",
    "            del input_values, attention_mask, labels\n",
    "            nan_batches += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_values, attention_mask=attention_mask)\n",
    "                    loss = loss_fn(outputs.logits, labels) / accumulation_steps\n",
    "\n",
    "                # check for NaN loss (silently skip)\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    del input_values, attention_mask, labels, outputs, loss\n",
    "                    optimizer.zero_grad()\n",
    "                    nan_batches += 1\n",
    "                    continue\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                outputs = model(input_values, attention_mask=attention_mask)\n",
    "                loss = loss_fn(outputs.logits, labels) / accumulation_steps\n",
    "\n",
    "                # check for NaN loss (silently skip)\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    del input_values, attention_mask, labels, outputs, loss\n",
    "                    optimizer.zero_grad()\n",
    "                    nan_batches += 1\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "            total_loss += loss.item() * accumulation_steps\n",
    "            n_batches += 1\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                if scaler is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "\n",
    "                # gradient clipping (CRITICAL for stability)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                if scaler is not None:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            optimizer.zero_grad()\n",
    "            nan_batches += 1\n",
    "            continue\n",
    "\n",
    "        # memory management - synchronize periodically\n",
    "        if device == 'mps' and (step + 1) % 25 == 0:\n",
    "            torch.mps.synchronize()\n",
    "        elif device == 'cuda' and (step + 1) % 100 == 0:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # cleanup batch tensors\n",
    "        del input_values, attention_mask, labels, outputs, loss\n",
    "\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "        # update progress bar with current stats\n",
    "        if verbose and hasattr(pbar, 'set_postfix'):\n",
    "            avg_batch_time = sum(batch_times[-10:]) / len(batch_times[-10:])\n",
    "            current_loss = total_loss / n_batches if n_batches > 0 else float('nan')\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{current_loss:.4f}',\n",
    "                'nan': nan_batches\n",
    "            }, refresh=False)\n",
    "\n",
    "    # end of epoch cleanup\n",
    "    if device == 'mps':\n",
    "        torch.mps.synchronize()\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "    elif device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / n_batches if n_batches > 0 else float('nan')\n",
    "\n",
    "    # only print warning at end of epoch if there were NaN batches\n",
    "    if nan_batches > 0 and nan_batches < len(loader):\n",
    "        print(f\"    note: {nan_batches}/{len(loader)} batches skipped (NaN)\")\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D9m8XboJtZEf"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, config=None, desc=\"evaluating\"):\n",
    "    \"\"\"evaluate model on dataset with memory management and progress tracking.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # use label smoothing in eval too for consistency\n",
    "    label_smoothing = config.get('label_smoothing', 0.0) if config else 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    pbar = tqdm(loader, desc=desc, leave=False, unit=\"batch\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_values, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "        # cleanup batch tensors\n",
    "        del input_values, attention_mask, labels, outputs, probs, preds, loss\n",
    "\n",
    "    # memory cleanup\n",
    "    if device == 'mps':\n",
    "        torch.mps.synchronize()\n",
    "        torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "    elif device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    n_batches = len(loader)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / n_batches if n_batches > 0 else 0,\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'predictions': np.array(all_preds),\n",
    "        'labels': np.array(all_labels),\n",
    "        'probabilities': np.array(all_probs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANB0SU_7tZEf"
   },
   "source": [
    "## 4. loso cross-validation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jh8V97ctZEf"
   },
   "outputs": [],
   "source": "def train_fold(\n    dataset,\n    train_indices: np.ndarray,\n    test_indices: np.ndarray,\n    config: dict,\n    device: str,\n    fold_idx: int,\n    output_dir: Path\n) -> Dict:\n    \"\"\"\n    train model on single loso fold with comprehensive stability measures.\n\n    key improvements:\n    - no manual normalization (wav2vec2 handles it internally)\n    - gradient clipping\n    - label smoothing\n    - NaN detection and recovery\n    - class weight handling for imbalanced data\n    - proper learning rate scheduling\n    \"\"\"\n\n    fold_start_time = time.time()\n\n    print(f\"    [fold {fold_idx + 1}] creating data subsets...\")\n    # create data subsets\n    train_subset = Subset(dataset, train_indices.tolist())\n    test_subset = Subset(dataset, test_indices.tolist())\n\n    # compute class weights for training data\n    train_labels = np.array([dataset.samples[i]['label'] for i in train_indices])\n    class_weights = compute_class_weights(train_labels)\n    print(f\"    [fold {fold_idx + 1}] class weights: HC={class_weights[0]:.3f}, PD={class_weights[1]:.3f}\")\n\n    # feature extractor and custom collate function\n    # CRITICAL: do_normalize=False because we pass raw audio to wav2vec2\n    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n        config['model_name'],\n        return_attention_mask=False\n    )\n    max_length = int(config['max_duration'] * config['target_sr'])\n    collate_fn = create_collate_fn(feature_extractor, max_length)\n\n    print(f\"    [fold {fold_idx + 1}] creating dataloaders (batch_size={config['batch_size']})...\")\n    # dataloaders - num_workers=0 for cached data to avoid pickle overhead\n    train_loader = DataLoader(\n        train_subset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=0,  # use 0 for cached data\n        pin_memory=(device == 'cuda'),\n        drop_last=True  # avoid small batches that can cause instability\n    )\n\n    test_loader = DataLoader(\n        test_subset,\n        batch_size=config['batch_size'] * 2,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=0,\n        pin_memory=(device == 'cuda')\n    )\n\n    print(f\"    [fold {fold_idx + 1}] initializing model ({config['model_name']})...\")\n    model_init_start = time.time()\n    # create fresh model with stability settings\n    model = create_model(config, device)\n    print(f\"    [fold {fold_idx + 1}] model initialized in {time.time() - model_init_start:.1f}s\")\n\n    # optimizer with numerical stability settings\n    optimizer = AdamW(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        eps=1e-8  # numerical stability\n    )\n\n    # scheduler with warmup for stability\n    steps_per_epoch = max(1, len(train_loader) // config['gradient_accumulation_steps'])\n    total_steps = steps_per_epoch * config['num_epochs']\n    warmup_steps = int(total_steps * config['warmup_ratio'])\n\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n\n    # gradient scaler for fp16 (disabled in config for stability)\n    if config.get('fp16', False) and device == 'cuda':\n        scaler = torch.cuda.amp.GradScaler(\n            init_scale=2**10,\n            growth_interval=100\n        )\n    else:\n        scaler = None\n\n    print(f\"    [fold {fold_idx + 1}] starting training ({config['num_epochs']} epochs, {len(train_loader)} batches/epoch)...\")\n    print(f\"    [fold {fold_idx + 1}] lr={config['learning_rate']}, warmup={warmup_steps}/{total_steps} steps\")\n\n    # training loop with early stopping\n    best_loss = float('inf')\n    best_acc = 0.0\n    patience_counter = 0\n    best_metrics = None\n    epoch_times = []\n\n    for epoch in range(config['num_epochs']):\n        epoch_start = time.time()\n\n        train_loss = train_epoch(\n            model, train_loader, optimizer, scheduler, scaler,\n            device, config,\n            epoch_num=epoch + 1, verbose=True\n        )\n\n        # check for training failure\n        if math.isnan(train_loss):\n            print(f\"      WARNING: training loss is NaN at epoch {epoch + 1}\")\n            print(f\"      this should NOT happen with fixed collate function - check audio data!\")\n            break\n\n        test_metrics = evaluate(model, test_loader, device, config, desc=f\"eval epoch {epoch+1}\")\n\n        epoch_time = time.time() - epoch_start\n        epoch_times.append(epoch_time)\n\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"      epoch {epoch + 1}/{config['num_epochs']}: \"\n              f\"train_loss={train_loss:.4f}, test_loss={test_metrics['loss']:.4f}, \"\n              f\"test_acc={test_metrics['accuracy']:.1%}, lr={current_lr:.2e}, time={epoch_time:.1f}s\")\n\n        # save best metrics (by validation loss)\n        if test_metrics['loss'] < best_loss:\n            best_loss = test_metrics['loss']\n            best_acc = test_metrics['accuracy']\n            patience_counter = 0\n            best_metrics = test_metrics.copy()\n        else:\n            patience_counter += 1\n            if patience_counter >= config['early_stopping_patience']:\n                print(f\"      early stopping triggered at epoch {epoch + 1}\")\n                break\n\n        # cleanup after each epoch\n        if device == 'cuda':\n            torch.cuda.synchronize()\n            torch.cuda.empty_cache()\n        elif device == 'mps':\n            torch.mps.synchronize()\n            torch.mps.empty_cache()\n        gc.collect()\n\n    print(f\"    [fold {fold_idx + 1}] final evaluation...\")\n    # final evaluation\n    final_metrics = evaluate(model, test_loader, device, config, desc=\"final eval\") if best_metrics is None else best_metrics\n\n    fold_total_time = time.time() - fold_start_time\n    print(f\"    [fold {fold_idx + 1}] completed in {fold_total_time:.1f}s ({fold_total_time/60:.1f}m)\")\n    print(f\"    [fold {fold_idx + 1}] best accuracy: {best_acc:.1%}\")\n\n    print(f\"    [fold {fold_idx + 1}] cleaning up...\")\n    # aggressive cleanup\n    del model, optimizer, scheduler, train_loader, test_loader\n    del train_subset, test_subset, feature_extractor, collate_fn\n\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    elif device == 'mps':\n        torch.mps.synchronize()\n        torch.mps.empty_cache()\n\n    gc.collect()\n\n    return {\n        'fold': fold_idx,\n        'train_samples': len(train_indices),\n        'test_samples': len(test_indices),\n        'accuracy': final_metrics['accuracy'],\n        'predictions': final_metrics['predictions'],\n        'labels': final_metrics['labels'],\n        'probabilities': final_metrics['probabilities'],\n        'fold_time': fold_total_time,\n        'avg_epoch_time': sum(epoch_times) / len(epoch_times) if epoch_times else 0,\n        'best_loss': best_loss\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T17zKbTEtZEf",
    "outputId": "ba2b2f0e-7798-4e00-ae61-8a1330652948"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STARTING LOSO CROSS-VALIDATION\n",
      "================================================================================\n",
      "total folds to run: 3\n",
      "device: cuda\n",
      "model: facebook/wav2vec2-base-960h\n",
      "batch size: 8\n",
      "learning rate: 2e-05\n",
      "max epochs per fold: 20\n",
      "early stopping patience: 5\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "FOLD 1/3\n",
      "================================================================================\n",
      "  test subject: HC_elderly_AGNESE_P (hc)\n",
      "  train samples: 815, test samples: 16\n",
      "  starting training...\n",
      "    [fold 1] creating data subsets...\n",
      "    [fold 1] class weights: HC=1.072, PD=0.928\n",
      "    [fold 1] creating dataloaders (batch_size=8)...\n",
      "    [fold 1] initializing model (facebook/wav2vec2-base-960h)...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      model parameters: 94,569,090 total, 33,665,666 trainable (35.6%), 60,903,424 frozen\n",
      "    [fold 1] model initialized in 0.9s\n",
      "    [fold 1] starting training (20 epochs, 101 batches/epoch)...\n",
      "    [fold 1] lr=2e-05, warmup=100/500 steps\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 1\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 2\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 3\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 4\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 5\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 6\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      WARNING: training loss is NaN at epoch 7\n",
      "      attempting to continue with reduced learning rate...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch 8:  24%|██▍       | 24/101 [00:08<00:28,  2.74batch/s]"
     ]
    }
   ],
   "source": [
    "# run loso cross-validation\n",
    "logo = LeaveOneGroupOut()\n",
    "n_folds = logo.get_n_splits(groups=subject_ids)\n",
    "\n",
    "if config['max_folds']:\n",
    "    n_folds = min(n_folds, config['max_folds'])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"STARTING LOSO CROSS-VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"total folds to run: {n_folds}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"model: {config['model_name']}\")\n",
    "print(f\"batch size: {config['batch_size']}\")\n",
    "print(f\"learning rate: {config['learning_rate']}\")\n",
    "print(f\"max epochs per fold: {config['num_epochs']}\")\n",
    "print(f\"early stopping patience: {config['early_stopping_patience']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fold_results = []\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "all_subject_ids = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(\n",
    "    logo.split(np.arange(len(dataset)), labels, subject_ids)\n",
    "):\n",
    "    if config['max_folds'] and fold_idx >= config['max_folds']:\n",
    "        break\n",
    "\n",
    "    fold_start = datetime.now()\n",
    "\n",
    "    test_subject = subject_ids[test_idx[0]]\n",
    "    test_label = labels[test_idx[0]]\n",
    "    label_str = \"pd\" if test_label == 1 else \"hc\"\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{n_folds}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"  test subject: {test_subject} ({label_str})\")\n",
    "    print(f\"  train samples: {len(train_idx)}, test samples: {len(test_idx)}\")\n",
    "    print(f\"  starting training...\")\n",
    "\n",
    "    result = train_fold(\n",
    "        dataset=dataset,\n",
    "        train_indices=train_idx,\n",
    "        test_indices=test_idx,\n",
    "        config=config,\n",
    "        device=device,\n",
    "        fold_idx=fold_idx,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    fold_time = (datetime.now() - fold_start).total_seconds()\n",
    "    elapsed_total = datetime.now() - start_time\n",
    "    avg_time_per_fold = elapsed_total.total_seconds() / (fold_idx + 1)\n",
    "    remaining_folds = n_folds - (fold_idx + 1)\n",
    "    eta = remaining_folds * avg_time_per_fold\n",
    "\n",
    "    fold_results.append(result)\n",
    "    all_predictions.extend(result['predictions'])\n",
    "    all_labels.extend(result['labels'])\n",
    "    all_probabilities.extend(result['probabilities'])\n",
    "    all_subject_ids.extend([test_subject] * len(result['predictions']))\n",
    "\n",
    "    # calculate running accuracy\n",
    "    running_acc = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"\\n  FOLD {fold_idx + 1} COMPLETE:\")\n",
    "    print(f\"    fold accuracy: {result['accuracy']:.1%}\")\n",
    "    print(f\"    fold time: {fold_time:.1f}s ({fold_time/60:.1f}m)\")\n",
    "    print(f\"    running overall accuracy: {running_acc:.1%}\")\n",
    "    print(f\"    time elapsed: {elapsed_total}\")\n",
    "    print(f\"    estimated time remaining: {eta/60:.1f}m ({eta/3600:.1f}h)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "elapsed = datetime.now() - start_time\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"LOSO CV COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"total time: {elapsed} ({elapsed.total_seconds()/60:.1f}m)\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4B8J1HdtZEg"
   },
   "source": [
    "## 5. aggregate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOn5sCGptZEg"
   },
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "# overall metrics\n",
    "overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "overall_precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "overall_recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "overall_f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "\n",
    "try:\n",
    "    overall_auc = roc_auc_score(all_labels, all_probabilities)\n",
    "except:\n",
    "    overall_auc = 0.5\n",
    "\n",
    "# per-fold accuracy\n",
    "fold_accuracies = [r['accuracy'] for r in fold_results]\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "std_accuracy = np.std(fold_accuracies)\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOSO CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\noverall metrics (aggregated across all folds):\")\n",
    "print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
    "print(f\"  precision: {overall_precision:.3f}\")\n",
    "print(f\"  recall: {overall_recall:.3f}\")\n",
    "print(f\"  f1 score: {overall_f1:.3f}\")\n",
    "print(f\"  auc-roc: {overall_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nper-fold statistics:\")\n",
    "print(f\"  mean accuracy: {mean_accuracy:.1%} ± {std_accuracy:.1%}\")\n",
    "print(f\"  min: {min(fold_accuracies):.1%}, max: {max(fold_accuracies):.1%}\")\n",
    "\n",
    "print(f\"\\nconfusion matrix:\")\n",
    "print(f\"           predicted\")\n",
    "print(f\"            hc    pd\")\n",
    "print(f\"actual hc  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       pd  {cm[1,0]:4d}  {cm[1,1]:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# per-subject accuracy analysis\n",
    "subject_results = {}\n",
    "for subj, pred, label in zip(all_subject_ids, all_predictions, all_labels):\n",
    "    if subj not in subject_results:\n",
    "        subject_results[subj] = {'correct': 0, 'total': 0, 'label': label}\n",
    "    subject_results[subj]['total'] += 1\n",
    "    if pred == label:\n",
    "        subject_results[subj]['correct'] += 1\n",
    "\n",
    "subject_accuracies = []\n",
    "subject_data = []\n",
    "\n",
    "for subj, data in subject_results.items():\n",
    "    acc = data['correct'] / data['total']\n",
    "    subject_accuracies.append(acc)\n",
    "    diagnosis = 'pd' if data['label'] == 1 else 'hc'\n",
    "    subject_data.append({\n",
    "        'subject_id': subj,\n",
    "        'diagnosis': diagnosis,\n",
    "        'accuracy': acc,\n",
    "        'correct': data['correct'],\n",
    "        'total': data['total']\n",
    "    })\n",
    "\n",
    "subject_df = pd.DataFrame(subject_data)\n",
    "subject_df = subject_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(f\"\\nper-subject accuracy:\")\n",
    "print(f\"  mean: {np.mean(subject_accuracies):.1%}\")\n",
    "print(f\"  median: {np.median(subject_accuracies):.1%}\")\n",
    "print(f\"  subjects with 100% accuracy: {sum(1 for a in subject_accuracies if a == 1.0)}/{len(subject_accuracies)}\")\n",
    "print(f\"  subjects with <50% accuracy: {sum(1 for a in subject_accuracies if a < 0.5)}/{len(subject_accuracies)}\")"
   ],
   "metadata": {
    "id": "_dhI7xY_vhTz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. comparison with clinical baseline"
   ],
   "metadata": {
    "id": "XIrQ2kzUvj1u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9-lbWZbtZEg"
   },
   "outputs": [],
   "source": [
    "# load clinical baseline results for comparison\n",
    "baseline_path = project_root / 'results' / 'clinical_baseline_results.json'\n",
    "\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path) as f:\n",
    "        baseline_results = json.load(f)\n",
    "\n",
    "    clinical_acc = baseline_results['svm']['accuracy_mean']\n",
    "    clinical_std = baseline_results['svm']['accuracy_std']\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPARISON WITH CLINICAL BASELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nclinical baseline (svm, 17 features):\")\n",
    "    print(f\"  accuracy: {clinical_acc:.1%} \\u00b1 {clinical_std:.1%}\")\n",
    "\n",
    "    print(f\"\\nwav2vec2 (loso cv):\")\n",
    "    print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
    "    print(f\"  per-fold mean: {mean_accuracy:.1%} \\u00b1 {std_accuracy:.1%}\")\n",
    "\n",
    "    diff = overall_accuracy - clinical_acc\n",
    "    print(f\"\\ndifference: {diff:+.1%}\")\n",
    "\n",
    "    if diff > 0:\n",
    "        print(\"  wav2vec2 outperforms clinical baseline\")\n",
    "    elif diff < -0.05:\n",
    "        print(\"  clinical baseline outperforms wav2vec2\")\n",
    "    else:\n",
    "        print(\"  comparable performance\")\n",
    "else:\n",
    "    print(\"clinical baseline results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIWoHalptZEg"
   },
   "source": [
    "## 7. save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOiBOGSptZEg"
   },
   "outputs": [],
   "source": [
    "# save comprehensive results\n",
    "results = {\n",
    "    'experiment': experiment_name,\n",
    "    'config': config,\n",
    "    'device': device,\n",
    "    'n_folds': len(fold_results),\n",
    "    'n_subjects': n_subjects,\n",
    "    'n_samples': len(dataset),\n",
    "\n",
    "    'overall_metrics': {\n",
    "        'accuracy': float(overall_accuracy),\n",
    "        'precision': float(overall_precision),\n",
    "        'recall': float(overall_recall),\n",
    "        'f1': float(overall_f1),\n",
    "        'auc': float(overall_auc)\n",
    "    },\n",
    "\n",
    "    'per_fold_stats': {\n",
    "        'mean_accuracy': float(mean_accuracy),\n",
    "        'std_accuracy': float(std_accuracy),\n",
    "        'min_accuracy': float(min(fold_accuracies)),\n",
    "        'max_accuracy': float(max(fold_accuracies))\n",
    "    },\n",
    "\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "\n",
    "    'fold_results': [\n",
    "        {k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in r.items()}\n",
    "        for r in fold_results\n",
    "    ],\n",
    "\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'elapsed_time': str(elapsed)\n",
    "}\n",
    "\n",
    "# save to json\n",
    "results_path = output_dir / 'wav2vec2_loso_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# save subject-level results\n",
    "subject_path = output_dir / 'wav2vec2_subject_accuracy.csv'\n",
    "subject_df.to_csv(subject_path, index=False)\n",
    "\n",
    "# also save to main results folder for easy access\n",
    "main_results_path = project_root / 'results' / 'wav2vec2_loso_results.json'\n",
    "with open(main_results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"results saved to:\")\n",
    "print(f\"  {results_path}\")\n",
    "print(f\"  {subject_path}\")\n",
    "print(f\"  {main_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2QrqiitZEh"
   },
   "source": [
    "## 8. visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2971VCstZEh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# publication-quality style with latex and times new roman\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': False, # Changed from True to False\n",
    "    #'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{amssymb}', # Removed as not needed when usetex is False\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times', 'Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 11,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'legend.fontsize': 9,\n",
    "    'legend.frameon': True,\n",
    "    'legend.fancybox': False,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1,\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# fold accuracy distribution\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, len(fold_accuracies) + 1), fold_accuracies, color='#3498db', alpha=0.8, edgecolor='white')\n",
    "ax1.axhline(y=mean_accuracy, color='#e74c3c', linestyle='--', linewidth=2,\n",
    "            label=rf'Mean: {mean_accuracy:.1%}')\n",
    "ax1.set_xlabel(r'Fold')\n",
    "ax1.set_ylabel(r'Accuracy')\n",
    "ax1.set_title(r'Per-Fold Accuracy (LOSO CV)')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# confusion matrix\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "            xticklabels=[r'HC', r'PD'], yticklabels=[r'HC', r'PD'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax2.set_xlabel(r'Predicted')\n",
    "ax2.set_ylabel(r'Actual')\n",
    "ax2.set_title(rf'Confusion Matrix (Accuracy: {overall_accuracy:.1%})')\n",
    "\n",
    "# subject accuracy histogram\n",
    "ax3 = axes[2]\n",
    "ax3.hist(subject_accuracies, bins=10, color='#3498db', alpha=0.8, edgecolor='white')\n",
    "ax3.axvline(x=np.mean(subject_accuracies), color='#e74c3c', linestyle='--', linewidth=2,\n",
    "            label=rf'Mean: {np.mean(subject_accuracies):.1%}')\n",
    "ax3.set_xlabel(r'Accuracy')\n",
    "ax3.set_ylabel(r'Count')\n",
    "ax3.set_title(r'Per-Subject Accuracy Distribution')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save to experiment dir\n",
    "plt.savefig(output_dir / 'wav2vec2_results_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'wav2vec2_results_summary.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# also save to main figures folder\n",
    "main_fig_dir = project_root / 'results' / 'figures'\n",
    "main_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"figures saved to {output_dir} and {main_fig_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "c6d04213"
   },
   "source": [
    "# Install cm-super for additional LaTeX fonts, which often includes type1ec.sty\n",
    "!apt-get install -y cm-super\n",
    "\n",
    "# Update TeX Live filename database and font maps to recognize new packages\n",
    "!texhash\n",
    "!updmap-sys --force\n",
    "!fmtutil-sys --all"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "e17eb635"
   },
   "source": [
    "# Install TeX Live packages for LaTeX rendering in matplotlib\n",
    "!apt-get update\n",
    "!apt-get install -y texlive-latex-extra texlive-fonts-recommended dvipng"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6d1769b8"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# publication-quality style with latex and times new roman\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True, # Revert to True to use LaTeX\n",
    "    'text.latex.preamble': r'\\usepackage{amsmath}\\usepackage{amssymb}', # Re-add LaTeX preamble\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times', 'Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 11,\n",
    "    'axes.labelsize': 10,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'legend.fontsize': 9,\n",
    "    'legend.frameon': True,\n",
    "    'legend.fancybox': False,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1,\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# fold accuracy distribution\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(1, len(fold_accuracies) + 1), fold_accuracies, color='#3498db', alpha=0.8, edgecolor='white')\n",
    "ax1.axhline(y=mean_accuracy, color='#e74c3c', linestyle='--', linewidth=2,\n",
    "            label=rf'Mean: {mean_accuracy:.1%}')\n",
    "ax1.set_xlabel(r'Fold')\n",
    "ax1.set_ylabel(r'Accuracy')\n",
    "ax1.set_title(r'Per-Fold Accuracy (LOSO CV)')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# confusion matrix\n",
    "ax2 = axes[1]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "            xticklabels=[r'HC', r'PD'], yticklabels=[r'HC', r'PD'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "ax2.set_xlabel(r'Predicted')\n",
    "ax2.set_ylabel(r'Actual')\n",
    "ax2.set_title(rf'Confusion Matrix (Accuracy: {overall_accuracy:.1%})')\n",
    "\n",
    "# subject accuracy histogram\n",
    "ax3 = axes[2]\n",
    "ax3.hist(subject_accuracies, bins=10, color='#3498db', alpha=0.8, edgecolor='white')\n",
    "ax3.axvline(x=np.mean(subject_accuracies), color='#e74c3c', linestyle='--', linewidth=2,\n",
    "            label=rf'Mean: {np.mean(subject_accuracies):.1%}')\n",
    "ax3.set_xlabel(r'Accuracy')\n",
    "ax3.set_ylabel(r'Count')\n",
    "ax3.set_title(r'Per-Subject Accuracy Distribution')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# save to experiment dir\n",
    "plt.savefig(output_dir / 'wav2vec2_results_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(output_dir / 'wav2vec2_results_summary.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# also save to main figures folder\n",
    "main_fig_dir = project_root / 'results' / 'figures'\n",
    "main_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(main_fig_dir / 'fig_p3_01_wav2vec2_loso_summary.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"figures saved to {output_dir} and {main_fig_dir}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2cNHUv5tZEh"
   },
   "outputs": [],
   "source": [
    "# comparison bar chart with clinical baseline\n",
    "if baseline_path.exists():\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    models = [r'Clinical Baseline' + '\\n' + r'(SVM, 17 features)',\n",
    "              r'Wav2Vec2' + '\\n' + r'(Fine-tuned)']\n",
    "    accuracies = [clinical_acc, overall_accuracy]\n",
    "    stds = [clinical_std, std_accuracy]\n",
    "    colors = ['#2ecc71', '#3498db']\n",
    "\n",
    "    bars = ax.bar(models, accuracies, yerr=stds, capsize=8, color=colors, alpha=0.8,\n",
    "                  edgecolor='white', linewidth=1.5)\n",
    "\n",
    "    for bar, acc, std in zip(bars, accuracies, stds):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.02,\n",
    "                rf'{acc:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "    ax.set_ylabel(r'Accuracy')\n",
    "    ax.set_title(r'Model Comparison: LOSO Cross-Validation')\n",
    "    ax.set_ylim(0, 1.15)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5, label=r'Chance Level')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save figures\n",
    "    plt.savefig(output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'model_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    main_fig_dir = project_root / 'results' / 'figures'\n",
    "    plt.savefig(main_fig_dir / 'fig_p3_02_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(main_fig_dir / 'fig_p3_02_model_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"comparison figures saved\")\n",
    "else:\n",
    "    print(\"clinical baseline results not found - skipping comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yy_YP7H9tZEh"
   },
   "source": [
    "## 9. summary and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HOogyVUtZEh"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3 COMPLETE: WAV2VEC2 FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nmodel: {config['model_name']}\")\n",
    "print(f\"device: {device}\")\n",
    "print(f\"loso cv folds: {len(fold_results)}\")\n",
    "print(f\"training time: {elapsed}\")\n",
    "\n",
    "print(f\"\\nresults:\")\n",
    "print(f\"  accuracy: {overall_accuracy:.1%}\")\n",
    "print(f\"  precision: {overall_precision:.3f}\")\n",
    "print(f\"  recall: {overall_recall:.3f}\")\n",
    "print(f\"  f1 score: {overall_f1:.3f}\")\n",
    "print(f\"  auc-roc: {overall_auc:.3f}\")\n",
    "\n",
    "print(f\"\\nper-fold accuracy: {mean_accuracy:.1%} ± {std_accuracy:.1%}\")\n",
    "\n",
    "print(f\"\\nnext steps:\")\n",
    "print(f\"  1. phase 4: activation extraction (notebook 04)\")\n",
    "print(f\"  2. phase 5: probing experiments (notebook 05)\")\n",
    "print(f\"  3. phase 6: activation patching (notebook 06)\")\n",
    "\n",
    "print(f\"\\noutputs saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j-oT8SVtZEh"
   },
   "source": [
    "## 10. train final model for activation extraction\n",
    "\n",
    "train a single model on all data for use in probing and patching experiments.\n",
    "this model will be used to extract activations in phase 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir4mBU1dtZEh"
   },
   "outputs": [],
   "source": "# train final model on 80% of data (hold out 20% for testing)\n# uses the same corrected approach as LOSO CV training\n\nprint(\"=\" * 60)\nprint(\"TRAINING FINAL MODEL FOR ACTIVATION EXTRACTION\")\nprint(\"=\" * 60)\n\ntrain_subset, _, test_subset = dataset.get_subject_split(\n    test_size=0.2,\n    val_size=0.0,\n    random_state=config['random_seed']\n)\n\nprint(f\"training final model...\")\nprint(f\"  train samples: {len(train_subset)}\")\nprint(f\"  test samples: {len(test_subset)}\")\n\n# compute class weights\ntrain_indices = train_subset.indices\ntrain_labels = np.array([dataset.samples[i]['label'] for i in train_indices])\nclass_weights = compute_class_weights(train_labels)\nprint(f\"  class weights: HC={class_weights[0]:.3f}, PD={class_weights[1]:.3f}\")\n\n# create model with stability settings\nfinal_model = create_model(config, device)\n\n# feature extractor - no normalization, wav2vec2 handles it internally\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n    config['model_name'],\n    return_attention_mask=False\n)\nmax_length = int(config['max_duration'] * config['target_sr'])\ncollate_fn = create_collate_fn(feature_extractor, max_length)\n\n# dataloaders\ntrain_loader = DataLoader(\n    train_subset,\n    batch_size=config['batch_size'],\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=(device == 'cuda'),\n    drop_last=True\n)\n\ntest_loader = DataLoader(\n    test_subset,\n    batch_size=config['batch_size'] * 2,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=0,\n    pin_memory=(device == 'cuda')\n)\n\n# optimizer and scheduler\noptimizer = AdamW(\n    [p for p in final_model.parameters() if p.requires_grad],\n    lr=config['learning_rate'],\n    weight_decay=config['weight_decay'],\n    eps=1e-8\n)\n\nsteps_per_epoch = max(1, len(train_loader) // config['gradient_accumulation_steps'])\ntotal_steps = steps_per_epoch * config['num_epochs']\nwarmup_steps = int(total_steps * config['warmup_ratio'])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# gradient scaler for fp16 (disabled in config for stability)\nif config.get('fp16', False) and device == 'cuda':\n    scaler = torch.cuda.amp.GradScaler(\n        init_scale=2**10,\n        growth_interval=100\n    )\nelse:\n    scaler = None\n\nprint(f\"  epochs: {config['num_epochs']}\")\nprint(f\"  learning rate: {config['learning_rate']}\")\nprint(f\"  warmup steps: {warmup_steps}/{total_steps}\")\nprint()\n\n# training loop with early stopping\nbest_acc = 0\nbest_loss = float('inf')\npatience_counter = 0\ncheckpoint_path = None\n\nfor epoch in range(config['num_epochs']):\n    train_loss = train_epoch(\n        final_model, train_loader, optimizer, scheduler, scaler,\n        device, config, epoch_num=epoch + 1, verbose=True\n    )\n\n    # check for NaN\n    if math.isnan(train_loss):\n        print(f\"  WARNING: NaN loss at epoch {epoch + 1}\")\n        print(f\"  this should NOT happen with fixed collate function - check audio data!\")\n        break\n\n    test_metrics = evaluate(final_model, test_loader, device, config)\n\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"  epoch {epoch+1}/{config['num_epochs']}: loss={train_loss:.4f}, \"\n          f\"test_loss={test_metrics['loss']:.4f}, acc={test_metrics['accuracy']:.1%}, lr={current_lr:.2e}\")\n\n    # save best model\n    if test_metrics['accuracy'] > best_acc:\n        best_acc = test_metrics['accuracy']\n        best_loss = test_metrics['loss']\n        patience_counter = 0\n        # save checkpoint\n        checkpoint_path = output_dir / 'final_model'\n        checkpoint_path.mkdir(parents=True, exist_ok=True)\n        final_model.save_pretrained(checkpoint_path)\n        feature_extractor.save_pretrained(checkpoint_path)\n        print(f\"    -> saved best model (acc: {best_acc:.1%})\")\n    else:\n        patience_counter += 1\n        if patience_counter >= config['early_stopping_patience']:\n            print(f\"  early stopping at epoch {epoch + 1}\")\n            break\n\n    # cleanup\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n    gc.collect()\n\nprint()\nprint(\"=\" * 60)\nprint(\"FINAL MODEL TRAINING COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"  best test accuracy: {best_acc:.1%}\")\nprint(f\"  best test loss: {best_loss:.4f}\")\nif checkpoint_path:\n    print(f\"  model saved to: {checkpoint_path}\")\n\n    # also copy to main results folder for easy access\n    main_model_path = project_root / 'results' / 'final_model'\n    main_model_path.mkdir(parents=True, exist_ok=True)\n    final_model.save_pretrained(main_model_path)\n    feature_extractor.save_pretrained(main_model_path)\n    print(f\"  also saved to: {main_model_path}\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}