{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff19821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /Volumes/usb drive/pd-interpretability\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from src.data.datasets import ItalianPVSDataset\n",
    "from src.features.clinical import ClinicalFeatureExtractor, get_clinical_feature_names\n",
    "\n",
    "print(f\"project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "DATA_ROOT = project_root / 'data'\n",
    "RAW_DATA = DATA_ROOT / 'raw'\n",
    "CLINICAL_FEATURES_DIR = DATA_ROOT / 'clinical_features'\n",
    "RESULTS_DIR = project_root / 'results'\n",
    "\n",
    "CLINICAL_FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb20ca",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b10596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded: 831 samples\n",
      "subjects: 61\n",
      "class distribution: 394 hc, 437 pd\n"
     ]
    }
   ],
   "source": [
    "italian_pvs_path = RAW_DATA / 'italian_pvs'\n",
    "\n",
    "dataset = ItalianPVSDataset(\n",
    "    root_dir=str(italian_pvs_path),\n",
    "    task=None,\n",
    "    max_duration=10.0\n",
    ")\n",
    "\n",
    "print(f\"dataset loaded: {len(dataset)} samples\")\n",
    "\n",
    "# get sample info\n",
    "n_subjects = len(set(s['subject_id'] for s in dataset.samples))\n",
    "labels = [s['label'] for s in dataset.samples]\n",
    "n_pd = sum(labels)\n",
    "n_hc = len(labels) - n_pd\n",
    "\n",
    "print(f\"subjects: {n_subjects}\")\n",
    "print(f\"class distribution: {n_hc} hc, {n_pd} pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74447002",
   "metadata": {},
   "source": [
    "## 2. extract clinical features for all samples\n",
    "\n",
    "extracting jitter, shimmer, hnr, and f0 statistics using parselmouth (praat interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad74f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting clinical features for all samples...\n",
      "this may take several minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting features: 100%|██████████| 831/831 [05:22<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "extracted features for 0 samples\n",
      "failed: 831 samples\n",
      "first 5 failures: [(0, \"module 'parselmouth' has no attribute 'UNDEFINED'\"), (1, \"module 'parselmouth' has no attribute 'UNDEFINED'\"), (2, \"module 'parselmouth' has no attribute 'UNDEFINED'\"), (3, \"module 'parselmouth' has no attribute 'UNDEFINED'\"), (4, \"module 'parselmouth' has no attribute 'UNDEFINED'\")]\n",
      "saved to /Volumes/usb drive/pd-interpretability/data/clinical_features/italian_pvs_features.csv\n"
     ]
    }
   ],
   "source": [
    "# check if features already extracted\n",
    "features_csv_path = CLINICAL_FEATURES_DIR / 'italian_pvs_features.csv'\n",
    "\n",
    "if features_csv_path.exists():\n",
    "    print(f\"loading existing features from {features_csv_path}\")\n",
    "    features_df = pd.read_csv(features_csv_path)\n",
    "    print(f\"loaded {len(features_df)} samples\")\n",
    "else:\n",
    "    print(\"extracting clinical features for all samples...\")\n",
    "    print(\"this may take several minutes.\")\n",
    "    \n",
    "    extractor = ClinicalFeatureExtractor(\n",
    "        f0_min=75.0,\n",
    "        f0_max=600.0\n",
    "    )\n",
    "    \n",
    "    features_list = []\n",
    "    failed_samples = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset)), desc=\"extracting features\"):\n",
    "        sample = dataset.samples[i]\n",
    "        \n",
    "        try:\n",
    "            features = extractor.extract(str(sample['path']))\n",
    "            \n",
    "            features['sample_idx'] = i\n",
    "            features['path'] = str(sample['path'])\n",
    "            features['subject_id'] = sample['subject_id']\n",
    "            features['label'] = sample['label']\n",
    "            features['diagnosis'] = 'pd' if sample['label'] == 1 else 'hc'\n",
    "            \n",
    "            features_list.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_samples.append((i, str(e)))\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    print(f\"\\nextracted features for {len(features_df)} samples\")\n",
    "    print(f\"failed: {len(failed_samples)} samples\")\n",
    "    \n",
    "    if failed_samples:\n",
    "        print(f\"first 5 failures: {failed_samples[:5]}\")\n",
    "    \n",
    "    # save to csv\n",
    "    features_df.to_csv(features_csv_path, index=False)\n",
    "    print(f\"saved to {features_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de53c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature summary\n",
    "clinical_feature_cols = [\n",
    "    'f0_mean', 'f0_std', 'f0_min', 'f0_max', 'f0_range',\n",
    "    'jitter_local', 'jitter_local_abs', 'jitter_rap', 'jitter_ppq5', 'jitter_ddp',\n",
    "    'shimmer_local', 'shimmer_local_db', 'shimmer_apq3', 'shimmer_apq5', 'shimmer_apq11', 'shimmer_dda',\n",
    "    'hnr', 'nhr'\n",
    "]\n",
    "\n",
    "available_features = [f for f in clinical_feature_cols if f in features_df.columns]\n",
    "\n",
    "print(f\"available clinical features: {len(available_features)}\")\n",
    "print(features_df[available_features].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(\"missing values per feature:\")\n",
    "missing = features_df[available_features].isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "# drop rows with any missing clinical features\n",
    "features_clean = features_df.dropna(subset=available_features)\n",
    "print(f\"\\nsamples after removing missing: {len(features_clean)} / {len(features_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f0e01a",
   "metadata": {},
   "source": [
    "## 3. prepare data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d396f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare feature matrix and labels\n",
    "X = features_clean[available_features].values\n",
    "y = features_clean['label'].values\n",
    "groups = features_clean['subject_id'].values\n",
    "\n",
    "print(f\"feature matrix shape: {X.shape}\")\n",
    "print(f\"labels shape: {y.shape}\")\n",
    "print(f\"unique subjects: {len(np.unique(groups))}\")\n",
    "print(f\"class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08003b74",
   "metadata": {},
   "source": [
    "## 4. leave-one-subject-out cross-validation\n",
    "\n",
    "loso cv is the gold standard for medical ml with limited subjects.\n",
    "it ensures the model is evaluated on completely unseen subjects,\n",
    "preventing any data leakage between train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d41fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "print(f\"number of folds (subjects): {logo.get_n_splits(X, y, groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm baseline with rbf kernel\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale', random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print(\"running svm with loso cv...\")\n",
    "svm_scores = cross_val_score(svm_pipeline, X, y, cv=logo, groups=groups, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nsvm accuracy: {svm_scores.mean():.3f} +/- {svm_scores.std():.3f}\")\n",
    "print(f\"min: {svm_scores.min():.3f}, max: {svm_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77a2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest baseline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"running random forest with loso cv...\")\n",
    "rf_scores = cross_val_score(rf_pipeline, X, y, cv=logo, groups=groups, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nrandom forest accuracy: {rf_scores.mean():.3f} +/- {rf_scores.std():.3f}\")\n",
    "print(f\"min: {rf_scores.min():.3f}, max: {rf_scores.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72645a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed classification report using cross_val_predict\n",
    "print(\"generating detailed metrics using best model...\")\n",
    "\n",
    "best_model = svm_pipeline if svm_scores.mean() > rf_scores.mean() else rf_pipeline\n",
    "best_name = 'svm' if svm_scores.mean() > rf_scores.mean() else 'random forest'\n",
    "\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=logo, groups=groups)\n",
    "\n",
    "print(f\"\\n{best_name} classification report:\")\n",
    "print(classification_report(y, y_pred, target_names=['healthy', 'parkinson']))\n",
    "\n",
    "print(\"\\nconfusion matrix:\")\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(f\"           predicted\")\n",
    "print(f\"            hc    pd\")\n",
    "print(f\"actual hc  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       pd  {cm[1,0]:4d}  {cm[1,1]:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c8674",
   "metadata": {},
   "source": [
    "## 5. per-subject analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze per-subject accuracy\n",
    "unique_subjects = np.unique(groups)\n",
    "subject_results = []\n",
    "\n",
    "for subject in unique_subjects:\n",
    "    mask = groups == subject\n",
    "    subject_true = y[mask]\n",
    "    subject_pred = y_pred[mask]\n",
    "    \n",
    "    subject_acc = accuracy_score(subject_true, subject_pred)\n",
    "    subject_label = 'pd' if subject_true[0] == 1 else 'hc'\n",
    "    n_samples = mask.sum()\n",
    "    \n",
    "    subject_results.append({\n",
    "        'subject_id': subject,\n",
    "        'diagnosis': subject_label,\n",
    "        'n_samples': n_samples,\n",
    "        'accuracy': subject_acc,\n",
    "        'correct': int(subject_acc * n_samples),\n",
    "        'total': n_samples\n",
    "    })\n",
    "\n",
    "subject_df = pd.DataFrame(subject_results)\n",
    "\n",
    "print(\"per-subject accuracy distribution:\")\n",
    "print(f\"  mean: {subject_df['accuracy'].mean():.3f}\")\n",
    "print(f\"  median: {subject_df['accuracy'].median():.3f}\")\n",
    "print(f\"  subjects with 100% accuracy: {(subject_df['accuracy'] == 1.0).sum()}\")\n",
    "print(f\"  subjects with 0% accuracy: {(subject_df['accuracy'] == 0.0).sum()}\")\n",
    "\n",
    "print(\"\\naccuracy by diagnosis:\")\n",
    "print(subject_df.groupby('diagnosis')['accuracy'].agg(['mean', 'std', 'min', 'max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9aae8e",
   "metadata": {},
   "source": [
    "## 6. feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1714d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train rf on full data to get feature importance\n",
    "rf_full = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "rf_full.fit(X_scaled, y)\n",
    "\n",
    "importances = rf_full.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"feature importance ranking:\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbb707",
   "metadata": {},
   "source": [
    "## 7. statistical comparison: pd vs hc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stat_results = []\n",
    "\n",
    "for feature in available_features:\n",
    "    hc_values = features_clean[features_clean['label'] == 0][feature]\n",
    "    pd_values = features_clean[features_clean['label'] == 1][feature]\n",
    "    \n",
    "    t_stat, p_val = stats.ttest_ind(hc_values, pd_values)\n",
    "    \n",
    "    # cohen's d effect size\n",
    "    pooled_std = np.sqrt((hc_values.std()**2 + pd_values.std()**2) / 2)\n",
    "    cohens_d = (pd_values.mean() - hc_values.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    stat_results.append({\n",
    "        'feature': feature,\n",
    "        'hc_mean': hc_values.mean(),\n",
    "        'hc_std': hc_values.std(),\n",
    "        'pd_mean': pd_values.mean(),\n",
    "        'pd_std': pd_values.std(),\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_val,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_val < 0.05\n",
    "    })\n",
    "\n",
    "stat_df = pd.DataFrame(stat_results)\n",
    "\n",
    "print(\"statistical comparison (pd vs hc):\")\n",
    "print(stat_df[['feature', 'hc_mean', 'pd_mean', 'p_value', 'cohens_d', 'significant']].to_string(index=False))\n",
    "\n",
    "n_sig = stat_df['significant'].sum()\n",
    "print(f\"\\nsignificant features (p < 0.05): {n_sig} / {len(stat_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8fb28",
   "metadata": {},
   "source": [
    "## 8. save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "baseline_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset': 'italian_pvs',\n",
    "    'n_samples': len(features_clean),\n",
    "    'n_subjects': len(np.unique(groups)),\n",
    "    'n_features': len(available_features),\n",
    "    'features_used': available_features,\n",
    "    'cv_method': 'leave_one_subject_out',\n",
    "    'n_folds': logo.get_n_splits(X, y, groups),\n",
    "    'svm': {\n",
    "        'accuracy_mean': float(svm_scores.mean()),\n",
    "        'accuracy_std': float(svm_scores.std()),\n",
    "        'accuracy_min': float(svm_scores.min()),\n",
    "        'accuracy_max': float(svm_scores.max()),\n",
    "        'per_fold_scores': svm_scores.tolist()\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'accuracy_mean': float(rf_scores.mean()),\n",
    "        'accuracy_std': float(rf_scores.std()),\n",
    "        'accuracy_min': float(rf_scores.min()),\n",
    "        'accuracy_max': float(rf_scores.max()),\n",
    "        'per_fold_scores': rf_scores.tolist()\n",
    "    },\n",
    "    'best_model': best_name,\n",
    "    'feature_importance': importance_df.to_dict('records'),\n",
    "    'statistical_comparison': stat_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# save results\n",
    "baseline_path = RESULTS_DIR / 'clinical_baseline_results.json'\n",
    "with open(baseline_path, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(f\"baseline results saved to {baseline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subject-level results\n",
    "subject_results_path = RESULTS_DIR / 'clinical_baseline_subjects.csv'\n",
    "subject_df.to_csv(subject_results_path, index=False)\n",
    "print(f\"subject results saved to {subject_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8245c7",
   "metadata": {},
   "source": [
    "## 9. summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: CLINICAL BASELINE - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\ndataset: italian pvs\")\n",
    "print(f\"samples: {len(features_clean)}\")\n",
    "print(f\"subjects: {len(np.unique(groups))}\")\n",
    "print(f\"features: {len(available_features)} clinical biomarkers\")\n",
    "print(f\"\\ncross-validation: leave-one-subject-out ({logo.get_n_splits(X, y, groups)} folds)\")\n",
    "print(f\"\\nBASELINE RESULTS:\")\n",
    "print(f\"  svm (rbf):       {svm_scores.mean()*100:.1f}% +/- {svm_scores.std()*100:.1f}%\")\n",
    "print(f\"  random forest:   {rf_scores.mean()*100:.1f}% +/- {rf_scores.std()*100:.1f}%\")\n",
    "print(f\"\\ntarget range: 70-85%\")\n",
    "\n",
    "best_acc = max(svm_scores.mean(), rf_scores.mean()) * 100\n",
    "if 70 <= best_acc <= 85:\n",
    "    print(f\"status: WITHIN TARGET RANGE\")\n",
    "elif best_acc > 85:\n",
    "    print(f\"status: ABOVE TARGET (excellent clinical features)\")\n",
    "else:\n",
    "    print(f\"status: BELOW TARGET (may need feature engineering)\")\n",
    "\n",
    "print(f\"\\ntop 5 most important features:\")\n",
    "for i, row in importance_df.head(5).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\nsignificant features (p < 0.05): {n_sig} / {len(stat_df)}\")\n",
    "print(\"\\nphase 2 complete. ready to proceed to phase 3 (wav2vec2 fine-tuning).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
