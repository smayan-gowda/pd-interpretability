{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "DATA_DIR = project_root / 'data'\n",
    "RESULTS_DIR = project_root / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f509a1",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define result paths\n",
    "result_paths = {\n",
    "    'probing': RESULTS_DIR / 'probing' / 'probing_results.json',\n",
    "    'patching': RESULTS_DIR / 'patching' / 'patching_results.json',\n",
    "    'cross_dataset': RESULTS_DIR / 'generalization' / 'cross_dataset_results.json'\n",
    "}\n",
    "\n",
    "# load available results\n",
    "results = {}\n",
    "\n",
    "for name, path in result_paths.items():\n",
    "    if path.exists():\n",
    "        with open(path, 'r') as f:\n",
    "            results[name] = json.load(f)\n",
    "        print(f\"loaded {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"not found: {path}\")\n",
    "        results[name] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d158c33",
   "metadata": {},
   "source": [
    "## 2. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import ResultsAggregator, HypothesisTester\n",
    "\n",
    "# create aggregator\n",
    "aggregator = ResultsAggregator(\n",
    "    experiment_name='pd_interpretability_main',\n",
    "    output_dir=str(RESULTS_DIR)\n",
    ")\n",
    "\n",
    "# add probing results\n",
    "if results['probing'] and 'layerwise_probing' in results['probing']:\n",
    "    probing_data = {}\n",
    "    for layer_str, data in results['probing']['layerwise_probing'].items():\n",
    "        probing_data[int(layer_str)] = data\n",
    "    aggregator.add_probing_results(probing_data)\n",
    "    print(f\"added probing results for {len(probing_data)} layers\")\n",
    "\n",
    "# add patching results\n",
    "if results['patching'] and 'layer_patching' in results['patching']:\n",
    "    patching_data = {}\n",
    "    for layer_str, data in results['patching']['layer_patching'].items():\n",
    "        patching_data[int(layer_str)] = data\n",
    "    aggregator.add_patching_results(patching_data)\n",
    "    print(f\"added patching results for {len(patching_data)} layers\")\n",
    "\n",
    "# add clinical probing if available\n",
    "if results['probing'] and 'clinical_probing' in results['probing']:\n",
    "    aggregator.add_clinical_probing_results(results['probing']['clinical_probing'])\n",
    "    print(\"added clinical probing results\")\n",
    "\n",
    "# add cross-dataset results\n",
    "if results['cross_dataset']:\n",
    "    aggregator.add_cross_dataset_results(results['cross_dataset'])\n",
    "    print(\"added cross-dataset results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dace37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary report\n",
    "print(aggregator.generate_summary_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17fb2ca",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hypothesis tester\n",
    "tester = HypothesisTester(aggregator)\n",
    "\n",
    "# run all hypothesis tests\n",
    "hypothesis_results = tester.run_all_hypothesis_tests()\n",
    "\n",
    "# generate report\n",
    "print(tester.generate_hypothesis_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da819502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed hypothesis 1 analysis\n",
    "h1 = hypothesis_results['hypothesis_1']\n",
    "\n",
    "print(\"HYPOTHESIS 1: CLINICAL FEATURE ENCODING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nphonatory features (jitter, shimmer):\")\n",
    "for feat, data in h1.get('phonatory', {}).items():\n",
    "    print(f\"  {feat}: layer {data['best_layer']} (r² = {data['r2']:.3f})\")\n",
    "\n",
    "print(\"\\nprosodic features (f0, hnr):\")\n",
    "for feat, data in h1.get('prosodic', {}).items():\n",
    "    print(f\"  {feat}: layer {data['best_layer']} (r² = {data['r2']:.3f})\")\n",
    "\n",
    "print(f\"\\nprediction: phonatory in early layers (2-4)\")\n",
    "print(f\"  actual mean layer: {h1.get('phonatory_mean_layer', 'N/A')}\")\n",
    "print(f\"  fraction in early: {h1.get('phonatory_early_fraction', 0):.1%}\")\n",
    "\n",
    "print(f\"\\nprediction: prosodic in middle layers (5-8)\")\n",
    "print(f\"  actual mean layer: {h1.get('prosodic_mean_layer', 'N/A')}\")\n",
    "print(f\"  fraction in middle: {h1.get('prosodic_middle_fraction', 0):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed hypothesis 2 analysis\n",
    "h2 = hypothesis_results['hypothesis_2']\n",
    "\n",
    "print(\"HYPOTHESIS 2: CAUSAL FEATURE DEPENDENCY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\ncausal layers (patching recovery > 0.1): {h2['causal_layers']}\")\n",
    "\n",
    "if h2['probing_patching_correlation']:\n",
    "    corr = h2['probing_patching_correlation']\n",
    "    print(f\"\\nprobing-patching correlation:\")\n",
    "    print(f\"  spearman r: {corr['spearman_r']:.3f}\")\n",
    "    print(f\"  p-value: {corr['spearman_p']:.4f}\")\n",
    "    print(f\"  n layers: {corr['n_layers']}\")\n",
    "\n",
    "print(f\"\\nverdict: {'SUPPORTED' if h2['supported'] else 'NOT SUPPORTED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b33db",
   "metadata": {},
   "source": [
    "## 4. Generate Publication Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fabfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import FigureGenerator, set_publication_style\n",
    "\n",
    "# set publication style\n",
    "set_publication_style()\n",
    "\n",
    "# create figure generator\n",
    "fig_gen = FigureGenerator(output_dir=str(FIGURES_DIR))\n",
    "\n",
    "print(f\"figures will be saved to: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure 1: overview\n",
    "if aggregator.probing_results and aggregator.patching_results:\n",
    "    probing_dict = {k: {'mean': v.mean_score, 'std': v.std_score} \n",
    "                    for k, v in aggregator.probing_results.items()}\n",
    "    patching_dict = {k: {'mean_recovery': v.mean_recovery, 'std_recovery': v.std_recovery}\n",
    "                     for k, v in aggregator.patching_results.items()}\n",
    "    \n",
    "    fig1 = fig_gen.figure_1_overview(\n",
    "        probing_dict,\n",
    "        patching_dict,\n",
    "        model_accuracy=results['probing'].get('model_accuracy', 0.85) if results['probing'] else 0.85\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"insufficient data for figure 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca075044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure 2: clinical encoding heatmap\n",
    "if aggregator.clinical_results:\n",
    "    clinical_dict = {\n",
    "        feat: {\n",
    "            layer: {'mean': r.r2_score, 'std': r.std_score}\n",
    "            for layer, r in layer_results.items()\n",
    "        }\n",
    "        for feat, layer_results in aggregator.clinical_results.items()\n",
    "    }\n",
    "    \n",
    "    fig2 = fig_gen.figure_2_clinical_encoding(clinical_dict)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"no clinical results for figure 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b109091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure 3: hypothesis summary\n",
    "fig3 = fig_gen.figure_3_hypothesis_summary(hypothesis_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc7d43",
   "metadata": {},
   "source": [
    "## 5. Save Results and Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57228fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save aggregated results\n",
    "results_file = aggregator.save('aggregated_results.json')\n",
    "print(f\"saved results to: {results_file}\")\n",
    "\n",
    "# save text report\n",
    "report_file = aggregator.save_report('experiment_report.txt')\n",
    "print(f\"saved report to: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2421290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hypothesis test results\n",
    "hypothesis_file = RESULTS_DIR / 'hypothesis_results.json'\n",
    "\n",
    "with open(hypothesis_file, 'w') as f:\n",
    "    json.dump(hypothesis_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"saved hypothesis results to: {hypothesis_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6140ea54",
   "metadata": {},
   "source": [
    "## 6. Abstract Summary Statistics\n",
    "\n",
    "Key numbers needed for the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d99806",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ABSTRACT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# model performance\n",
    "if results['probing']:\n",
    "    model_acc = results['probing'].get('best_accuracy', 'N/A')\n",
    "    print(f\"\\nmodel classification accuracy: {model_acc}\")\n",
    "\n",
    "# best probing layer\n",
    "if aggregator.probing_results:\n",
    "    best_layer, best_acc = aggregator.get_best_probing_layer()\n",
    "    print(f\"best probing layer: {best_layer} (accuracy = {best_acc:.3f})\")\n",
    "\n",
    "# best patching layer\n",
    "if aggregator.patching_results:\n",
    "    best_patch, best_rec = aggregator.get_best_patching_layer()\n",
    "    print(f\"best patching layer: {best_patch} (recovery = {best_rec:.3f})\")\n",
    "\n",
    "# important layers\n",
    "if aggregator.probing_results or aggregator.patching_results:\n",
    "    important = aggregator.get_important_layers()\n",
    "    print(f\"important layers (both methods): {important['both']}\")\n",
    "\n",
    "# probing-patching correlation\n",
    "if aggregator.probing_results and aggregator.patching_results:\n",
    "    corr = aggregator.compute_probing_patching_correlation()\n",
    "    if corr:\n",
    "        print(f\"probing-patching correlation: r = {corr['spearman_r']:.2f}, p = {corr['spearman_p']:.4f}\")\n",
    "\n",
    "# hypothesis support\n",
    "print(f\"\\nhypothesis support:\")\n",
    "print(f\"  h1 (clinical encoding): {'SUPPORTED' if hypothesis_results['hypothesis_1']['supported'] else 'NOT SUPPORTED'}\")\n",
    "print(f\"  h2 (causal dependency): {'SUPPORTED' if hypothesis_results['hypothesis_2']['supported'] else 'NOT SUPPORTED'}\")\n",
    "print(f\"  h3 (generalization): {'SUPPORTED' if hypothesis_results['hypothesis_3']['supported'] else 'NOT SUPPORTED'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f484f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate abstract text snippet\n",
    "print(\"\\nSUGGESTED ABSTRACT SNIPPET:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if aggregator.probing_results:\n",
    "    best_layer, best_acc = aggregator.get_best_probing_layer()\n",
    "    \n",
    "    snippet = f\"\"\"We found that Parkinson's disease information is primarily encoded \n",
    "in middle transformer layers, with layer {best_layer} achieving {best_acc:.1%} probing accuracy.\n",
    "\"\"\"\n",
    "    \n",
    "    if aggregator.patching_results:\n",
    "        best_patch, best_rec = aggregator.get_best_patching_layer()\n",
    "        snippet += f\"\"\"Activation patching confirmed these layers are causally important, \n",
    "with layer {best_patch} recovering {best_rec:.1%} of the logit difference.\n",
    "\"\"\"\n",
    "    \n",
    "    if aggregator.probing_results and aggregator.patching_results:\n",
    "        corr = aggregator.compute_probing_patching_correlation()\n",
    "        if corr:\n",
    "            snippet += f\"\"\"The strong correlation between probing accuracy and causal importance \n",
    "(r = {corr['spearman_r']:.2f}, p = {corr['spearman_p']:.4f}) supports our hypothesis that \n",
    "clinically-meaningful representations drive the model's predictions.\n",
    "\"\"\"\n",
    "    \n",
    "    print(snippet)\n",
    "else:\n",
    "    print(\"insufficient results for abstract generation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
