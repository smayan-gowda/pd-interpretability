{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ac248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/pd-interpretability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q transformers datasets librosa scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.dpi': 150\n",
    "})\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5434fd7",
   "metadata": {},
   "source": [
    "## 1. Configuration and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "CONFIG = {\n",
    "    'data_path': '/content/drive/MyDrive/pd-interpretability/data',\n",
    "    'activations_path': '/content/drive/MyDrive/pd-interpretability/data/activations',\n",
    "    'output_path': '/content/drive/MyDrive/pd-interpretability/results/probing',\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# create output directory\n",
    "output_path = Path(CONFIG['output_path'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"output directory: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545be6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-extracted activations\n",
    "activations_path = Path(CONFIG['activations_path'])\n",
    "\n",
    "# load activations and metadata\n",
    "activations_file = activations_path / 'activations.npy'\n",
    "metadata_file = activations_path / 'metadata.json'\n",
    "\n",
    "if activations_file.exists():\n",
    "    activations = np.load(activations_file)\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"loaded activations: {activations.shape}\")\n",
    "    print(f\"samples: {metadata.get('n_samples', len(metadata.get('labels', [])))}\")\n",
    "    print(f\"layers: {activations.shape[1]}\")\n",
    "    print(f\"hidden size: {activations.shape[2]}\")\n",
    "else:\n",
    "    print(\"activations not found, need to run extraction first\")\n",
    "    activations = None\n",
    "    metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfc5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract labels and subject ids from metadata\n",
    "if metadata:\n",
    "    labels = np.array(metadata['labels'])\n",
    "    subject_ids = np.array(metadata['subject_ids'])\n",
    "    \n",
    "    print(f\"label distribution: PD={sum(labels==1)}, HC={sum(labels==0)}\")\n",
    "    print(f\"unique subjects: {len(np.unique(subject_ids))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f97de",
   "metadata": {},
   "source": [
    "## 2. Layer-wise PD Classification Probing\n",
    "\n",
    "For each transformer layer, train a linear classifier to predict PD vs HC.\n",
    "Uses leave-one-subject-out cross-validation for unbiased estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5eb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.probes import LayerwiseProber\n",
    "\n",
    "# run layer-wise probing\n",
    "prober = LayerwiseProber(task='classification', regularization=1.0)\n",
    "\n",
    "print(\"running layer-wise pd classification probing...\")\n",
    "print(\"(using leave-one-subject-out cross-validation)\\n\")\n",
    "\n",
    "probing_results = prober.probe_all_layers(\n",
    "    activations,\n",
    "    labels,\n",
    "    groups=subject_ids\n",
    ")\n",
    "\n",
    "print(\"\\nlayer-wise probing accuracy:\")\n",
    "print(\"-\" * 50)\n",
    "for layer_idx, result in sorted(probing_results.items()):\n",
    "    print(f\"layer {layer_idx:2d}: {result['mean']:.3f} ± {result['std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualization import plot_layerwise_probing\n",
    "\n",
    "# create publication-quality figure\n",
    "fig = plot_layerwise_probing(\n",
    "    probing_results,\n",
    "    title=\"layer-wise pd classification probing accuracy\",\n",
    "    save_path=str(output_path / 'layerwise_probing.png'),\n",
    "    chance_level=0.5\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# identify best layer\n",
    "best_layer = max(probing_results.keys(), key=lambda x: probing_results[x]['mean'])\n",
    "best_acc = probing_results[best_layer]['mean']\n",
    "\n",
    "print(f\"\\nbest probing layer: {best_layer} (accuracy = {best_acc:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical analysis: is best layer significantly better than chance?\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "best_scores = probing_results[best_layer]['scores']\n",
    "t_stat, p_value = ttest_1samp(best_scores, 0.5)\n",
    "\n",
    "print(f\"\\nstatistical test (layer {best_layer} vs chance):\")\n",
    "print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "print(f\"  p-value: {p_value:.4e}\")\n",
    "print(f\"  significant at α=0.05: {p_value < 0.05}\")\n",
    "\n",
    "# effect size\n",
    "cohens_d = (np.mean(best_scores) - 0.5) / np.std(best_scores)\n",
    "print(f\"  cohen's d: {cohens_d:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0629dc",
   "metadata": {},
   "source": [
    "## 3. Clinical Feature Probing\n",
    "\n",
    "Probe each layer for clinical voice biomarkers:\n",
    "- Jitter (pitch perturbation)\n",
    "- Shimmer (amplitude perturbation)\n",
    "- HNR (harmonics-to-noise ratio)\n",
    "- F0 statistics (fundamental frequency)\n",
    "\n",
    "This reveals WHERE clinical features are encoded in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clinical features\n",
    "clinical_path = Path(CONFIG['data_path']) / 'clinical_features' / 'italian_pvs_features.csv'\n",
    "\n",
    "if clinical_path.exists():\n",
    "    clinical_df = pd.read_csv(clinical_path)\n",
    "    print(f\"loaded clinical features: {clinical_df.shape}\")\n",
    "    print(f\"features: {list(clinical_df.columns)}\")\n",
    "else:\n",
    "    print(\"clinical features not found, extracting...\")\n",
    "    clinical_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35bf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features to probe\n",
    "feature_names = [\n",
    "    'jitter_local',\n",
    "    'jitter_rap',\n",
    "    'shimmer_local',\n",
    "    'shimmer_apq3',\n",
    "    'hnr',\n",
    "    'f0_mean',\n",
    "    'f0_std'\n",
    "]\n",
    "\n",
    "# filter to available features\n",
    "if clinical_df is not None:\n",
    "    available_features = [f for f in feature_names if f in clinical_df.columns]\n",
    "    print(f\"probing features: {available_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.probes import MultiFeatureProber\n",
    "\n",
    "if clinical_df is not None and len(available_features) > 0:\n",
    "    # build feature matrix\n",
    "    feature_matrix = clinical_df[available_features].values\n",
    "    \n",
    "    # run multi-feature probing\n",
    "    multi_prober = MultiFeatureProber(\n",
    "        feature_names=available_features,\n",
    "        task='regression',\n",
    "        regularization=1.0\n",
    "    )\n",
    "    \n",
    "    print(\"running clinical feature probing...\\n\")\n",
    "    \n",
    "    clinical_results = multi_prober.probe_all_features(\n",
    "        activations,\n",
    "        feature_matrix,\n",
    "        groups=subject_ids\n",
    "    )\n",
    "    \n",
    "    # print results\n",
    "    for feat_name, layer_results in clinical_results.items():\n",
    "        if layer_results:\n",
    "            best_layer = max(layer_results.keys(), key=lambda x: layer_results[x]['mean'])\n",
    "            best_r2 = layer_results[best_layer]['mean']\n",
    "            print(f\"{feat_name}: best layer = {best_layer}, r² = {best_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualization import plot_clinical_feature_heatmap\n",
    "\n",
    "if clinical_df is not None:\n",
    "    # create heatmap\n",
    "    fig = plot_clinical_feature_heatmap(\n",
    "        clinical_results,\n",
    "        feature_names=available_features,\n",
    "        metric='mean',\n",
    "        title=\"clinical feature encoding across layers (r²)\",\n",
    "        save_path=str(output_path / 'clinical_feature_heatmap.png'),\n",
    "        cmap='viridis',\n",
    "        annot=True\n",
    "    )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4588f",
   "metadata": {},
   "source": [
    "## 4. Control Task Probing\n",
    "\n",
    "Validate that probes learn meaningful features, not spurious correlations.\n",
    "Control tasks (e.g., predicting recording ID) should NOT be predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a744b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.probes import ControlTaskProber\n",
    "\n",
    "# create control labels (should not be predictable)\n",
    "control_labels = {\n",
    "    'segment_index': np.arange(len(labels)),  # should not be predictable\n",
    "    'random_label': np.random.randint(0, 2, len(labels))  # definitely not predictable\n",
    "}\n",
    "\n",
    "# probe best layer with control tasks\n",
    "control_prober = ControlTaskProber(regularization=1.0)\n",
    "\n",
    "best_layer_acts = activations[:, best_layer, :]\n",
    "\n",
    "control_results = control_prober.fit_with_controls(\n",
    "    best_layer_acts,\n",
    "    labels,\n",
    "    control_labels,\n",
    "    groups=subject_ids\n",
    ")\n",
    "\n",
    "print(\"control task analysis (layer {}):\" .format(best_layer))\n",
    "print(\"-\" * 50)\n",
    "print(f\"target (pd/hc): {control_results['target']['mean']:.3f} ± {control_results['target']['std']:.3f}\")\n",
    "\n",
    "for ctrl_name, result in control_results.items():\n",
    "    if ctrl_name != 'target' and 'mean' in result:\n",
    "        print(f\"control ({ctrl_name}): {result['mean']:.3f} ± {result['std']:.3f}\")\n",
    "\n",
    "# compute selectivity\n",
    "selectivity = control_results['target']['mean'] - control_results.get('random_label', {}).get('mean', 0.5)\n",
    "print(f\"\\nselectivity score: {selectivity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7080617",
   "metadata": {},
   "source": [
    "## 5. Probing Dynamics Analysis\n",
    "\n",
    "Analyze how information flows through layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute layer-to-layer improvement\n",
    "layers = sorted(probing_results.keys())\n",
    "accuracies = [probing_results[l]['mean'] for l in layers]\n",
    "\n",
    "# find steepest improvement\n",
    "improvements = np.diff(accuracies)\n",
    "steepest_idx = np.argmax(improvements)\n",
    "\n",
    "print(f\"layer-wise accuracy progression:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (layer, acc) in enumerate(zip(layers, accuracies)):\n",
    "    if i > 0:\n",
    "        delta = acc - accuracies[i-1]\n",
    "        print(f\"layer {layer:2d}: {acc:.3f} (Δ = {delta:+.3f})\")\n",
    "    else:\n",
    "        print(f\"layer {layer:2d}: {acc:.3f}\")\n",
    "\n",
    "print(f\"\\nsteepest improvement: layer {layers[steepest_idx]} → {layers[steepest_idx+1]} ({improvements[steepest_idx]:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize probing dynamics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# left: layer-wise accuracy with gradient\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(layers)))\n",
    "ax1.bar(layers, accuracies, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', linewidth=2, label='chance')\n",
    "ax1.set_xlabel('layer', fontweight='bold')\n",
    "ax1.set_ylabel('probing accuracy', fontweight='bold')\n",
    "ax1.set_title('layer-wise pd classification', fontweight='bold')\n",
    "ax1.set_ylim([0.4, max(accuracies) + 0.1])\n",
    "\n",
    "# right: layer-to-layer improvement\n",
    "ax2 = axes[1]\n",
    "bar_colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "ax2.bar(layers[1:], improvements, color=bar_colors, edgecolor='black', alpha=0.8)\n",
    "ax2.axhline(y=0, color='black', linewidth=1)\n",
    "ax2.set_xlabel('layer', fontweight='bold')\n",
    "ax2.set_ylabel('accuracy change', fontweight='bold')\n",
    "ax2.set_title('layer-to-layer improvement', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'probing_dynamics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f8743",
   "metadata": {},
   "source": [
    "## 6. Hypothesis Testing\n",
    "\n",
    "Test Hypothesis 1: Clinical features are encoded in specific layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis 1 testing\n",
    "print(\"HYPOTHESIS 1 EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nclaim: clinical voice biomarkers are linearly decodable from\")\n",
    "print(\"specific transformer layers, with prosodic features in middle\")\n",
    "print(\"layers (5-8) and phonatory features in early layers (2-4).\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# group features by type\n",
    "phonatory_features = ['jitter_local', 'jitter_rap', 'shimmer_local', 'shimmer_apq3']\n",
    "prosodic_features = ['f0_mean', 'f0_std']\n",
    "\n",
    "if clinical_df is not None:\n",
    "    print(\"\\npeak encoding layers by feature type:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    phonatory_peaks = []\n",
    "    prosodic_peaks = []\n",
    "    \n",
    "    for feat_name, layer_results in clinical_results.items():\n",
    "        if layer_results:\n",
    "            best_layer = max(layer_results.keys(), key=lambda x: layer_results[x]['mean'])\n",
    "            best_r2 = layer_results[best_layer]['mean']\n",
    "            \n",
    "            if feat_name in phonatory_features:\n",
    "                phonatory_peaks.append(best_layer)\n",
    "                print(f\"  {feat_name} (phonatory): layer {best_layer}\")\n",
    "            elif feat_name in prosodic_features:\n",
    "                prosodic_peaks.append(best_layer)\n",
    "                print(f\"  {feat_name} (prosodic): layer {best_layer}\")\n",
    "    \n",
    "    print(\"\\nsummary:\")\n",
    "    if phonatory_peaks:\n",
    "        print(f\"  phonatory features peak at: mean layer {np.mean(phonatory_peaks):.1f}\")\n",
    "        hypothesis_early = np.mean(phonatory_peaks) <= 5\n",
    "        print(f\"  hypothesis (early layers 2-4): {'SUPPORTED' if hypothesis_early else 'NOT SUPPORTED'}\")\n",
    "    \n",
    "    if prosodic_peaks:\n",
    "        print(f\"  prosodic features peak at: mean layer {np.mean(prosodic_peaks):.1f}\")\n",
    "        hypothesis_middle = 5 <= np.mean(prosodic_peaks) <= 8\n",
    "        print(f\"  hypothesis (middle layers 5-8): {'SUPPORTED' if hypothesis_middle else 'NOT SUPPORTED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998578e",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b81982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile all results\n",
    "full_results = {\n",
    "    'config': CONFIG,\n",
    "    'layerwise_probing': {\n",
    "        str(k): {\n",
    "            'mean': v['mean'],\n",
    "            'std': v['std'],\n",
    "            'scores': v['scores']\n",
    "        } for k, v in probing_results.items()\n",
    "    },\n",
    "    'best_layer': int(best_layer),\n",
    "    'best_accuracy': float(best_acc),\n",
    "    'statistical_test': {\n",
    "        't_statistic': float(t_stat),\n",
    "        'p_value': float(p_value),\n",
    "        'cohens_d': float(cohens_d)\n",
    "    }\n",
    "}\n",
    "\n",
    "# add clinical probing if available\n",
    "if clinical_df is not None:\n",
    "    full_results['clinical_probing'] = {\n",
    "        feat: {\n",
    "            str(layer): {\n",
    "                'mean': results['mean'],\n",
    "                'std': results['std']\n",
    "            } for layer, results in layer_results.items()\n",
    "        } for feat, layer_results in clinical_results.items()\n",
    "    }\n",
    "\n",
    "# save to json\n",
    "results_path = output_path / 'probing_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f473c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROBING EXPERIMENTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nsamples analyzed: {len(labels)}\")\n",
    "print(f\"unique subjects: {len(np.unique(subject_ids))}\")\n",
    "print(f\"\\npd classification probing:\")\n",
    "print(f\"  best layer: {best_layer}\")\n",
    "print(f\"  accuracy: {best_acc:.3f} ± {probing_results[best_layer]['std']:.3f}\")\n",
    "print(f\"  significance: p = {p_value:.2e}\")\n",
    "print(f\"  effect size: d = {cohens_d:.2f}\")\n",
    "\n",
    "if clinical_df is not None:\n",
    "    print(f\"\\nclinical feature probing:\")\n",
    "    for feat_name, layer_results in clinical_results.items():\n",
    "        if layer_results:\n",
    "            best_l = max(layer_results.keys(), key=lambda x: layer_results[x]['mean'])\n",
    "            print(f\"  {feat_name}: layer {best_l} (r² = {layer_results[best_l]['mean']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
