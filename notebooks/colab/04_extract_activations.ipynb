{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a02e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# phase 4: activation extraction for mechanistic interpretability\n",
    "# ============================================================\n",
    "# extracts intermediate layer representations from fine-tuned wav2vec2\n",
    "# for probing classifiers and activation patching experiments.\n",
    "#\n",
    "# requirements:\n",
    "# - trained wav2vec2 model checkpoint (from phase 3)\n",
    "# - italian pvs or neurovoz dataset\n",
    "# - gpu recommended (cuda or mps)\n",
    "# ============================================================\n",
    "\n",
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = '/content/drive/MyDrive/pd-interpretability'\n",
    "os.chdir(project_root)\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"phase 4: activation extraction setup\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"project root: {project_root}\")\n",
    "print(f\"working directory: {os.getcwd()}\")\n",
    "\n",
    "# install dependencies\n",
    "print(\"\\ninstalling dependencies...\")\n",
    "!pip install -q -r requirements-colab.txt\n",
    "print(\"dependencies installed ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd400b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Config\n",
    ")\n",
    "\n",
    "from src.data.datasets import ItalianPVSDataset, NeuroVozDataset\n",
    "from src.interpretability.extraction import (\n",
    "    Wav2Vec2ActivationExtractor,\n",
    "    load_activations_memmap,\n",
    "    AttentionExtractor\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"imports complete\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "print(f\"cuda available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"cuda device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"vram: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} gb\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf246bf0",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed25763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# configuration\n",
    "# ============================================================\n",
    "# choose dataset: 'italian_pvs' or 'neurovoz'\n",
    "dataset_name = 'italian_pvs'  # change to 'neurovoz' for neurovoz dataset\n",
    "\n",
    "config = {\n",
    "    'dataset': dataset_name,\n",
    "    'max_duration': 10.0,\n",
    "    'target_sr': 16000,\n",
    "    'pooling': 'mean',\n",
    "    'batch_size': 8,\n",
    "    'extract_attention': True,\n",
    "    'n_attention_samples': 100  # attention is memory-intensive\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# find model checkpoint\n",
    "# ============================================================\n",
    "project_path = Path(project_root)\n",
    "\n",
    "# primary location: results/final_model (saved during training)\n",
    "primary_checkpoint = project_path / 'results' / 'final_model'\n",
    "\n",
    "# fallback: look in checkpoints directory for latest loso run\n",
    "checkpoints_dir = project_path / 'results' / 'checkpoints'\n",
    "\n",
    "checkpoint_path = None\n",
    "\n",
    "if primary_checkpoint.exists():\n",
    "    checkpoint_path = primary_checkpoint\n",
    "    print(f\"found primary checkpoint: {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"primary checkpoint not found at: {primary_checkpoint}\")\n",
    "    \n",
    "    if checkpoints_dir.exists():\n",
    "        # find latest wav2vec2_loso_* checkpoint with final_model subdirectory\n",
    "        loso_checkpoints = sorted(checkpoints_dir.glob('wav2vec2_loso_*'))\n",
    "        \n",
    "        for ckpt in reversed(loso_checkpoints):  # newest first\n",
    "            final_model_path = ckpt / 'final_model'\n",
    "            if final_model_path.exists() and (final_model_path / 'model.safetensors').exists():\n",
    "                checkpoint_path = final_model_path\n",
    "                print(f\"found checkpoint: {checkpoint_path}\")\n",
    "                break\n",
    "            elif (ckpt / 'model.safetensors').exists():\n",
    "                checkpoint_path = ckpt\n",
    "                print(f\"found checkpoint: {checkpoint_path}\")\n",
    "                break\n",
    "\n",
    "if checkpoint_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"no trained model checkpoint found!\\n\"\n",
    "        \"please ensure phase 3 training is complete and model is saved to:\\n\"\n",
    "        f\"  - {primary_checkpoint}\\n\"\n",
    "        f\"  - or {checkpoints_dir}/wav2vec2_loso_*/final_model/\"\n",
    "    )\n",
    "\n",
    "# output paths\n",
    "output_dir = project_path / 'data' / 'activations'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "activations_filename = f\"activations_{config['dataset']}_{timestamp}.dat\"\n",
    "activations_path = output_dir / activations_filename\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"dataset:         {config['dataset']}\")\n",
    "print(f\"checkpoint:      {checkpoint_path}\")\n",
    "print(f\"output:          {activations_path}\")\n",
    "print(f\"pooling:         {config['pooling']}\")\n",
    "print(f\"batch size:      {config['batch_size']}\")\n",
    "print(f\"extract attention: {config['extract_attention']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c063f6",
   "metadata": {},
   "source": [
    "## load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# load fine-tuned model\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 1: loading fine-tuned wav2vec2 model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load model config and weights with error handling\n",
    "try:\n",
    "    model_config = Wav2Vec2Config.from_pretrained(checkpoint_path)\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"failed to load model from {checkpoint_path}\\n\"\n",
    "        f\"error: {str(e)}\\n\\n\"\n",
    "        f\"possible causes:\\n\"\n",
    "        f\"  - checkpoint files are corrupted or missing\\n\"\n",
    "        f\"  - checkpoint was saved with incompatible transformers version\\n\"\n",
    "        f\"  - checkpoint directory is incomplete (missing config.json or model.safetensors)\\n\\n\"\n",
    "        f\"please verify the checkpoint exists and is complete.\"\n",
    "    ) from e\n",
    "\n",
    "# load feature extractor from base model (not saved with checkpoint)\n",
    "# this is safe because we only use it for dataset loading, not inference\n",
    "try:\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "        'facebook/wav2vec2-base-960h'\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        f\"failed to load wav2vec2 feature extractor\\n\"\n",
    "        f\"error: {str(e)}\\n\\n\"\n",
    "        f\"this may indicate missing transformers library or network issues.\"\n",
    "    ) from e\n",
    "\n",
    "print(f\"\\nmodel loaded successfully ✓\")\n",
    "print(f\"  checkpoint: {checkpoint_path.name}\")\n",
    "print(f\"  device: {device}\")\n",
    "print(f\"  num_labels: {model_config.num_labels}\")\n",
    "print(f\"  num_layers: {len(model.wav2vec2.encoder.layers)}\")\n",
    "print(f\"  hidden_size: {model_config.hidden_size}\")\n",
    "print(f\"  num_attention_heads: {model_config.num_attention_heads}\")\n",
    "\n",
    "# count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  total parameters: {total_params:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d02771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# load dataset\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 2: loading dataset\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_root = project_path / 'data' / 'raw'\n",
    "\n",
    "if config['dataset'] == 'italian_pvs':\n",
    "    dataset = ItalianPVSDataset(\n",
    "        root_dir=str(data_root / 'italian_pvs'),\n",
    "        task=None,  # all tasks\n",
    "        max_duration=config['max_duration'],\n",
    "        target_sr=config['target_sr']\n",
    "    )\n",
    "elif config['dataset'] == 'neurovoz':\n",
    "    dataset = NeuroVozDataset(\n",
    "        root_dir=str(data_root / 'neurovoz'),\n",
    "        task=None,  # all tasks\n",
    "        max_duration=config['max_duration'],\n",
    "        target_sr=config['target_sr']\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"unknown dataset: {config['dataset']}\")\n",
    "\n",
    "# get dataset statistics\n",
    "labels = [dataset.samples[i]['label'] for i in range(len(dataset))]\n",
    "n_hc = sum(1 for l in labels if l == 0)\n",
    "n_pd = sum(1 for l in labels if l == 1)\n",
    "\n",
    "print(f\"\\ndataset: {config['dataset']}\")\n",
    "print(f\"  total samples: {len(dataset):,}\")\n",
    "print(f\"  hc samples: {n_hc:,}\")\n",
    "print(f\"  pd samples: {n_pd:,}\")\n",
    "print(f\"  class balance: {n_pd / len(dataset) * 100:.1f}% pd\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7041475",
   "metadata": {},
   "source": [
    "## model validation\n",
    "\n",
    "verify that the loaded model produces valid predictions before extraction.\n",
    "a degenerate model (predicting all one class) should not be used for interpretability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# model validation - check for degenerate predictions\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 3: model validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# run predictions on a sample of the dataset\n",
    "n_validation_samples = min(100, len(dataset))\n",
    "validation_indices = np.linspace(0, len(dataset)-1, n_validation_samples, dtype=int)\n",
    "\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "print(f\"validating on {n_validation_samples} samples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(validation_indices, desc=\"validating model\"):\n",
    "        sample = dataset[idx]\n",
    "        input_values = sample['input_values']\n",
    "        \n",
    "        if isinstance(input_values, np.ndarray):\n",
    "            input_values = torch.from_numpy(input_values)\n",
    "        \n",
    "        input_values = input_values.unsqueeze(0).to(device)\n",
    "        \n",
    "        outputs = model(input_values)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        all_preds.append(pred.cpu().item())\n",
    "        all_probs.append(probs[0, 1].cpu().item())  # pd probability\n",
    "        all_labels.append(sample['label'])\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_probs = np.array(all_probs)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# calculate metrics\n",
    "val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "val_auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.5\n",
    "\n",
    "# check prediction diversity\n",
    "n_pred_0 = (all_preds == 0).sum()\n",
    "n_pred_1 = (all_preds == 1).sum()\n",
    "\n",
    "print(f\"\\nvalidation results:\")\n",
    "print(f\"  accuracy: {val_accuracy:.1%}\")\n",
    "print(f\"  auc-roc:  {val_auc:.3f}\")\n",
    "print(f\"  predictions: {n_pred_0} hc, {n_pred_1} pd\")\n",
    "print(f\"  labels:      {(all_labels == 0).sum()} hc, {(all_labels == 1).sum()} pd\")\n",
    "\n",
    "# check for degenerate model\n",
    "is_degenerate = False\n",
    "if n_pred_0 == 0 or n_pred_1 == 0:\n",
    "    print(\"\\n⚠️  WARNING: model predicts only one class!\")\n",
    "    print(\"    this is a degenerate model - interpretability results will be meaningless\")\n",
    "    is_degenerate = True\n",
    "elif val_auc < 0.55:\n",
    "    print(\"\\n⚠️  WARNING: model has near-random performance (auc < 0.55)\")\n",
    "    print(\"    consider using a better-trained checkpoint\")\n",
    "    is_degenerate = True\n",
    "else:\n",
    "    print(\"\\n✓ model produces valid predictions\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ask user to confirm if model is degenerate\n",
    "if is_degenerate:\n",
    "    print(\"\\nthe loaded model appears to be degenerate or poorly trained.\")\n",
    "    print(\"activation extraction will proceed, but results may not be meaningful.\")\n",
    "    print(\"consider waiting for full loso training to complete.\")\n",
    "\n",
    "# cleanup validation arrays to free memory before extraction\n",
    "del all_preds, all_probs, all_labels\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ gpu memory cleared after validation\")\n",
    "elif device == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"\\n✓ mps memory cleared after validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0442252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# create activation extractor\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 4: initializing activation extractor\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "extractor = Wav2Vec2ActivationExtractor(\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nextractor initialized ✓\")\n",
    "print(f\"  num_layers: {extractor.num_layers}\")\n",
    "print(f\"  hidden_size: {extractor.hidden_size}\")\n",
    "print(f\"  pooling: {config['pooling']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2108ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# test extraction on single sample\n",
    "# ============================================================\n",
    "print(\"testing extraction on single sample...\")\n",
    "\n",
    "sample = dataset[0]\n",
    "input_values = sample['input_values']\n",
    "\n",
    "if isinstance(input_values, np.ndarray):\n",
    "    input_values = torch.from_numpy(input_values)\n",
    "\n",
    "test_activations = extractor.extract(\n",
    "    input_values,\n",
    "    pooling=config['pooling']\n",
    ")\n",
    "\n",
    "print(\"\\ntest extraction results:\")\n",
    "for name, act in sorted(test_activations.items()):\n",
    "    if name.startswith('layer_'):\n",
    "        print(f\"  {name}: shape {act.shape}, mean {act.mean():.4f}, std {act.std():.4f}\")\n",
    "\n",
    "print(f\"\\ncnn_features: shape {test_activations.get('cnn_features', np.array([])).shape}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b358fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# prepare samples for extraction\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 5: preparing samples for extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "input_values_list = []\n",
    "sample_metadata = []\n",
    "\n",
    "print(f\"\\nloading {len(dataset)} samples...\")\n",
    "\n",
    "for i in tqdm(range(len(dataset)), desc=\"loading samples\"):\n",
    "    sample = dataset[i]\n",
    "    \n",
    "    input_values = sample['input_values']\n",
    "    if isinstance(input_values, np.ndarray):\n",
    "        input_values = torch.from_numpy(input_values)\n",
    "    \n",
    "    input_values_list.append(input_values)\n",
    "    \n",
    "    sample_metadata.append({\n",
    "        'idx': i,\n",
    "        'label': sample['label'],\n",
    "        'subject_id': sample['subject_id'],\n",
    "        'path': str(sample['path']),\n",
    "        'task': sample.get('task', 'unknown')\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ loaded {len(input_values_list)} samples\")\n",
    "print(f\"  hc: {sum(1 for s in sample_metadata if s['label'] == 0)}\")\n",
    "print(f\"  pd: {sum(1 for s in sample_metadata if s['label'] == 1)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# extract activations to memmap\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 6: extracting activations\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nextracting activations for {len(input_values_list)} samples...\")\n",
    "print(f\"output path: {activations_path}\")\n",
    "print(f\"this may take 10-30 minutes depending on dataset size and gpu.\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "activations_memmap = extractor.extract_to_memmap(\n",
    "    input_values_list=input_values_list,\n",
    "    output_path=str(activations_path),\n",
    "    pooling=config['pooling'],\n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "\n",
    "extraction_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n✓ extraction complete!\")\n",
    "print(f\"  time elapsed: {extraction_time/60:.1f} minutes\")\n",
    "print(f\"  shape: {activations_memmap.shape}\")\n",
    "print(f\"  size: {activations_memmap.nbytes / 1e6:.2f} MB\")\n",
    "print(f\"  rate: {len(input_values_list) / extraction_time:.1f} samples/sec\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772923d",
   "metadata": {},
   "source": [
    "## save enhanced metadata\n",
    "\n",
    "save sample metadata alongside activations for probing experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# save enhanced metadata\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 7: saving enhanced metadata\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metadata_path = str(activations_path).replace('.dat', '_metadata.json')\n",
    "\n",
    "# load existing metadata with error handling\n",
    "try:\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"✓ loaded existing metadata from {metadata_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"⚠️  warning: metadata file not found at {metadata_path}\")\n",
    "    print(\"    creating new metadata from scratch...\")\n",
    "    \n",
    "    # create new metadata structure matching what extract_to_memmap should produce\n",
    "    metadata = {\n",
    "        'shape': list(activations_memmap.shape),\n",
    "        'n_samples': len(input_values_list),\n",
    "        'n_layers': extractor.num_layers,\n",
    "        'hidden_size': extractor.hidden_size,\n",
    "        'pooling': config['pooling'],\n",
    "        'dtype': 'float32'\n",
    "    }\n",
    "except json.JSONDecodeError as e:\n",
    "    raise RuntimeError(\n",
    "        f\"metadata file exists but contains invalid json: {metadata_path}\\n\"\n",
    "        f\"error: {str(e)}\\n\\n\"\n",
    "        f\"the file may be corrupted. consider deleting it and re-running extraction.\"\n",
    "    ) from e\n",
    "\n",
    "# add comprehensive sample info\n",
    "metadata['samples'] = sample_metadata\n",
    "metadata['labels'] = [s['label'] for s in sample_metadata]\n",
    "metadata['subject_ids'] = [s['subject_id'] for s in sample_metadata]\n",
    "metadata['tasks'] = [s['task'] for s in sample_metadata]\n",
    "\n",
    "# add configuration\n",
    "metadata['config'] = config\n",
    "metadata['timestamp'] = timestamp\n",
    "metadata['model_checkpoint'] = str(checkpoint_path)\n",
    "metadata['dataset_name'] = config['dataset']\n",
    "\n",
    "# add validation results (use defaults if validation was skipped)\n",
    "_val_accuracy = val_accuracy if 'val_accuracy' in dir() else None\n",
    "_val_auc = val_auc if 'val_auc' in dir() else None\n",
    "_n_val_samples = n_validation_samples if 'n_validation_samples' in dir() else 0\n",
    "_is_degenerate = is_degenerate if 'is_degenerate' in dir() else None\n",
    "\n",
    "metadata['validation'] = {\n",
    "    'accuracy': float(_val_accuracy) if _val_accuracy is not None else None,\n",
    "    'auc': float(_val_auc) if _val_auc is not None else None,\n",
    "    'n_samples': int(_n_val_samples),\n",
    "    'is_degenerate': _is_degenerate\n",
    "}\n",
    "\n",
    "# save updated metadata\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ metadata saved to {metadata_path}\")\n",
    "print(f\"  n_samples: {metadata['n_samples']}\")\n",
    "print(f\"  n_layers: {metadata['n_layers']}\")\n",
    "print(f\"  hidden_size: {metadata['hidden_size']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3fd19",
   "metadata": {},
   "source": [
    "## extract attention weights (optional)\n",
    "\n",
    "extract attention patterns for visualization and analysis.\n",
    "this is memory-intensive so we limit to a subset of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ca213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# extract attention weights\n",
    "# ============================================================\n",
    "if config['extract_attention']:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"step 8: extracting attention weights\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    attention_extractor = AttentionExtractor(\n",
    "        model=model,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    n_attention_samples = min(config['n_attention_samples'], len(dataset))\n",
    "    print(f\"\\nextracting attention for {n_attention_samples} samples...\")\n",
    "    \n",
    "    attention_data = []\n",
    "    failed_attention = 0\n",
    "    \n",
    "    for i in tqdm(range(n_attention_samples), desc=\"extracting attention\"):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        try:\n",
    "            input_values = sample['input_values']\n",
    "            if isinstance(input_values, np.ndarray):\n",
    "                input_values = torch.from_numpy(input_values)\n",
    "            \n",
    "            attentions = attention_extractor.extract_attention(\n",
    "                input_values,\n",
    "                layer_idx=None  # all layers\n",
    "            )\n",
    "            \n",
    "            # compute summary statistics (full matrices too large to store)\n",
    "            attention_summary = {\n",
    "                'idx': i,\n",
    "                'label': sample['label'],\n",
    "                'subject_id': sample['subject_id'],\n",
    "                'layer_entropy': [],\n",
    "                'layer_max_attention': [],\n",
    "                'layer_mean_attention': []\n",
    "            }\n",
    "            \n",
    "            for layer_att in attentions:\n",
    "                # average over heads: [num_heads, seq_len, seq_len] -> [seq_len, seq_len]\n",
    "                avg_att = layer_att.mean(axis=0)\n",
    "                \n",
    "                # entropy (measure of attention spread)\n",
    "                att_flat = avg_att.flatten()\n",
    "                att_probs = att_flat / (att_flat.sum() + 1e-9)\n",
    "                entropy = -np.sum(att_probs * np.log(att_probs + 1e-9))\n",
    "                \n",
    "                attention_summary['layer_entropy'].append(float(entropy))\n",
    "                attention_summary['layer_max_attention'].append(float(avg_att.max()))\n",
    "                attention_summary['layer_mean_attention'].append(float(avg_att.mean()))\n",
    "            \n",
    "            attention_data.append(attention_summary)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_attention += 1\n",
    "            if failed_attention <= 3:\n",
    "                print(f\"  warning: failed on sample {i}: {str(e)[:50]}\")\n",
    "    \n",
    "    # save attention summaries\n",
    "    attention_path = output_dir / f\"attention_summary_{config['dataset']}_{timestamp}.json\"\n",
    "    \n",
    "    with open(attention_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'data': attention_data,\n",
    "            'n_samples': len(attention_data),\n",
    "            'n_failed': failed_attention,\n",
    "            'n_layers': extractor.num_layers,\n",
    "            'timestamp': timestamp,\n",
    "            'dataset': config['dataset']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ attention summaries saved to {attention_path}\")\n",
    "    print(f\"  extracted: {len(attention_data)} samples\")\n",
    "    print(f\"  failed: {failed_attention} samples\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    attention_path = None\n",
    "    print(\"skipping attention extraction (disabled in config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# verification: reload and check activations\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"step 9: verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "loaded_activations, loaded_metadata = load_activations_memmap(activations_path)\n",
    "\n",
    "print(f\"\\nloaded activations:\")\n",
    "print(f\"  shape: {loaded_activations.shape}\")\n",
    "print(f\"  n_samples: {loaded_metadata['n_samples']}\")\n",
    "print(f\"  n_layers: {loaded_metadata['n_layers']}\")\n",
    "print(f\"  hidden_size: {loaded_metadata['hidden_size']}\")\n",
    "\n",
    "# check for invalid values\n",
    "n_nan = np.isnan(loaded_activations).sum()\n",
    "n_inf = np.isinf(loaded_activations).sum()\n",
    "\n",
    "print(f\"\\ndata quality:\")\n",
    "print(f\"  nan values: {n_nan}\")\n",
    "print(f\"  inf values: {n_inf}\")\n",
    "\n",
    "if n_nan > 0 or n_inf > 0:\n",
    "    print(\"  ⚠️ warning: invalid values detected!\")\n",
    "else:\n",
    "    print(\"  ✓ all values valid\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# activation statistics by layer\n",
    "# ============================================================\n",
    "print(\"activation statistics by layer:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for layer_idx in range(loaded_metadata['n_layers']):\n",
    "    layer_acts = loaded_activations[:, layer_idx, :]\n",
    "    \n",
    "    print(f\"layer {layer_idx:2d}: \"\n",
    "          f\"mean={layer_acts.mean():+.4f}, \"\n",
    "          f\"std={layer_acts.std():.4f}, \"\n",
    "          f\"min={layer_acts.min():+.4f}, \"\n",
    "          f\"max={layer_acts.max():+.4f}\")\n",
    "\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9b677",
   "metadata": {},
   "source": [
    "## class separability analysis\n",
    "\n",
    "quick sanity check: measure cosine similarity within and between classes.\n",
    "if the model learned meaningful representations, within-class similarity should exceed between-class similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# class separability analysis\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"class separability (cosine similarity)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "labels = np.array(loaded_metadata['labels'])\n",
    "\n",
    "print(f\"\\nlayer   pd-pd   hc-hc   pd-hc   separability\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "separability_scores = []\n",
    "\n",
    "for layer_idx in [0, 3, 6, 9, 11]:  # sample of layers\n",
    "    layer_acts = loaded_activations[:, layer_idx, :]\n",
    "    \n",
    "    pd_acts = layer_acts[labels == 1]\n",
    "    hc_acts = layer_acts[labels == 0]\n",
    "    \n",
    "    # within-class similarity (sample to reduce compute)\n",
    "    n_sample = min(50, len(pd_acts), len(hc_acts))\n",
    "    pd_sample = pd_acts[np.random.choice(len(pd_acts), n_sample, replace=False)]\n",
    "    hc_sample = hc_acts[np.random.choice(len(hc_acts), n_sample, replace=False)]\n",
    "    \n",
    "    pd_sim = cosine_similarity(pd_sample).mean()\n",
    "    hc_sim = cosine_similarity(hc_sample).mean()\n",
    "    cross_sim = cosine_similarity(pd_sample, hc_sample).mean()\n",
    "    \n",
    "    # separability: how much more similar within-class than between-class\n",
    "    avg_within = (pd_sim + hc_sim) / 2\n",
    "    separability = avg_within - cross_sim\n",
    "    separability_scores.append((layer_idx, separability))\n",
    "    \n",
    "    print(f\"layer {layer_idx:2d}  {pd_sim:.3f}   {hc_sim:.3f}   {cross_sim:.3f}   {separability:+.4f}\")\n",
    "\n",
    "# find most separable layer\n",
    "best_layer = max(separability_scores, key=lambda x: x[1])\n",
    "print(\"-\" * 50)\n",
    "print(f\"best separability: layer {best_layer[0]} (score: {best_layer[1]:+.4f})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc38027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# extraction summary\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 4 EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nmodel checkpoint: {checkpoint_path}\")\n",
    "print(f\"dataset: {config['dataset']} ({len(dataset)} samples)\")\n",
    "print(f\"pooling: {config['pooling']}\")\n",
    "\n",
    "# report validation results if available\n",
    "if 'val_accuracy' in dir() and val_accuracy is not None:\n",
    "    print(f\"\\nmodel validation:\")\n",
    "    print(f\"  accuracy: {val_accuracy:.1%}\")\n",
    "    print(f\"  auc-roc:  {val_auc:.3f}\")\n",
    "    if 'is_degenerate' in dir() and is_degenerate:\n",
    "        print(f\"  ⚠️ WARNING: model appears degenerate!\")\n",
    "else:\n",
    "    print(f\"\\nmodel validation: skipped\")\n",
    "\n",
    "print(f\"\\nactivations shape: {loaded_activations.shape}\")\n",
    "print(f\"activations size: {loaded_activations.nbytes / 1e6:.2f} MB\")\n",
    "print(f\"\\ngenerated files:\")\n",
    "print(f\"  ├── {activations_path.name}\")\n",
    "print(f\"  ├── {activations_path.name.replace('.dat', '_metadata.json')}\")\n",
    "if 'attention_path' in dir() and attention_path:\n",
    "    print(f\"  └── {attention_path.name}\")\n",
    "print(f\"\\nnext steps:\")\n",
    "print(f\"  1. download activations to local machine (if running on colab)\")\n",
    "print(f\"  2. run probing experiments (notebooks/colab/05_activation_patching.ipynb)\")\n",
    "print(f\"  3. run probing classifiers (notebooks/colab/07_probing_experiments.ipynb)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# cleanup gpu memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"\\ngpu memory cleared ✓\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
