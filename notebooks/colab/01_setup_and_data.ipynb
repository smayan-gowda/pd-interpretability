{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75de55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project path - adjust this to your drive location\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/pd-interpretability'\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f'working directory: {os.getcwd()}')\n",
    "print(f'project files: {os.listdir(\".\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements\n",
    "!pip install -q -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeebe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify gpu availability\n",
    "import torch\n",
    "\n",
    "print(f'pytorch version: {torch.__version__}')\n",
    "print(f'cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'gpu device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'gpu memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    print('warning: no gpu detected. enable gpu runtime: Runtime -> Change runtime type -> GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42000066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify imports\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import librosa\n",
    "import parselmouth\n",
    "\n",
    "print('all core packages imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify project module imports\n",
    "from src.data.datasets import ItalianPVSDataset, MDVRKCLDataset, ArkansasDataset\n",
    "from src.data.preprocessing import segment_audio, normalize_audio, AudioPreprocessor\n",
    "from src.features.clinical import ClinicalFeatureExtractor\n",
    "from src.models.classifier import Wav2Vec2PDClassifier, DataCollatorWithPadding\n",
    "\n",
    "print('all project modules imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cb0dd",
   "metadata": {},
   "source": [
    "## data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available datasets\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(PROJECT_ROOT) / 'data' / 'raw'\n",
    "\n",
    "datasets_available = {\n",
    "    'italian_pvs': (data_root / 'italian_pvs').exists(),\n",
    "    'mdvr_kcl': (data_root / 'mdvr-kcl').exists(),\n",
    "    'arkansas': (data_root / 'arkansas (figshare)').exists()\n",
    "}\n",
    "\n",
    "print('dataset availability:')\n",
    "for name, available in datasets_available.items():\n",
    "    status = 'available' if available else 'not found'\n",
    "    print(f'  {name}: {status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load italian pvs dataset for testing\n",
    "try:\n",
    "    italian_dataset = ItalianPVSDataset(\n",
    "        root_dir=str(data_root / 'italian_pvs'),\n",
    "        task=None,\n",
    "        max_duration=10.0\n",
    "    )\n",
    "    print(f'italian pvs dataset loaded: {len(italian_dataset)} samples')\n",
    "    \n",
    "    # get class distribution\n",
    "    labels = [s['label'] for s in italian_dataset.samples]\n",
    "    n_pd = sum(labels)\n",
    "    n_hc = len(labels) - n_pd\n",
    "    print(f'class distribution: {n_hc} hc, {n_pd} pd')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'failed to load italian pvs: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sample loading\n",
    "sample = italian_dataset[0]\n",
    "\n",
    "print(f'sample keys: {sample.keys()}')\n",
    "print(f'input_values shape: {sample[\"input_values\"].shape}')\n",
    "print(f'label: {sample[\"label\"]}')\n",
    "print(f'subject_id: {sample[\"subject_id\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af207ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test subject-wise split\n",
    "train_idx, val_idx, test_idx = italian_dataset.get_subject_split(\n",
    "    test_size=0.2,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'train samples: {len(train_idx)}')\n",
    "print(f'val samples: {len(val_idx)}')\n",
    "print(f'test samples: {len(test_idx)}')\n",
    "\n",
    "# verify no subject overlap\n",
    "train_subjects = set(italian_dataset.samples[i]['subject_id'] for i in train_idx)\n",
    "val_subjects = set(italian_dataset.samples[i]['subject_id'] for i in val_idx)\n",
    "test_subjects = set(italian_dataset.samples[i]['subject_id'] for i in test_idx)\n",
    "\n",
    "assert len(train_subjects & val_subjects) == 0, 'subject leakage: train-val'\n",
    "assert len(train_subjects & test_subjects) == 0, 'subject leakage: train-test'\n",
    "assert len(val_subjects & test_subjects) == 0, 'subject leakage: val-test'\n",
    "\n",
    "print('no subject leakage detected - splits are valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test clinical feature extraction on sample\n",
    "extractor = ClinicalFeatureExtractor()\n",
    "\n",
    "# extract from first sample\n",
    "sample_path = italian_dataset.samples[0]['path']\n",
    "features = extractor.extract(str(sample_path))\n",
    "\n",
    "print('extracted clinical features:')\n",
    "for key, value in features.items():\n",
    "    if value is not None:\n",
    "        print(f'  {key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974a13e",
   "metadata": {},
   "source": [
    "## model verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model loading\n",
    "classifier = Wav2Vec2PDClassifier(\n",
    "    model_name='facebook/wav2vec2-base-960h',\n",
    "    num_labels=2,\n",
    "    freeze_feature_extractor=True,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "params = classifier.count_parameters()\n",
    "print('model parameters:')\n",
    "print(f'  total: {params[\"total\"]:,}')\n",
    "print(f'  trainable: {params[\"trainable\"]:,}')\n",
    "print(f'  frozen: {params[\"frozen\"]:,}')\n",
    "print(f'  trainable %: {params[\"trainable_percent\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "sample_input = sample['input_values'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = classifier.forward(sample_input)\n",
    "\n",
    "print(f'input shape: {sample_input.shape}')\n",
    "print(f'output logits shape: {logits.shape}')\n",
    "print(f'output logits: {logits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f2c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data collator\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "collator = DataCollatorWithPadding(classifier.feature_extractor)\n",
    "\n",
    "# create small batch\n",
    "batch_samples = [italian_dataset[i] for i in range(4)]\n",
    "batch = collator(batch_samples)\n",
    "\n",
    "print(f'batch keys: {batch.keys()}')\n",
    "print(f'input_values shape: {batch[\"input_values\"].shape}')\n",
    "print(f'attention_mask shape: {batch[\"attention_mask\"].shape}')\n",
    "print(f'labels: {batch[\"labels\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66998037",
   "metadata": {},
   "source": [
    "## environment saved\n",
    "\n",
    "environment is verified and ready for training.\n",
    "proceed to notebook 02 for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d888c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save environment info for reproducibility\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "env_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    'datasets': {\n",
    "        'italian_pvs': len(italian_dataset) if 'italian_dataset' in dir() else 0\n",
    "    },\n",
    "    'model_params': params\n",
    "}\n",
    "\n",
    "env_path = Path(PROJECT_ROOT) / 'results' / 'env_info.json'\n",
    "env_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "print(f'environment info saved to {env_path}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
