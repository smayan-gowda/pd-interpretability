{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/pd-interpretability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4213cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q transformers datasets librosa scipy tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# verify gpu\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f8604",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12515053",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'data_path': '/content/drive/MyDrive/pd-interpretability/data',\n",
    "    'output_path': '/content/drive/MyDrive/pd-interpretability/results/generalization',\n",
    "    \n",
    "    # training parameters\n",
    "    'batch_size': 8,\n",
    "    'epochs': 3,  # fewer epochs per dataset for quick analysis\n",
    "    'learning_rate': 1e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    \n",
    "    # probing parameters\n",
    "    'probe_epochs': 50,\n",
    "    'probe_lr': 0.01,\n",
    "    \n",
    "    # clinical features for alignment\n",
    "    'clinical_features': ['jitter', 'shimmer', 'hnr', 'speech_rate'],\n",
    "    \n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "output_path = Path(CONFIG['output_path'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0a3a3",
   "metadata": {},
   "source": [
    "## 2. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb21cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    ItalianPVSDataset,\n",
    "    ArkansasDataset,\n",
    "    MDVRKCLDataset,\n",
    "    UCIParkinsonDataset\n",
    ")\n",
    "\n",
    "data_path = Path(CONFIG['data_path'])\n",
    "\n",
    "# load available datasets\n",
    "datasets = {}\n",
    "\n",
    "# italian pvs\n",
    "italian_path = data_path / 'raw' / 'italian_pvs'\n",
    "if italian_path.exists():\n",
    "    datasets['italian_pvs'] = ItalianPVSDataset(italian_path, max_duration=10.0)\n",
    "    print(f\"loaded italian_pvs: {len(datasets['italian_pvs'])} samples\")\n",
    "\n",
    "# arkansas\n",
    "arkansas_path = data_path / 'raw' / 'arkansas (figshare)'\n",
    "if arkansas_path.exists():\n",
    "    datasets['arkansas'] = ArkansasDataset(arkansas_path, max_duration=10.0)\n",
    "    print(f\"loaded arkansas: {len(datasets['arkansas'])} samples\")\n",
    "\n",
    "# mdvr-kcl\n",
    "mdvr_path = data_path / 'raw' / 'mdvr-kcl'\n",
    "if mdvr_path.exists():\n",
    "    datasets['mdvr_kcl'] = MDVRKCLDataset(mdvr_path, max_duration=10.0)\n",
    "    print(f\"loaded mdvr_kcl: {len(datasets['mdvr_kcl'])} samples\")\n",
    "\n",
    "print(f\"\\ntotal datasets loaded: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset statistics\n",
    "print(\"\\ndataset statistics:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    labels = [ds[i][1] for i in range(len(ds))]\n",
    "    n_pd = sum(labels)\n",
    "    n_hc = len(labels) - n_pd\n",
    "    print(f\"{name:15s}: {len(ds):4d} samples (PD: {n_pd:3d}, HC: {n_hc:3d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda04b2e",
   "metadata": {},
   "source": [
    "## 3. Train Dataset-Specific Models\n",
    "\n",
    "Train a separate model on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab028cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import DatasetSpecificTrainer, Wav2Vec2PDClassifier\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "trainer = DatasetSpecificTrainer(\n",
    "    model_class=Wav2Vec2PDClassifier,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5caf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models on each dataset\n",
    "print(\"training dataset-specific models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models = {}\n",
    "training_metrics = {}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    print(f\"\\ntraining on {name}...\")\n",
    "    \n",
    "    # split dataset\n",
    "    train_size = int(0.8 * len(ds))\n",
    "    val_size = len(ds) - train_size\n",
    "    train_ds, val_ds = random_split(ds, [train_size, val_size])\n",
    "    \n",
    "    # train\n",
    "    model, metrics = trainer.train(\n",
    "        train_dataset=train_ds,\n",
    "        val_dataset=val_ds,\n",
    "        dataset_name=name\n",
    "    )\n",
    "    \n",
    "    models[name] = model\n",
    "    training_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"  final train accuracy: {metrics['train_accuracy']:.3f}\")\n",
    "    print(f\"  final val accuracy: {metrics['val_accuracy']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"trained {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b72b8",
   "metadata": {},
   "source": [
    "## 4. Build Cross-Dataset Evaluation Matrix\n",
    "\n",
    "Evaluate each model on all datasets to build N×N performance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0113a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CrossDatasetEvaluator\n",
    "\n",
    "evaluator = CrossDatasetEvaluator(device=device)\n",
    "\n",
    "# build evaluation matrix\n",
    "print(\"building cross-dataset evaluation matrix...\")\n",
    "\n",
    "results = evaluator.evaluate_all(\n",
    "    models=models,\n",
    "    datasets=datasets,\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(\"\\nevaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display accuracy matrix\n",
    "dataset_names = list(datasets.keys())\n",
    "n_datasets = len(dataset_names)\n",
    "\n",
    "print(\"\\naccuracy matrix (row=train, col=test):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# header\n",
    "header = \"train\\\\test   \" + \"  \".join([f\"{n[:8]:>8s}\" for n in dataset_names])\n",
    "print(header)\n",
    "\n",
    "# rows\n",
    "for train_name in dataset_names:\n",
    "    row_values = []\n",
    "    for test_name in dataset_names:\n",
    "        acc = results.accuracy_matrix.get((train_name, test_name), 0.0)\n",
    "        row_values.append(f\"{acc:.3f}\")\n",
    "    print(f\"{train_name[:12]:12s}  \" + \"    \".join(row_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f34a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize as heatmap\n",
    "acc_matrix = np.zeros((n_datasets, n_datasets))\n",
    "\n",
    "for i, train_name in enumerate(dataset_names):\n",
    "    for j, test_name in enumerate(dataset_names):\n",
    "        acc_matrix[i, j] = results.accuracy_matrix.get((train_name, test_name), 0.0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(acc_matrix, cmap='RdYlGn', vmin=0.4, vmax=1.0)\n",
    "\n",
    "# add text annotations\n",
    "for i in range(n_datasets):\n",
    "    for j in range(n_datasets):\n",
    "        color = 'white' if acc_matrix[i, j] < 0.7 else 'black'\n",
    "        text = ax.text(j, i, f'{acc_matrix[i, j]:.3f}',\n",
    "                       ha='center', va='center', color=color, fontsize=12)\n",
    "\n",
    "ax.set_xticks(range(n_datasets))\n",
    "ax.set_yticks(range(n_datasets))\n",
    "ax.set_xticklabels([n.replace('_', '\\n') for n in dataset_names], fontsize=10)\n",
    "ax.set_yticklabels([n.replace('_', '\\n') for n in dataset_names], fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Test Dataset', fontsize=12)\n",
    "ax.set_ylabel('Train Dataset', fontsize=12)\n",
    "ax.set_title('Cross-Dataset Generalization Matrix\\n(Accuracy)', fontsize=14)\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Accuracy', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'cross_dataset_accuracy_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute generalization gaps\n",
    "print(\"\\ngeneralization gaps (in-domain - out-of-domain):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for train_name in dataset_names:\n",
    "    in_domain = results.accuracy_matrix.get((train_name, train_name), 0.0)\n",
    "    \n",
    "    out_domain_accs = [\n",
    "        results.accuracy_matrix.get((train_name, test_name), 0.0)\n",
    "        for test_name in dataset_names if test_name != train_name\n",
    "    ]\n",
    "    out_domain_mean = np.mean(out_domain_accs) if out_domain_accs else 0.0\n",
    "    \n",
    "    gap = results.generalization_gaps.get(train_name, in_domain - out_domain_mean)\n",
    "    \n",
    "    print(f\"{train_name:15s}: in-domain={in_domain:.3f}, out-of-domain={out_domain_mean:.3f}, gap={gap:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2646a4",
   "metadata": {},
   "source": [
    "## 5. Clinical Alignment Analysis\n",
    "\n",
    "Compute layerwise probing accuracy for clinical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ce0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clinical features\n",
    "clinical_data = {}\n",
    "\n",
    "for name in datasets.keys():\n",
    "    clinical_path = Path(CONFIG['data_path']) / 'clinical_features' / f'{name}_features.json'\n",
    "    if clinical_path.exists():\n",
    "        with open(clinical_path, 'r') as f:\n",
    "            clinical_data[name] = json.load(f)\n",
    "        print(f\"loaded clinical features for {name}\")\n",
    "    else:\n",
    "        print(f\"no clinical features for {name}\")\n",
    "\n",
    "print(f\"\\nclinical features available for {len(clinical_data)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ClinicalAlignmentAnalyzer\n",
    "\n",
    "if len(clinical_data) > 0:\n",
    "    alignment_analyzer = ClinicalAlignmentAnalyzer(\n",
    "        probe_epochs=CONFIG['probe_epochs'],\n",
    "        probe_lr=CONFIG['probe_lr'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # compute alignment profiles for each model-dataset pair\n",
    "    alignment_profiles = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        for dataset_name, ds in datasets.items():\n",
    "            if dataset_name not in clinical_data:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\ncomputing alignment: {model_name} model → {dataset_name} data\")\n",
    "            \n",
    "            # get clinical features for this dataset\n",
    "            features = clinical_data[dataset_name]['features']\n",
    "            sample_ids = clinical_data[dataset_name]['sample_ids']\n",
    "            \n",
    "            profile = alignment_analyzer.compute_alignment_profile(\n",
    "                model=model,\n",
    "                dataset=ds,\n",
    "                clinical_features=features,\n",
    "                sample_ids=sample_ids\n",
    "            )\n",
    "            \n",
    "            alignment_profiles[(model_name, dataset_name)] = profile\n",
    "            print(f\"  overall alignment score: {profile.overall_alignment:.3f}\")\n",
    "    \n",
    "    print(f\"\\ncomputed {len(alignment_profiles)} alignment profiles\")\n",
    "else:\n",
    "    print(\"skipping alignment analysis - no clinical features available\")\n",
    "    alignment_profiles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(alignment_profiles) > 0:\n",
    "    # visualize layerwise alignment for each feature\n",
    "    sample_profile = list(alignment_profiles.values())[0]\n",
    "    n_layers = len(sample_profile.layerwise_probing_accuracy.get('jitter', {}))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    for ax, feature in zip(axes.flat, CONFIG['clinical_features'][:4]):\n",
    "        for (model_name, dataset_name), profile in alignment_profiles.items():\n",
    "            if feature in profile.layerwise_probing_accuracy:\n",
    "                layer_accs = profile.layerwise_probing_accuracy[feature]\n",
    "                layers = list(range(len(layer_accs)))\n",
    "                accuracies = [layer_accs.get(l, 0.5) for l in layers]\n",
    "                \n",
    "                label = f\"{model_name[:6]}→{dataset_name[:6]}\"\n",
    "                ax.plot(layers, accuracies, marker='o', label=label, alpha=0.7)\n",
    "        \n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel('Probing Accuracy')\n",
    "        ax.set_title(f'{feature.upper()} Encoding by Layer')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.set_ylim(0.4, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'clinical_alignment_layers.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af9965",
   "metadata": {},
   "source": [
    "## 6. Generalization-Interpretability Correlation\n",
    "\n",
    "Test hypothesis: Higher clinical alignment → Better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f602ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import GeneralizationInterpretabilityAnalyzer\n",
    "\n",
    "if len(alignment_profiles) > 0:\n",
    "    correlation_analyzer = GeneralizationInterpretabilityAnalyzer()\n",
    "    \n",
    "    # gather data points for correlation\n",
    "    alignment_scores = []\n",
    "    generalization_scores = []\n",
    "    labels = []\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        # overall alignment score for this model\n",
    "        model_alignments = [\n",
    "            profile.overall_alignment \n",
    "            for (m, d), profile in alignment_profiles.items() \n",
    "            if m == model_name\n",
    "        ]\n",
    "        \n",
    "        if not model_alignments:\n",
    "            continue\n",
    "            \n",
    "        avg_alignment = np.mean(model_alignments)\n",
    "        \n",
    "        # out-of-domain accuracy for this model\n",
    "        out_domain_accs = [\n",
    "            results.accuracy_matrix.get((model_name, test_name), 0.0)\n",
    "            for test_name in datasets.keys() if test_name != model_name\n",
    "        ]\n",
    "        \n",
    "        if not out_domain_accs:\n",
    "            continue\n",
    "            \n",
    "        avg_generalization = np.mean(out_domain_accs)\n",
    "        \n",
    "        alignment_scores.append(avg_alignment)\n",
    "        generalization_scores.append(avg_generalization)\n",
    "        labels.append(model_name)\n",
    "    \n",
    "    # compute correlation\n",
    "    correlation_result = correlation_analyzer.compute_correlation(\n",
    "        alignment_scores=alignment_scores,\n",
    "        generalization_scores=generalization_scores\n",
    "    )\n",
    "    \n",
    "    print(\"\\ngeneralization-interpretability correlation:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"spearman correlation: {correlation_result['spearman_r']:.3f}\")\n",
    "    print(f\"p-value: {correlation_result['p_value']:.4f}\")\n",
    "    print(f\"interpretation: {correlation_result['interpretation']}\")\n",
    "else:\n",
    "    print(\"skipping correlation analysis - no alignment profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(alignment_profiles) > 0 and len(alignment_scores) >= 2:\n",
    "    # visualize correlation\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))\n",
    "    \n",
    "    for i, (a, g, l, c) in enumerate(zip(alignment_scores, generalization_scores, labels, colors)):\n",
    "        ax.scatter(a, g, s=150, c=[c], label=l, edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    # trend line\n",
    "    if len(alignment_scores) >= 2:\n",
    "        z = np.polyfit(alignment_scores, generalization_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(min(alignment_scores), max(alignment_scores), 100)\n",
    "        ax.plot(x_line, p(x_line), 'r--', alpha=0.7, linewidth=2, label='trend')\n",
    "    \n",
    "    ax.set_xlabel('Clinical Alignment Score', fontsize=12)\n",
    "    ax.set_ylabel('Out-of-Domain Accuracy', fontsize=12)\n",
    "    ax.set_title(f'Generalization vs. Clinical Alignment\\n(ρ = {correlation_result[\"spearman_r\"]:.3f}, p = {correlation_result[\"p_value\"]:.4f})', \n",
    "                 fontsize=14)\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'generalization_interpretability_correlation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b3525e",
   "metadata": {},
   "source": [
    "## 7. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile all results\n",
    "full_results = {\n",
    "    'config': CONFIG,\n",
    "    'datasets': {name: len(ds) for name, ds in datasets.items()},\n",
    "    'training_metrics': training_metrics,\n",
    "    'cross_dataset_evaluation': {\n",
    "        'accuracy_matrix': {f\"{k[0]}_to_{k[1]}\": v for k, v in results.accuracy_matrix.items()},\n",
    "        'f1_matrix': {f\"{k[0]}_to_{k[1]}\": v for k, v in results.f1_matrix.items()},\n",
    "        'auc_matrix': {f\"{k[0]}_to_{k[1]}\": v for k, v in results.auc_matrix.items()},\n",
    "        'generalization_gaps': results.generalization_gaps\n",
    "    }\n",
    "}\n",
    "\n",
    "if len(alignment_profiles) > 0:\n",
    "    full_results['clinical_alignment'] = {\n",
    "        f\"{m}_{d}\": {\n",
    "            'overall_alignment': profile.overall_alignment,\n",
    "            'feature_scores': profile.feature_alignment_scores\n",
    "        }\n",
    "        for (m, d), profile in alignment_profiles.items()\n",
    "    }\n",
    "\n",
    "if len(alignment_profiles) > 0 and len(alignment_scores) >= 2:\n",
    "    full_results['generalization_interpretability_correlation'] = correlation_result\n",
    "\n",
    "# save to json\n",
    "results_path = output_path / 'generalization_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(full_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-DATASET GENERALIZATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\ndatasets analyzed: {list(datasets.keys())}\")\n",
    "print(f\"total models trained: {len(models)}\")\n",
    "\n",
    "print(\"\\nbest generalizing model:\")\n",
    "best_model = min(results.generalization_gaps.items(), key=lambda x: x[1])\n",
    "print(f\"  {best_model[0]}: generalization gap = {best_model[1]:+.3f}\")\n",
    "\n",
    "if len(alignment_profiles) > 0:\n",
    "    print(\"\\nhighest clinical alignment:\")\n",
    "    best_alignment = max(\n",
    "        [(f\"{m}→{d}\", p.overall_alignment) for (m, d), p in alignment_profiles.items()],\n",
    "        key=lambda x: x[1]\n",
    "    )\n",
    "    print(f\"  {best_alignment[0]}: alignment = {best_alignment[1]:.3f}\")\n",
    "\n",
    "if len(alignment_profiles) > 0 and len(alignment_scores) >= 2:\n",
    "    print(\"\\ngeneralization-interpretability correlation:\")\n",
    "    print(f\"  spearman ρ = {correlation_result['spearman_r']:.3f} (p = {correlation_result['p_value']:.4f})\")\n",
    "    print(f\"  {correlation_result['interpretation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
