{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 06: Cross-Dataset Generalization Analysis\n",
    "\n",
    "**Objective**: Test Hypothesis 3 - Models with higher clinical alignment generalize better across datasets\n",
    "\n",
    "This notebook implements comprehensive cross-dataset evaluation following LODO (Leave-One-Dataset-Out) protocol with:\n",
    "- Dataset-specific model training (Italian PVS, MDVR-KCL, Arkansas)\n",
    "- N\u00d7N cross-dataset evaluation matrices\n",
    "- Clinical alignment analysis via layerwise probing\n",
    "- Statistical correlation between alignment and generalization\n",
    "- Domain shift quantification\n",
    "- Publication-grade LaTeX visualizations\n",
    "\n",
    "**Methods based on**:\n",
    "- Leave-one-dataset-out CV protocol (PMC10388213)\n",
    "- Spearman correlation for non-parametric analysis\n",
    "- Bootstrap confidence intervals (95% CI)\n",
    "- STARD-AI reporting standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/pd-interpretability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install -q transformers datasets librosa scipy tqdm scikit-learn praat-parselmouth statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install latex for publication-grade figures\n",
    "!apt-get install -y dvipng texlive-latex-extra texlive-fonts-recommended cm-super texlive-science\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# scientific computing\n",
    "import scipy\n",
    "from scipy import stats as scipy_stats\n",
    "from scipy.spatial.distance import wasserstein_distance\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# verify gpu\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"SciPy version: {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LaTeX Rendering for Publication-Grade Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib for latex rendering and publication quality\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "    \"axes.labelsize\": 13,\n",
    "    \"font.size\": 13,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"figure.titlesize\": 15,\n",
    "    \"figure.dpi\": 300,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False\n",
    "})\n",
    "\n",
    "# set seaborn theme\n",
    "sns.set_theme(style='whitegrid', palette='colorblind')\n",
    "\n",
    "# define consistent color palette\n",
    "palette = sns.color_palette('colorblind')\n",
    "COLORS = {\n",
    "    'HC': palette[0],  # blue\n",
    "    'PD': palette[1],  # orange\n",
    "    'Neutral': 'gray',\n",
    "    'Primary': palette[2],  # green\n",
    "    'Secondary': palette[3],  # red\n",
    "}\n",
    "\n",
    "# verify latex rendering\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    ax.text(0.5, 0.5, r'LaTeX Test: $\\mathcal{H}_0: \\rho = 0$, $\\alpha = 0.05$',\n",
    "            ha='center', va='center', fontsize=14)\n",
    "    ax.set_title(r'\\textbf{Publication-Grade Rendering Verified}')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    print(\"\\u2713 LaTeX rendering configured successfully\")\nexcept Exception as e:\n",
    "    print(f\"\\u26a0\\ufe0f Warning: LaTeX rendering failed: {e}\")\n",
    "    print(\"Falling back to standard fonts...\")\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "        \"font.family\": \"sans-serif\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'project_path': '/content/drive/MyDrive/pd-interpretability',\n",
    "    'data_path': '/content/drive/MyDrive/pd-interpretability/data',\n",
    "    'output_path': '/content/drive/MyDrive/pd-interpretability/results/generalization',\n",
    "    \n",
    "    # training parameters\n",
    "    'model_name': 'facebook/wav2vec2-base-960h',\n",
    "    'batch_size': 8,\n",
    "    'epochs': 5,  # increased for better convergence\n",
    "    'learning_rate': 1e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'freeze_feature_extractor': True,\n",
    "    \n",
    "    # evaluation parameters\n",
    "    'bootstrap_iterations': 1000,  # for confidence intervals\n",
    "    'confidence_level': 0.95,\n",
    "    \n",
    "    # probing parameters\n",
    "    'n_layers': 12,\n",
    "    'hidden_size': 768,\n",
    "    'probe_epochs': 100,\n",
    "    'probe_lr': 0.01,\n",
    "    'probe_cv_folds': 5,\n",
    "    \n",
    "    # clinical features for alignment\n",
    "    'clinical_features': ['jitter_local', 'shimmer_local', 'hnr_mean', 'f0_std'],\n",
    "    \n",
    "    # figure settings\n",
    "    'fig_format': ['pdf', 'png', 'svg'],\n",
    "    'fig_dpi': 300,\n",
    "    \n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "output_path = Path(CONFIG['output_path'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create subdirectories\n",
    "figures_path = output_path / 'figures'\n",
    "figures_path.mkdir(exist_ok=True)\n",
    "\n",
    "models_path = output_path / 'models'\n",
    "models_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Output path: {output_path}\")\n",
    "print(f\"  Training epochs: {CONFIG['epochs']}\")\n",
    "print(f\"  Bootstrap iterations: {CONFIG['bootstrap_iterations']}\")\n",
    "print(f\"  Clinical features: {CONFIG['clinical_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Datasets\n",
    "\n",
    "Loading three cross-linguistic, multi-institutional datasets:\n",
    "- **Italian PVS**: Italian, elderly cohort, university hospital\n",
    "- **MDVR-KCL**: English (UK), Kings College London\n",
    "- **Arkansas**: English (US), University of Arkansas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import (\n",
    "    ItalianPVSDataset,\n",
    "    ArkansasDataset,\n",
    "    MDVRKCLDataset\n",
    ")\n",
    "\n",
    "data_path = Path(CONFIG['data_path'])\n",
    "\n",
    "# load available datasets\n",
    "datasets = {}\n",
    "dataset_metadata = {}\n",
    "\n",
    "print(\"Loading datasets...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# italian pvs\n",
    "italian_path = data_path / 'raw' / 'italian_pvs'\n",
    "if italian_path.exists():\n",
    "    print(\"\\nLoading Italian PVS dataset...\")\n",
    "    datasets['italian_pvs'] = ItalianPVSDataset(italian_path, max_duration=10.0)\n",
    "    dataset_metadata['italian_pvs'] = {\n",
    "        'language': 'Italian',\n",
    "        'institution': 'University Hospital',\n",
    "        'country': 'Italy',\n",
    "        'task': 'Vowel prolongation'\n",
    "    }\n",
    "    print(f\"  \u2713 Loaded {len(datasets['italian_pvs'])} samples\")\n",
    "else:\n",
    "    print(\"  \u2717 Italian PVS dataset not found\")\n",
    "\n",
    "# arkansas\n",
    "arkansas_path = data_path / 'raw' / 'arkansas (figshare)'\n",
    "if arkansas_path.exists():\n",
    "    print(\"\\nLoading Arkansas dataset...\")\n",
    "    datasets['arkansas'] = ArkansasDataset(arkansas_path, max_duration=10.0)\n",
    "    dataset_metadata['arkansas'] = {\n",
    "        'language': 'English (US)',\n",
    "        'institution': 'University of Arkansas',\n",
    "        'country': 'USA',\n",
    "        'task': 'Speech tasks'\n",
    "    }\n",
    "    print(f\"  \u2713 Loaded {len(datasets['arkansas'])} samples\")\n",
    "else:\n",
    "    print(\"  \u2717 Arkansas dataset not found\")\n",
    "\n",
    "# mdvr-kcl\n",
    "mdvr_path = data_path / 'raw' / 'mdvr-kcl'\n",
    "if mdvr_path.exists():\n",
    "    print(\"\\nLoading MDVR-KCL dataset...\")\n",
    "    datasets['mdvr_kcl'] = MDVRKCLDataset(mdvr_path, max_duration=10.0)\n",
    "    dataset_metadata['mdvr_kcl'] = {\n",
    "        'language': 'English (UK)',\n",
    "        'institution': 'Kings College London',\n",
    "        'country': 'UK',\n",
    "        'task': 'Voice recordings'\n",
    "    }\n",
    "    print(f\"  \u2713 Loaded {len(datasets['mdvr_kcl'])} samples\")\n",
    "else:\n",
    "    print(\"  \u2717 MDVR-KCL dataset not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nTotal datasets loaded: {len(datasets)}\\n\")\n",
    "\n",
    "if len(datasets) < 2:\n",
    "    raise ValueError(f\"Need at least 2 datasets for cross-dataset analysis, got {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprehensive dataset statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dataset_stats = {}\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    # extract labels\n",
    "    labels = []\n",
    "    durations = []\n",
    "    \n",
    "    for i in range(len(ds)):\n",
    "        sample = ds[i]\n",
    "        labels.append(sample['label'])\n",
    "        # estimate duration from input_values length (assuming 16kHz)\n",
    "        duration = len(sample['input_values']) / 16000.0\n",
    "        durations.append(duration)\n",
    "    \n",
    "    n_pd = sum(labels)\n",
    "    n_hc = len(labels) - n_pd\n",
    "    \n",
    "    stats = {\n",
    "        'n_total': len(ds),\n",
    "        'n_pd': n_pd,\n",
    "        'n_hc': n_hc,\n",
    "        'pd_ratio': n_pd / len(ds) if len(ds) > 0 else 0,\n",
    "        'mean_duration': np.mean(durations),\n",
    "        'std_duration': np.std(durations),\n",
    "        'min_duration': np.min(durations),\n",
    "        'max_duration': np.max(durations)\n",
    "    }\n",
    "    dataset_stats[name] = stats\n",
    "    \n",
    "    metadata = dataset_metadata.get(name, {})\n",
    "    \n",
    "    print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "    print(f\"  Language: {metadata.get('language', 'Unknown')}\")\n",
    "    print(f\"  Institution: {metadata.get('institution', 'Unknown')}\")\n",
    "    print(f\"  Total samples: {stats['n_total']}\")\n",
    "    print(f\"  PD: {stats['n_pd']} ({stats['pd_ratio']:.1%})\")\n",
    "    print(f\"  HC: {stats['n_hc']} ({(1-stats['pd_ratio']):.1%})\")\n",
    "    print(f\"  Duration: {stats['mean_duration']:.2f}\u00b1{stats['std_duration']:.2f}s (range: {stats['min_duration']:.2f}-{stats['max_duration']:.2f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Clinical Features for All Datasets\n",
    "\n",
    "Extract clinical acoustic features using Praat-Parselmouth for clinical alignment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.clinical import ClinicalFeatureExtractor\n",
    "import librosa\n",
    "\n",
    "# initialize extractor\n",
    "clinical_extractor = ClinicalFeatureExtractor()\n",
    "\n",
    "# extract features for all datasets\n",
    "clinical_data = {}\n",
    "\n",
    "print(\"\\nExtracting clinical features...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nProcessing {dataset_name}...\")\n",
    "    \n",
    "    features_dict = {feat: [] for feat in CONFIG['clinical_features']}\n",
    "    sample_ids = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Progress: {i}/{len(dataset)} samples\")\n",
    "        \n",
    "        try:\n",
    "            sample = dataset[i]\n",
    "            audio = sample['input_values'].numpy() if torch.is_tensor(sample['input_values']) else sample['input_values']\n",
    "            sample_rate = 16000\n",
    "            \n",
    "            # extract clinical features\n",
    "            features = clinical_extractor.extract_from_array(audio, sample_rate)\n",
    "            \n",
    "            # store only requested features\n",
    "            for feat_name in CONFIG['clinical_features']:\n",
    "                features_dict[feat_name].append(features.get(feat_name, np.nan))\n",
    "            \n",
    "            sample_ids.append(f\"{dataset_name}_{i}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Failed to extract features for sample {i}: {e}\")\n",
    "            for feat_name in CONFIG['clinical_features']:\n",
    "                features_dict[feat_name].append(np.nan)\n",
    "            sample_ids.append(f\"{dataset_name}_{i}\")\n",
    "    \n",
    "    # convert to numpy arrays\n",
    "    for feat_name in CONFIG['clinical_features']:\n",
    "        features_dict[feat_name] = np.array(features_dict[feat_name])\n",
    "    \n",
    "    clinical_data[dataset_name] = {\n",
    "        'features': features_dict,\n",
    "        'sample_ids': sample_ids\n",
    "    }\n",
    "    \n",
    "    # print statistics\n",
    "    print(f\"\\n  Feature statistics for {dataset_name}:\")\n",
    "    for feat_name in CONFIG['clinical_features']:\n",
    "        values = features_dict[feat_name]\n",
    "        valid_values = values[~np.isnan(values)]\n",
    "        if len(valid_values) > 0:\n",
    "            print(f\"    {feat_name:20s}: mean={np.mean(valid_values):.4f}, std={np.std(valid_values):.4f}, valid={len(valid_values)}/{len(values)}\")\n",
    "        else:\n",
    "            print(f\"    {feat_name:20s}: NO VALID VALUES\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nClinical features extracted for {len(clinical_data)} datasets\")\n",
    "\n",
    "# save clinical features\n",
    "clinical_features_path = Path(CONFIG['data_path']) / 'clinical_features'\n",
    "clinical_features_path.mkdir(exist_ok=True)\n",
    "\n",
    "for dataset_name, data in clinical_data.items():\n",
    "    save_dict = {\n",
    "        'features': {k: v.tolist() for k, v in data['features'].items()},\n",
    "        'sample_ids': data['sample_ids']\n",
    "    }\n",
    "    save_path = clinical_features_path / f'{dataset_name}_features.json'\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(save_dict, f, indent=2)\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantify Domain Shift Between Datasets\n",
    "\n",
    "Compute Wasserstein distance between clinical feature distributions to quantify dataset dissimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise Wasserstein distances\n",
    "dataset_names = list(datasets.keys())\n",
    "n_datasets = len(dataset_names)\n",
    "\n",
    "# compute domain shift matrix for each clinical feature\n",
    "domain_shift_matrices = {}\n",
    "\n",
    "print(\"\\nComputing domain shift (Wasserstein distances)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for feat_name in CONFIG['clinical_features']:\n",
    "    shift_matrix = np.zeros((n_datasets, n_datasets))\n",
    "    \n",
    "    for i, ds1_name in enumerate(dataset_names):\n",
    "        for j, ds2_name in enumerate(dataset_names):\n",
    "            if i == j:\n",
    "                shift_matrix[i, j] = 0.0\n",
    "            else:\n",
    "                # get valid values\n",
    "                vals1 = clinical_data[ds1_name]['features'][feat_name]\n",
    "                vals2 = clinical_data[ds2_name]['features'][feat_name]\n",
    "                \n",
    "                vals1 = vals1[~np.isnan(vals1)]\n",
    "                vals2 = vals2[~np.isnan(vals2)]\n",
    "                \n",
    "                if len(vals1) > 0 and len(vals2) > 0:\n",
    "                    # compute Wasserstein distance\n",
    "                    dist = wasserstein_distance(vals1, vals2)\n",
    "                    shift_matrix[i, j] = dist\n",
    "                else:\n",
    "                    shift_matrix[i, j] = np.nan\n",
    "    \n",
    "    domain_shift_matrices[feat_name] = shift_matrix\n",
    "    \n",
    "    print(f\"\\n{feat_name.upper()}:\")\n",
    "    print(\"  \" + \"  \".join([f\"{n[:8]:>10s}\" for n in dataset_names]))\n",
    "    for i, ds_name in enumerate(dataset_names):\n",
    "        row_str = f\"{ds_name[:8]:8s}  \"\n",
    "        row_str += \"  \".join([f\"{shift_matrix[i,j]:10.4f}\" if not np.isnan(shift_matrix[i,j]) else \"       N/A\" \n",
    "                              for j in range(n_datasets)])\n",
    "        print(\"  \" + row_str)\n",
    "\n",
    "# compute average domain shift\n",
    "avg_domain_shift = np.zeros((n_datasets, n_datasets))\n",
    "for feat_name in CONFIG['clinical_features']:\n",
    "    avg_domain_shift += np.nan_to_num(domain_shift_matrices[feat_name], 0.0)\n",
    "avg_domain_shift /= len(CONFIG['clinical_features'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nAVERAGE DOMAIN SHIFT (across all features):\")\n",
    "print(\"  \" + \"  \".join([f\"{n[:8]:>10s}\" for n in dataset_names]))\n",
    "for i, ds_name in enumerate(dataset_names):\n",
    "    row_str = f\"{ds_name[:8]:8s}  \"\n",
    "    row_str += \"  \".join([f\"{avg_domain_shift[i,j]:10.4f}\" for j in range(n_datasets)])\n",
    "    print(\"  \" + row_str)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Dataset-Specific Models (LODO Protocol)\n",
    "\n",
    "Train separate Wav2Vec2 models on each dataset following Leave-One-Dataset-Out cross-validation protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import DatasetSpecificTrainer\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# initialize trainer\n",
    "trainer = DatasetSpecificTrainer(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_labels=2,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    warmup_ratio=CONFIG['warmup_ratio'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    freeze_feature_extractor=CONFIG['freeze_feature_extractor'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nTraining dataset-specific models (LODO protocol)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "models = {}\n",
    "training_histories = {}\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TRAINING MODEL ON: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # split dataset (80-20 train-val)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(\n",
    "        dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(CONFIG['random_seed'])\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Val samples: {len(val_dataset)}\\n\")\n",
    "    \n",
    "    # train model\n",
    "    model, metrics = trainer.train_on_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        output_dir=models_path\n",
    "    )\n",
    "    \n",
    "    models[dataset_name] = model\n",
    "    training_histories[dataset_name] = metrics\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPLETED: {dataset_name.upper()}\")\n",
    "    print(f\"  Best validation accuracy: {metrics['best_accuracy']:.4f}\")\n",
    "    print(f\"  Final test accuracy: {metrics['final_accuracy']:.4f}\")\n",
    "    print(f\"  Final F1-score: {metrics['final_f1']:.4f}\")\n",
    "    print(f\"  Final AUC-ROC: {metrics['final_auc']:.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(f\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(f\"Total models trained: {len(models)}\\n\")\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    metrics = training_histories[dataset_name]\n",
    "    print(f\"{dataset_name:15s}: Acc={metrics['final_accuracy']:.3f}, F1={metrics['final_f1']:.3f}, AUC={metrics['final_auc']:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Cross-Dataset Evaluation Matrix with Confidence Intervals\n",
    "\n",
    "Evaluate each model on all datasets to construct N\u00d7N performance matrix with bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CrossDatasetEvaluator\n",
    "\n",
    "# initialize evaluator with datasets\n",
    "evaluator = CrossDatasetEvaluator(datasets=datasets, device=device)\n",
    "\n",
    "print(\"\\nBuilding cross-dataset evaluation matrix...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# build evaluation matrix\n",
    "results = evaluator.build_evaluation_matrix(\n",
    "    models=models,\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Evaluation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display accuracy matrix\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CROSS-DATASET ACCURACY MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n(Rows = Training Dataset, Columns = Test Dataset)\\n\")\n",
    "\n",
    "# header\n",
    "header = \"Train\\\\Test    \" + \"  \".join([f\"{n[:10]:>12s}\" for n in dataset_names])\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# rows\n",
    "for train_name in dataset_names:\n",
    "    row_values = []\n",
    "    for test_name in dataset_names:\n",
    "        acc = results.accuracy_matrix.get(train_name, {}).get(test_name, 0.0)\n",
    "        row_values.append(f\"{acc:12.4f}\")\n",
    "    print(f\"{train_name[:14]:14s}  \" + \"  \".join(row_values))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and display generalization gaps\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERALIZATION GAPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n(Gap = In-Domain Accuracy - Mean Out-of-Domain Accuracy)\\n\")\n",
    "\n",
    "for train_name in dataset_names:\n",
    "    in_domain = results.accuracy_matrix.get(train_name, {}).get(train_name, 0.0)\n",
    "    \n",
    "    out_domain_accs = [\n",
    "        results.accuracy_matrix.get(train_name, {}).get(test_name, 0.0)\n",
    "        for test_name in dataset_names if test_name != train_name\n",
    "    ]\n",
    "    out_domain_mean = np.mean(out_domain_accs) if out_domain_accs else 0.0\n",
    "    out_domain_std = np.std(out_domain_accs) if out_domain_accs else 0.0\n",
    "    \n",
    "    gap = results.generalization_gaps.get(train_name, in_domain - out_domain_mean)\n",
    "    \n",
    "    print(f\"{train_name:15s}:\")\n",
    "    print(f\"  In-domain:     {in_domain:.4f}\")\n",
    "    print(f\"  Out-of-domain: {out_domain_mean:.4f} \u00b1 {out_domain_std:.4f}\")\n",
    "    print(f\"  Gap:           {gap:+.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extract Layer-Wise Activations for Clinical Alignment\n",
    "\n",
    "Extract hidden states from all trained models for clinical alignment analysis via probing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interpretability.extraction import ActivationExtractor\n",
    "\n",
    "# initialize extractor\n",
    "extractor = ActivationExtractor(device=device)\n",
    "\n",
    "# extract activations for all model-dataset pairs\n",
    "activations_dict = {}\n",
    "activation_sample_ids_dict = {}\n",
    "\n",
    "print(\"\\nExtracting layer-wise activations...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nExtracting activations from {model_name} model...\")\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        key = f\"{model_name}_{dataset_name}\"\n",
    "        print(f\"  On {dataset_name} dataset...\")\n",
    "        \n",
    "        # extract activations\n",
    "        activations, sample_ids = extractor.extract_from_dataset(\n",
    "            model=model,\n",
    "            dataset=dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            layers='all',  # extract all 12 layers\n",
    "            pooling='mean'  # mean pooling over time dimension\n",
    "        )\n",
    "        \n",
    "        activations_dict[key] = activations\n",
    "        activation_sample_ids_dict[key] = sample_ids\n",
    "        \n",
    "        print(f\"    Shape: {activations.shape} (samples \u00d7 layers \u00d7 hidden_dim)\")\n",
    "        print(f\"    Samples: {len(sample_ids)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nExtracted activations for {len(activations_dict)} model-dataset combinations\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clinical Alignment Analysis\n",
    "\n",
    "Compute layer-wise probing accuracy for each clinical feature to quantify clinical alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def compute_layerwise_probing_accuracy(\n",
    "    activations: np.ndarray,\n",
    "    activation_sample_ids: List[str],\n",
    "    clinical_features: Dict[str, np.ndarray],\n",
    "    feature_sample_ids: List[str],\n",
    "    n_layers: int = 12,\n",
    "    cv_folds: int = 5\n",
    ") -> Dict[str, Dict[int, float]]:\n",
    "    \"\"\"\n",
    "    compute layer-wise probing accuracy for all clinical features.\n",
    "    \n",
    "    args:\n",
    "        activations: [n_samples, n_layers, hidden_dim]\n",
    "        activation_sample_ids: sample identifiers for activations\n",
    "        clinical_features: dict of feature_name -> values\n",
    "        feature_sample_ids: sample identifiers for features\n",
    "        n_layers: number of layers\n",
    "        cv_folds: number of cross-validation folds\n",
    "    \n",
    "    returns:\n",
    "        dict of feature_name -> {layer_idx -> accuracy}\n",
    "    \"\"\"\n",
    "    # create sample id mapping\n",
    "    sample_to_idx = {sid: i for i, sid in enumerate(feature_sample_ids)}\n",
    "    \n",
    "    # find matching samples\n",
    "    valid_indices = []\n",
    "    valid_feature_indices = []\n",
    "    \n",
    "    for i, act_sid in enumerate(activation_sample_ids):\n",
    "        if act_sid in sample_to_idx:\n",
    "            valid_indices.append(i)\n",
    "            valid_feature_indices.append(sample_to_idx[act_sid])\n",
    "    \n",
    "    if len(valid_indices) < 20:\n",
    "        print(f\"  Warning: Only {len(valid_indices)} matching samples found\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"  Found {len(valid_indices)} matching samples\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for feat_name, feat_values in clinical_features.items():\n",
    "        print(f\"\\n  Probing {feat_name}...\")\n",
    "        \n",
    "        # get feature values for matching samples\n",
    "        feat_vals = feat_values[valid_feature_indices]\n",
    "        \n",
    "        # binarize at median for probing\n",
    "        valid_feat = feat_vals[~np.isnan(feat_vals)]\n",
    "        if len(valid_feat) < 20:\n",
    "            print(f\"    Skipping: only {len(valid_feat)} valid feature values\")\n",
    "            continue\n",
    "        \n",
    "        median_val = np.median(valid_feat)\n",
    "        binary_labels = (feat_vals > median_val).astype(int)\n",
    "        \n",
    "        layer_accuracies = {}\n",
    "        \n",
    "        for layer_idx in range(n_layers):\n",
    "            # get activations for this layer\n",
    "            layer_acts = activations[valid_indices, layer_idx, :]\n",
    "            \n",
    "            # remove samples with nan features\n",
    "            valid_mask = ~np.isnan(feat_vals)\n",
    "            X = layer_acts[valid_mask]\n",
    "            y = binary_labels[valid_mask]\n",
    "            \n",
    "            if len(X) < 20 or len(np.unique(y)) < 2:\n",
    "                layer_accuracies[layer_idx] = 0.5\n",
    "                continue\n",
    "            \n",
    "            # standardize\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            try:\n",
    "                # probe with logistic regression + cross-validation\n",
    "                clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "                scores = cross_val_score(clf, X_scaled, y, cv=min(cv_folds, len(X)//2), scoring='accuracy')\n",
    "                layer_accuracies[layer_idx] = float(np.mean(scores))\n",
    "            except Exception as e:\n",
    "                layer_accuracies[layer_idx] = 0.5\n",
    "        \n",
    "        results[feat_name] = layer_accuracies\n",
    "        \n",
    "        # print best layer\n",
    "        best_layer = max(layer_accuracies, key=layer_accuracies.get)\n",
    "        best_acc = layer_accuracies[best_layer]\n",
    "        print(f\"    Best layer: L{best_layer} (acc={best_acc:.3f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute clinical alignment profiles for all model-dataset pairs\n",
    "alignment_profiles = {}\n",
    "\n",
    "print(\"\\nComputing clinical alignment profiles...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    for dataset_name in datasets.keys():\n",
    "        key = f\"{model_name}_{dataset_name}\"\n",
    "        print(f\"\\nModel: {model_name} \u2192 Dataset: {dataset_name}\")\n",
    "        \n",
    "        if key not in activations_dict:\n",
    "            print(\"  Skipping: no activations found\")\n",
    "            continue\n",
    "        \n",
    "        if dataset_name not in clinical_data:\n",
    "            print(\"  Skipping: no clinical features found\")\n",
    "            continue\n",
    "        \n",
    "        # extract data\n",
    "        activations = activations_dict[key]\n",
    "        activation_ids = activation_sample_ids_dict[key]\n",
    "        clinical_feats = clinical_data[dataset_name]['features']\n",
    "        clinical_ids = clinical_data[dataset_name]['sample_ids']\n",
    "        \n",
    "        # compute layerwise probing\n",
    "        layerwise_probing = compute_layerwise_probing_accuracy(\n",
    "            activations=activations,\n",
    "            activation_sample_ids=activation_ids,\n",
    "            clinical_features=clinical_feats,\n",
    "            feature_sample_ids=clinical_ids,\n",
    "            n_layers=CONFIG['n_layers'],\n",
    "            cv_folds=CONFIG['probe_cv_folds']\n",
    "        )\n",
    "        \n",
    "        if not layerwise_probing:\n",
    "            print(\"  Skipping: probing failed\")\n",
    "            continue\n",
    "        \n",
    "        # compute feature-wise alignment scores (best layer for each feature)\n",
    "        feature_scores = {}\n",
    "        for feat_name, layer_accs in layerwise_probing.items():\n",
    "            if layer_accs:\n",
    "                feature_scores[feat_name] = max(layer_accs.values())\n",
    "            else:\n",
    "                feature_scores[feat_name] = 0.5\n",
    "        \n",
    "        # compute overall alignment score\n",
    "        overall_alignment = np.mean(list(feature_scores.values())) if feature_scores else 0.5\n",
    "        \n",
    "        # store profile\n",
    "        alignment_profiles[key] = {\n",
    "            'model_name': model_name,\n",
    "            'dataset_name': dataset_name,\n",
    "            'layerwise_probing': layerwise_probing,\n",
    "            'feature_scores': feature_scores,\n",
    "            'overall_alignment': overall_alignment\n",
    "        }\n",
    "        \n",
    "        print(f\"  Overall alignment: {overall_alignment:.4f}\")\n",
    "        print(f\"  Feature scores: \" + \", \".join([f\"{k}={v:.3f}\" for k, v in feature_scores.items()]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nComputed {len(alignment_profiles)} alignment profiles\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute Bootstrap Confidence Intervals\n",
    "\n",
    "Calculate 95% confidence intervals for all performance metrics using bootstrap resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(\n",
    "    metric_values: np.ndarray,\n",
    "    n_bootstrap: int = 1000,\n",
    "    confidence_level: float = 0.95\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    compute bootstrap confidence interval for metric.\n",
    "    \n",
    "    args:\n",
    "        metric_values: array of metric values\n",
    "        n_bootstrap: number of bootstrap iterations\n",
    "        confidence_level: confidence level (default 0.95)\n",
    "    \n",
    "    returns:\n",
    "        (mean, lower_ci, upper_ci)\n",
    "    \"\"\"\n",
    "    bootstrap_means = []\n",
    "    n = len(metric_values)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # resample with replacement\n",
    "        sample = resample(metric_values, n_samples=n, random_state=None)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    bootstrap_means = np.array(bootstrap_means)\n",
    "    \n",
    "    # compute percentile-based confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = 100 * (alpha / 2)\n",
    "    upper_percentile = 100 * (1 - alpha / 2)\n",
    "    \n",
    "    lower_ci = np.percentile(bootstrap_means, lower_percentile)\n",
    "    upper_ci = np.percentile(bootstrap_means, upper_percentile)\n",
    "    mean_val = np.mean(metric_values)\n",
    "    \n",
    "    return mean_val, lower_ci, upper_ci\n",
    "\n",
    "\n",
    "# compute confidence intervals for generalization gaps\n",
    "print(\"\\nComputing bootstrap confidence intervals...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "generalization_gap_ci = {}\n",
    "\n",
    "for model_name in models.keys():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    \n",
    "    # get in-domain and out-of-domain accuracies\n",
    "    in_domain_acc = results.accuracy_matrix.get(model_name, {}).get(model_name, 0.0)\n",
    "    \n",
    "    out_domain_accs = [\n",
    "        results.accuracy_matrix.get(model_name, {}).get(test_name, 0.0)\n",
    "        for test_name in dataset_names if test_name != model_name\n",
    "    ]\n",
    "    \n",
    "    if not out_domain_accs:\n",
    "        continue\n",
    "    \n",
    "    # bootstrap on out-of-domain accuracies\n",
    "    out_domain_arr = np.array(out_domain_accs)\n",
    "    mean_ood, lower_ci, upper_ci = bootstrap_confidence_interval(\n",
    "        out_domain_arr,\n",
    "        n_bootstrap=CONFIG['bootstrap_iterations'],\n",
    "        confidence_level=CONFIG['confidence_level']\n",
    "    )\n",
    "    \n",
    "    # gap and its CI\n",
    "    gap_mean = in_domain_acc - mean_ood\n",
    "    gap_lower = in_domain_acc - upper_ci  # note: reversed because gap = ID - OOD\n",
    "    gap_upper = in_domain_acc - lower_ci\n",
    "    \n",
    "    generalization_gap_ci[model_name] = {\n",
    "        'gap_mean': gap_mean,\n",
    "        'gap_lower': gap_lower,\n",
    "        'gap_upper': gap_upper,\n",
    "        'gap_ci_width': gap_upper - gap_lower,\n",
    "        'in_domain_acc': in_domain_acc,\n",
    "        'out_domain_mean': mean_ood,\n",
    "        'out_domain_lower': lower_ci,\n",
    "        'out_domain_upper': upper_ci\n",
    "    }\n",
    "    \n",
    "    print(f\"  In-domain: {in_domain_acc:.4f}\")\n",
    "    print(f\"  Out-of-domain: {mean_ood:.4f} (95% CI: [{lower_ci:.4f}, {upper_ci:.4f}])\")\n",
    "    print(f\"  Gap: {gap_mean:+.4f} (95% CI: [{gap_lower:+.4f}, {gap_upper:+.4f}])\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Bootstrap confidence intervals computed\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Analysis: Clinical Alignment vs. Generalization\n",
    "\n",
    "Test Hypothesis 3: Higher clinical alignment correlates with better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather data for correlation analysis\n",
    "alignment_scores = []\n",
    "generalization_gaps = []\n",
    "out_of_domain_accs = []\n",
    "model_labels = []\n",
    "\n",
    "print(\"\\nPreparing correlation analysis...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in models.keys():\n",
    "    # get alignment score for this model (average across all datasets)\n",
    "    model_alignments = [\n",
    "        profile['overall_alignment']\n",
    "        for key, profile in alignment_profiles.items()\n",
    "        if profile['model_name'] == model_name\n",
    "    ]\n",
    "    \n",
    "    if not model_alignments:\n",
    "        print(f\"  Skipping {model_name}: no alignment data\")\n",
    "        continue\n",
    "    \n",
    "    avg_alignment = np.mean(model_alignments)\n",
    "    \n",
    "    # get generalization metrics\n",
    "    if model_name not in generalization_gap_ci:\n",
    "        print(f\"  Skipping {model_name}: no generalization data\")\n",
    "        continue\n",
    "    \n",
    "    gap_data = generalization_gap_ci[model_name]\n",
    "    gap = gap_data['gap_mean']\n",
    "    ood_acc = gap_data['out_domain_mean']\n",
    "    \n",
    "    alignment_scores.append(avg_alignment)\n",
    "    generalization_gaps.append(gap)\n",
    "    out_of_domain_accs.append(ood_acc)\n",
    "    model_labels.append(model_name)\n",
    "    \n",
    "    print(f\"  {model_name:15s}: alignment={avg_alignment:.4f}, gap={gap:+.4f}, ood_acc={ood_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nData points for correlation: {len(alignment_scores)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlations\n",
    "if len(alignment_scores) >= 3:\n",
    "    print(\"\\nCorrelation Analysis Results:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    alignment_arr = np.array(alignment_scores)\n",
    "    gaps_arr = np.array(generalization_gaps)\n",
    "    ood_arr = np.array(out_of_domain_accs)\n",
    "    \n",
    "    # Hypothesis 1: Higher alignment \u2192 Smaller gap (negative correlation)\n",
    "    spearman_gap, p_gap = scipy_stats.spearmanr(alignment_arr, gaps_arr)\n",
    "    pearson_gap, p_gap_pearson = scipy_stats.pearsonr(alignment_arr, gaps_arr)\n",
    "    \n",
    "    print(\"\\n1. ALIGNMENT vs. GENERALIZATION GAP:\")\n",
    "    print(f\"   Hypothesis: Higher alignment \u2192 Smaller gap (expect negative \u03c1)\")\n",
    "    print(f\"   Spearman \u03c1 = {spearman_gap:.4f}, p = {p_gap:.4f}\")\n",
    "    print(f\"   Pearson r = {pearson_gap:.4f}, p = {p_gap_pearson:.4f}\")\n",
    "    \n",
    "    if p_gap < 0.05:\n",
    "        sig_str = \"SIGNIFICANT\" if p_gap < 0.01 else \"significant\"\n",
    "        print(f\"   Result: {sig_str} (p < 0.05)\")\n",
    "        if spearman_gap < -0.3:\n",
    "            print(f\"   Interpretation: SUPPORTS hypothesis - higher alignment correlates with better generalization\")\n",
    "        elif spearman_gap > 0.3:\n",
    "            print(f\"   Interpretation: CONTRADICTS hypothesis - higher alignment correlates with WORSE generalization\")\n",
    "        else:\n",
    "            print(f\"   Interpretation: Weak correlation - inconclusive\")\n",
    "    else:\n",
    "        print(f\"   Result: Not significant (p >= 0.05)\")\n",
    "        print(f\"   Interpretation: Insufficient evidence for correlation\")\n",
    "    \n",
    "    # Hypothesis 2: Higher alignment \u2192 Higher OOD accuracy (positive correlation)\n",
    "    spearman_ood, p_ood = scipy_stats.spearmanr(alignment_arr, ood_arr)\n",
    "    pearson_ood, p_ood_pearson = scipy_stats.pearsonr(alignment_arr, ood_arr)\n",
    "    \n",
    "    print(\"\\n2. ALIGNMENT vs. OUT-OF-DOMAIN ACCURACY:\")\n",
    "    print(f\"   Hypothesis: Higher alignment \u2192 Higher OOD accuracy (expect positive \u03c1)\")\n",
    "    print(f\"   Spearman \u03c1 = {spearman_ood:.4f}, p = {p_ood:.4f}\")\n",
    "    print(f\"   Pearson r = {pearson_ood:.4f}, p = {p_ood_pearson:.4f}\")\n",
    "    \n",
    "    if p_ood < 0.05:\n",
    "        sig_str = \"SIGNIFICANT\" if p_ood < 0.01 else \"significant\"\n",
    "        print(f\"   Result: {sig_str} (p < 0.05)\")\n",
    "        if spearman_ood > 0.3:\n",
    "            print(f\"   Interpretation: SUPPORTS hypothesis - higher alignment correlates with better OOD performance\")\n",
    "        elif spearman_ood < -0.3:\n",
    "            print(f\"   Interpretation: CONTRADICTS hypothesis - higher alignment correlates with WORSE OOD performance\")\n",
    "        else:\n",
    "            print(f\"   Interpretation: Weak correlation - inconclusive\")\n",
    "    else:\n",
    "        print(f\"   Result: Not significant (p >= 0.05)\")\n",
    "        print(f\"   Interpretation: Insufficient evidence for correlation\")\n",
    "    \n",
    "    # Store correlation results\n",
    "    correlation_results = {\n",
    "        'alignment_vs_gap': {\n",
    "            'spearman_rho': float(spearman_gap),\n",
    "            'spearman_p': float(p_gap),\n",
    "            'pearson_r': float(pearson_gap),\n",
    "            'pearson_p': float(p_gap_pearson),\n",
    "            'significant': p_gap < 0.05\n",
    "        },\n",
    "        'alignment_vs_ood_accuracy': {\n",
    "            'spearman_rho': float(spearman_ood),\n",
    "            'spearman_p': float(p_ood),\n",
    "            'pearson_r': float(pearson_ood),\n",
    "            'pearson_p': float(p_ood_pearson),\n",
    "            'significant': p_ood < 0.05\n",
    "        },\n",
    "        'n_models': len(alignment_scores),\n",
    "        'data': {\n",
    "            'model_names': model_labels,\n",
    "            'alignment_scores': alignment_scores,\n",
    "            'generalization_gaps': generalization_gaps,\n",
    "            'ood_accuracies': out_of_domain_accs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "else:\n",
    "    print(f\"\\nInsufficient data points for correlation analysis (need >= 3, got {len(alignment_scores)})\")\n",
    "    correlation_results = {'error': 'insufficient_data', 'n_models': len(alignment_scores)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute Effect Sizes (Cohen's d)\n",
    "\n",
    "Quantify the magnitude of differences in generalization gaps between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    compute Cohen's d effect size.\n",
    "    \n",
    "    args:\n",
    "        group1: first group of values\n",
    "        group2: second group of values\n",
    "    \n",
    "    returns:\n",
    "        Cohen's d\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0.0\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def interpret_cohens_d(d: float) -> str:\n",
    "    \"\"\"interpret Cohen's d magnitude.\"\"\"\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif abs_d < 0.5:\n",
    "        return \"small\"\n",
    "    elif abs_d < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "\n",
    "# compute effect sizes for generalization gaps\n",
    "print(\"\\nComputing effect sizes...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "effect_sizes = {}\n",
    "\n",
    "# compare all pairs of models\n",
    "for i, model1 in enumerate(model_labels):\n",
    "    for j, model2 in enumerate(model_labels):\n",
    "        if i >= j:\n",
    "            continue\n",
    "        \n",
    "        # get out-of-domain accuracies for each model on different datasets\n",
    "        model1_ood = []\n",
    "        model2_ood = []\n",
    "        \n",
    "        for test_dataset in dataset_names:\n",
    "            if test_dataset != model1:\n",
    "                acc1 = results.accuracy_matrix.get(model1, {}).get(test_dataset, None)\n",
    "                if acc1 is not None:\n",
    "                    model1_ood.append(acc1)\n",
    "            \n",
    "            if test_dataset != model2:\n",
    "                acc2 = results.accuracy_matrix.get(model2, {}).get(test_dataset, None)\n",
    "                if acc2 is not None:\n",
    "                    model2_ood.append(acc2)\n",
    "        \n",
    "        if len(model1_ood) > 0 and len(model2_ood) > 0:\n",
    "            d = cohens_d(np.array(model1_ood), np.array(model2_ood))\n",
    "            interpretation = interpret_cohens_d(d)\n",
    "            \n",
    "            effect_sizes[f\"{model1}_vs_{model2}\"] = {\n",
    "                'cohens_d': float(d),\n",
    "                'interpretation': interpretation,\n",
    "                'model1_mean_ood': float(np.mean(model1_ood)),\n",
    "                'model2_mean_ood': float(np.mean(model2_ood))\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{model1} vs. {model2}:\")\n",
    "            print(f\"  {model1} mean OOD: {np.mean(model1_ood):.4f}\")\n",
    "            print(f\"  {model2} mean OOD: {np.mean(model2_ood):.4f}\")\n",
    "            print(f\"  Cohen's d: {d:+.4f} ({interpretation})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Effect sizes computed\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}