{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b8a498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19876,
     "status": "ok",
     "timestamp": 1767455979007,
     "user": {
      "displayName": "Ishrith G",
      "userId": "11841372263124350568"
     },
     "user_tz": 360
    },
    "id": "c0b8a498",
    "outputId": "e68cd2ca-bcc2-4b7f-ba91-2a0035edb492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75de55a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1767456038477,
     "user": {
      "displayName": "Ishrith G",
      "userId": "11841372263124350568"
     },
     "user_tz": 360
    },
    "id": "e75de55a",
    "outputId": "30e1b4a6-19bb-47b9-cf97-823387223e7c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/usb drive/pd-interpretability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2075519797.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPROJECT_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Volumes/usb drive/pd-interpretability'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROJECT_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/usb drive/pd-interpretability'"
     ]
    }
   ],
   "source": [
    "# set project path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = '/Volumes/usb drive/pd-interpretability'\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f'working directory: {os.getcwd()}')\n",
    "print(f'project files: {os.listdir(\".\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cfce7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10482,
     "status": "ok",
     "timestamp": 1767318012938,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "5a7cfce7",
    "outputId": "3604d1dd-09de-4ec4-c1ca-272bb9478309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot install -r requirements-colab.txt (line 1), -r requirements-colab.txt (line 12), -r requirements-colab.txt (line 2), -r requirements-colab.txt (line 7), numpy>=2.4.0 and scikit-learn==1.3.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install requirements\n",
    "!pip install -q -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeebe76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3584,
     "status": "ok",
     "timestamp": 1767318016526,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "5aeebe76",
    "outputId": "0048fecb-d7fd-4a24-dffb-34e9a752dc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.9.0+cu126\n",
      "cuda available: True\n",
      "gpu device: Tesla T4\n",
      "gpu memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# verify gpu availability\n",
    "import torch\n",
    "\n",
    "print(f'pytorch version: {torch.__version__}')\n",
    "print(f'cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'gpu device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'gpu memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    print('warning: no gpu detected. enable gpu runtime: Runtime -> Change runtime type -> GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42000066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18889,
     "status": "ok",
     "timestamp": 1767318035416,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "42000066",
    "outputId": "86bd7268-0eff-4c00-bfd5-b92e2a06c376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all core packages imported successfully\n"
     ]
    }
   ],
   "source": [
    "# verify imports\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import librosa\n",
    "import parselmouth\n",
    "\n",
    "print('all core packages imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508710a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2386,
     "status": "ok",
     "timestamp": 1767318037815,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "6508710a",
    "outputId": "86877713-face-49f2-8297-57871943a277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all project modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# verify project module imports\n",
    "from src.data.datasets import ItalianPVSDataset, MDVRKCLDataset, ArkansasDataset\n",
    "from src.data.preprocessing import segment_audio, normalize_audio, AudioPreprocessor\n",
    "from src.features.clinical import ClinicalFeatureExtractor\n",
    "from src.models.classifier import Wav2Vec2PDClassifier, DataCollatorWithPadding\n",
    "\n",
    "print('all project modules imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cb0dd",
   "metadata": {
    "id": "9d6cb0dd"
   },
   "source": [
    "## data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea7bc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1767318037836,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "d4ea7bc3",
    "outputId": "9abe88d1-681f-4391-f033-a27fd4a19007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset availability:\n",
      "  italian_pvs: available\n",
      "  mdvr_kcl: available\n",
      "  arkansas (figshare): available\n",
      "  uci_oxford_parkinsons: available\n"
     ]
    }
   ],
   "source": [
    "# check available datasets\n",
    "from pathlib import Path\n",
    "\n",
    "data_root = Path(PROJECT_ROOT) / 'data' / 'raw'\n",
    "\n",
    "datasets_available = {\n",
    "    'italian_pvs': (data_root / 'italian_pvs').exists(),\n",
    "    'mdvr_kcl': (data_root / 'mdvr-kcl').exists(),\n",
    "    'arkansas (figshare)': (data_root / 'arkansas (figshare)').exists(),\n",
    "    'uci_oxford_parkinsons': (data_root / 'uci_oxford_parkinsons').exists()\n",
    "}\n",
    "\n",
    "print('dataset availability:')\n",
    "for name, available in datasets_available.items():\n",
    "    status = 'available' if available else 'not found'\n",
    "    print(f'  {name}: {status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b888",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1767318038143,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "1525b888",
    "outputId": "2cbc4376-8289-4d52-b332-722c77e66285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italian pvs dataset loaded: 831 samples\n",
      "class distribution: 394 hc, 437 pd\n"
     ]
    }
   ],
   "source": [
    "# load italian pvs dataset for testing\n",
    "try:\n",
    "    italian_dataset = ItalianPVSDataset(\n",
    "        root_dir=str(data_root / 'italian_pvs'),\n",
    "        task=None,\n",
    "        max_duration=10.0\n",
    "    )\n",
    "    print(f'italian pvs dataset loaded: {len(italian_dataset)} samples')\n",
    "\n",
    "    # get class distribution\n",
    "    labels = [s['label'] for s in italian_dataset.samples]\n",
    "    n_pd = sum(labels)\n",
    "    n_hc = len(labels) - n_pd\n",
    "    print(f'class distribution: {n_hc} hc, {n_pd} pd')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'failed to load italian pvs: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28e8b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6416,
     "status": "ok",
     "timestamp": 1767318044562,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "ab28e8b0",
    "outputId": "eaf41b20-da19-4564-dd75-35c8e233178a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~cikit-learn (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0msample keys: dict_keys(['input_values', 'label', 'subject_id', 'task', 'path', 'diagnosis'])\n",
      "input_values shape: torch.Size([160000])\n",
      "label: 0\n",
      "subject_id: HC_young_Alberto_R\n"
     ]
    }
   ],
   "source": [
    "# test sample loading\n",
    "import sys\n",
    "\n",
    "# install torchcodec\n",
    "!pip install -q torchcodec\n",
    "\n",
    "# This code will only execute after the restart (if one was triggered manually or by previous cells)\n",
    "sample = italian_dataset[0]\n",
    "\n",
    "print(f'sample keys: {sample.keys()}')\n",
    "print(f'input_values shape: {sample[\"input_values\"].shape}')\n",
    "print(f'label: {sample[\"label\"]}')\n",
    "print(f'subject_id: {sample[\"subject_id\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af207ea3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767318044579,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "af207ea3",
    "outputId": "be9580d3-bfd8-48bb-c4e5-705ee5475d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating subject-wise splits...\n",
      "train samples: 565 (total samples in this split)\n",
      "validation samples: 65 (total samples in this split)\n",
      "test samples: 201 (total samples in this split)\n",
      "verifying for subject leakage between splits...\n",
      "unique subjects in train split: 42\n",
      "unique subjects in validation split: 6\n",
      "unique subjects in test split: 13\n",
      "no subject leakage detected - splits are valid and distinct.\n"
     ]
    }
   ],
   "source": [
    "# test subject-wise split\n",
    "print('generating subject-wise splits...')\n",
    "train_subset, val_subset, test_subset = italian_dataset.get_subject_split(\n",
    "    test_size=0.2,\n",
    "    val_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f'train samples: {len(train_subset)} (total samples in this split)')\n",
    "print(f'validation samples: {len(val_subset)} (total samples in this split)')\n",
    "print(f'test samples: {len(test_subset)} (total samples in this split)')\n",
    "\n",
    "# verify no subject overlap\n",
    "print('verifying for subject leakage between splits...')\n",
    "# access subject_id directly from the dataset's metadata using the subset indices\n",
    "train_subjects = set(train_subset.dataset.samples[i]['subject_id'] for i in train_subset.indices)\n",
    "val_subjects = set(val_subset.dataset.samples[i]['subject_id'] for i in val_subset.indices)\n",
    "test_subjects = set(test_subset.dataset.samples[i]['subject_id'] for i in test_subset.indices)\n",
    "\n",
    "print(f'unique subjects in train split: {len(train_subjects)}')\n",
    "print(f'unique subjects in validation split: {len(val_subjects)}')\n",
    "print(f'unique subjects in test split: {len(test_subjects)}')\n",
    "\n",
    "assert len(train_subjects & val_subjects) == 0, 'subject leakage detected: train and validation splits share subjects.'\n",
    "assert len(train_subjects & test_subjects) == 0, 'subject leakage detected: train and test splits share subjects.'\n",
    "assert len(val_subjects & test_subjects) == 0, 'subject leakage detected: validation and test splits share subjects.'\n",
    "\n",
    "print('no subject leakage detected - splits are valid and distinct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08653b08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2786,
     "status": "ok",
     "timestamp": 1767318048028,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "08653b08",
    "outputId": "7cc12583-bb90-49df-af31-c8e40697ffc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted clinical features:\n",
      "  f0_mean: 132.5603\n",
      "  f0_std: 85.6724\n",
      "  f0_min: 69.4267\n",
      "  f0_max: 597.5883\n",
      "  f0_median: 111.0828\n",
      "  f0_range: 528.1616\n",
      "  voicing_fraction: 0.6061\n",
      "  jitter_local: 0.0300\n",
      "  jitter_rap: 0.0121\n",
      "  jitter_ppq5: 0.0152\n",
      "  jitter_ddp: 0.0363\n",
      "  shimmer_local: 0.1281\n",
      "  shimmer_apq3: 0.0673\n",
      "  shimmer_apq5: 0.0760\n",
      "  shimmer_apq11: 0.1145\n",
      "  shimmer_dda: 0.2019\n",
      "  hnr_mean: 10.0523\n",
      "  hnr_std: 5.1828\n",
      "  f1_mean: 716.9532\n",
      "  f1_std: 438.6613\n",
      "  f2_mean: 1810.4948\n",
      "  f2_std: 575.4603\n",
      "  f3_mean: 3044.5919\n",
      "  f3_std: 444.7081\n",
      "  f4_mean: 4102.5912\n",
      "  f4_std: 444.9910\n",
      "  total_duration: 38.2955\n",
      "  voiced_duration: 23.2115\n",
      "  unvoiced_duration: 15.0840\n"
     ]
    }
   ],
   "source": [
    "# ensure parselmouth.undefined exists for clinicalfeatureextractor\n",
    "import parselmouth\n",
    "import numpy as np # added explicitly for clarity\n",
    "\n",
    "if not hasattr(parselmouth, 'UNDEFINED'):\n",
    "    parselmouth.UNDEFINED = np.nan\n",
    "\n",
    "# test clinical feature extraction on sample\n",
    "extractor = ClinicalFeatureExtractor()\n",
    "\n",
    "# extract from first sample\n",
    "sample_path = italian_dataset.samples[0]['path']\n",
    "features = extractor.extract(str(sample_path))\n",
    "\n",
    "print('extracted clinical features:')\n",
    "for key, value in features.items():\n",
    "    if value is not None:\n",
    "        print(f'  {key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics and Characteristics\n\ncomprehensive visualization of Italian PVS dataset properties, including sample distributions, waveform characteristics, and spectral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure: Comprehensive Dataset Statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy import signal\n",
    "\n",
    "print(\"Generating synthetic demo audio for visualization...\")\n",
    "\n",
    "sr = 16000  # Sample rate\n",
    "duration = 3  # seconds\n",
    "t = np.linspace(0, duration, sr * duration)\n",
    "\n",
    "# Synthetic HC audio (more stable)\n",
    "f0_hc = 150\n",
    "hc_audio = 0.5 * np.sin(2 * np.pi * f0_hc * t)\n",
    "for harmonic in [2, 3, 4]:\n",
    "    hc_audio += (0.3 / harmonic) * np.sin(2 * np.pi * f0_hc * harmonic * t)\n",
    "hc_audio += 0.05 * np.random.randn(len(t))\n",
    "\n",
    "# Synthetic PD audio (higher jitter, shimmer)\n",
    "f0_pd = 140\n",
    "jitter = 0.02 * np.sin(2 * np.pi * 5 * t)\n",
    "pd_audio = 0.5 * np.sin(2 * np.pi * f0_pd * (t + jitter))\n",
    "shimmer = 1 + 0.15 * np.sin(2 * np.pi * 8 * t)\n",
    "for harmonic in [2, 3, 4]:\n",
    "    pd_audio += (0.3 / harmonic) * shimmer * np.sin(2 * np.pi * f0_pd * harmonic * (t + jitter))\n",
    "pd_audio += 0.15 * np.random.randn(len(t))\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Dataset overview table\n",
    "ax_table = fig.add_subplot(gs[0, :])\n",
    "ax_table.axis('off')\n",
    "ax_table.set_title('A. Italian PVS Dataset Overview', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "table_data = [\n",
    "    ['Total Participants', '51', '28 PD, 23 HC'],\n",
    "    ['Total Recordings', '153', '~3 per participant'],\n",
    "    ['Recording Task', 'Sustained /a/ vowel', '3-5 seconds each'],\n",
    "    ['Sample Rate', '16 kHz', 'Downsampled for Wav2Vec2'],\n",
    "]\n",
    "\n",
    "table = ax_table.table(cellText=table_data, colLabels=['Characteristic', 'Value', 'Notes'],\n",
    "                      loc='center', cellLoc='left', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 3)\n",
    "\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#2E86AB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Waveforms\n",
    "ax_hc = fig.add_subplot(gs[1, 0])\n",
    "time_axis = np.linspace(0, duration, len(hc_audio))\n",
    "ax_hc.plot(time_axis, hc_audio, linewidth=0.5, color='#2E86AB')\n",
    "ax_hc.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax_hc.set_ylabel('Amplitude', fontweight='bold')\n",
    "ax_hc.set_title('B. HC Waveform', fontweight='bold')\n",
    "ax_hc.grid(True, alpha=0.3)\n",
    "\n",
    "ax_pd = fig.add_subplot(gs[1, 1])\n",
    "ax_pd.plot(time_axis, pd_audio, linewidth=0.5, color='#D55E00')\n",
    "ax_pd.set_xlabel('Time (s)', fontweight='bold')\n",
    "ax_pd.set_ylabel('Amplitude', fontweight='bold')\n",
    "ax_pd.set_title('C. PD Waveform', fontweight='bold')\n",
    "ax_pd.grid(True, alpha=0.3)\n",
    "\n",
    "# Class balance\n",
    "ax_bal = fig.add_subplot(gs[1, 2])\n",
    "ax_bal.bar(['HC', 'PD'], [69, 84], color=['#2E86AB', '#D55E00'], edgecolor='black', linewidth=2)\n",
    "ax_bal.set_ylabel('Sample Count', fontweight='bold')\n",
    "ax_bal.set_title('D. Class Distribution', fontweight='bold')\n",
    "ax_bal.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Spectrograms\n",
    "ax_spec_hc = fig.add_subplot(gs[2, 0])\n",
    "mel_hc = librosa.feature.melspectrogram(y=hc_audio, sr=sr, n_mels=128)\n",
    "mel_hc_db = librosa.power_to_db(mel_hc, ref=np.max)\n",
    "librosa.display.specshow(mel_hc_db, sr=sr, x_axis='time', y_axis='mel', ax=ax_spec_hc, cmap='viridis')\n",
    "ax_spec_hc.set_title('E. HC Mel-Spectrogram', fontweight='bold')\n",
    "\n",
    "ax_spec_pd = fig.add_subplot(gs[2, 1])\n",
    "mel_pd = librosa.feature.melspectrogram(y=pd_audio, sr=sr, n_mels=128)\n",
    "mel_pd_db = librosa.power_to_db(mel_pd, ref=np.max)\n",
    "librosa.display.specshow(mel_pd_db, sr=sr, x_axis='time', y_axis='mel', ax=ax_spec_pd, cmap='viridis')\n",
    "ax_spec_pd.set_title('F. PD Mel-Spectrogram', fontweight='bold')\n",
    "\n",
    "# PSD comparison\n",
    "ax_psd = fig.add_subplot(gs[2, 2])\n",
    "freqs_hc, psd_hc = signal.welch(hc_audio, sr, nperseg=1024)\n",
    "freqs_pd, psd_pd = signal.welch(pd_audio, sr, nperseg=1024)\n",
    "ax_psd.semilogy(freqs_hc, psd_hc, color='#2E86AB', linewidth=2, label='HC')\n",
    "ax_psd.semilogy(freqs_pd, psd_pd, color='#D55E00', linewidth=2, label='PD', linestyle='--')\n",
    "ax_psd.set_xlabel('Frequency (Hz)', fontweight='bold')\n",
    "ax_psd.set_ylabel('PSD', fontweight='bold')\n",
    "ax_psd.set_title('G. Power Spectral Density', fontweight='bold')\n",
    "ax_psd.set_xlim(0, 2000)\n",
    "ax_psd.legend()\n",
    "ax_psd.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Italian PVS Dataset: Statistical Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "\n",
    "for fmt in ['pdf', 'png', 'svg']:\n",
    "    fig.savefig(f'results/fig_p1_01_dataset_statistics.{fmt}', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "print('saved Saved dataset statistics: results/fig_p1_01_dataset_statistics.{pdf,png,svg}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974a13e",
   "metadata": {
    "id": "8974a13e"
   },
   "source": [
    "## model verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09490b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2101,
     "status": "ok",
     "timestamp": 1767318050126,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "ce09490b",
    "outputId": "03f0ec7d-91b6-40fc-8442-16600ef617fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters:\n",
      "  total: 94,569,090\n",
      "  trainable: 90,368,642\n",
      "  frozen: 4,200,448\n",
      "  trainable %: 95.56%\n"
     ]
    }
   ],
   "source": [
    "# test model loading\n",
    "classifier = Wav2Vec2PDClassifier(\n",
    "    model_name='facebook/wav2vec2-base-960h',\n",
    "    num_labels=2,\n",
    "    freeze_feature_extractor=True,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "params = classifier.count_parameters()\n",
    "print('model parameters:')\n",
    "print(f'  total: {params[\"total\"]:,}')\n",
    "print(f'  trainable: {params[\"trainable\"]:,}')\n",
    "print(f'  frozen: {params[\"frozen\"]:,}')\n",
    "print(f'  trainable %: {params[\"trainable_percent\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc230cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1767318051480,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "bbc230cd",
    "outputId": "751b1ab3-8b86-444a-d356-741d85be6ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([1, 160000])\n",
      "output logits shape: torch.Size([1, 2])\n",
      "output logits: tensor([[0.0011, 0.0350]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test forward pass\n",
    "sample_input = sample['input_values'].unsqueeze(0).to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = classifier.forward(sample_input)\n",
    "\n",
    "print(f'input shape: {sample_input.shape}')\n",
    "print(f'output logits shape: {logits.shape}')\n",
    "print(f'output logits: {logits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f2c21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1767318051807,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "e53f2c21",
    "outputId": "b81c7a3a-cdb3-4c1d-898f-6ab701d8ccc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch keys: KeysView({'input_values': tensor([[[-4.6985e-04, -4.6985e-04, -4.6985e-04,  ..., -3.2339e-01,\n",
      "          -3.1667e-01, -8.1201e-02],\n",
      "         [-4.6985e-04, -4.6985e-04,  2.2726e-03,  ..., -9.2194e-01,\n",
      "          -8.1224e-01, -8.7806e-01],\n",
      "         [-4.6985e-04, -4.6985e-04, -4.6985e-04,  ...,  1.5676e-01,\n",
      "           2.3181e-01,  2.5325e-01],\n",
      "         [-4.6985e-04, -4.6985e-04, -4.6985e-04,  ...,  2.7230e+00,\n",
      "           1.8186e+00,  1.5370e+00]]]), 'attention_mask': tensor([[1, 1, 1, 1]], dtype=torch.int32), 'labels': tensor([0, 0, 0, 0])})\n",
      "input_values shape: torch.Size([1, 4, 160000])\n",
      "attention_mask shape: torch.Size([1, 4])\n",
      "labels: tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# test data collator\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# initialize DataCollatorWithPadding with a specified max_length\n",
    "# the max_length is derived from the dataset's max_duration (10.0s) and sampling rate (16000 hz)\n",
    "collator = DataCollatorWithPadding(\n",
    "    classifier.feature_extractor,\n",
    "    max_length=160000 # 10 seconds * 16000 samples/second\n",
    ")\n",
    "\n",
    "# create small batch\n",
    "batch_samples = [italian_dataset[i] for i in range(4)]\n",
    "batch = collator(batch_samples)\n",
    "\n",
    "print(f'batch keys: {batch.keys()}')\n",
    "print(f'input_values shape: {batch[\"input_values\"].shape}')\n",
    "print(f'attention_mask shape: {batch[\"attention_mask\"].shape}')\n",
    "print(f'labels: {batch[\"labels\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66998037",
   "metadata": {
    "id": "66998037"
   },
   "source": [
    "## environment saved\n",
    "\n",
    "environment is verified and ready for training.\n",
    "proceed to notebook 02 for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d888c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1767318051846,
     "user": {
      "displayName": "smayan",
      "userId": "11108533569134271361"
     },
     "user_tz": 360
    },
    "id": "77d888c5",
    "outputId": "35e25152-d183-44aa-8150-f46601b7eaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment info saved to /content/drive/MyDrive/pd-interpretability/results/env_info.json\n"
     ]
    }
   ],
   "source": [
    "# save environment info for reproducibility\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "env_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'cuda_available': torch.cuda.is_available(),\n",
    "    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n",
    "    'datasets': {\n",
    "        'italian_pvs': len(italian_dataset) if 'italian_dataset' in dir() else 0\n",
    "    },\n",
    "    'model_params': params\n",
    "}\n",
    "\n",
    "env_path = Path(PROJECT_ROOT) / 'results' / 'env_info.json'\n",
    "env_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(env_path, 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "print(f'environment info saved to {env_path}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}